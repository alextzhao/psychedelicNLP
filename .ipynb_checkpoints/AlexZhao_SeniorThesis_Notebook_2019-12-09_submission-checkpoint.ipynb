{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x9A-7AYkMFLr"
   },
   "source": [
    "# Dimensions of Non-Ordinary Experiences: Natural Language Processing of Trascendent Experience Reports\n",
    "\n",
    "`Senior Thesis [EAS 499]`\n",
    "\n",
    "`University of Pennsylvania, Fall 2019`\n",
    "\n",
    "#### Author: **Alex Tianheng Zhao**  \n",
    "> `alexzhao@seas.upenn.edu`  \n",
    "> Department of Computer and Information, School of Engineering and Applied Science, University of Pennsylvania  \n",
    "> Department of Statistics, The Wharton School, University of Pennsylvania  \n",
    "> üåê[`personal website`](https://alextzhao.io), [`github`](https://github.com/alextzhao), [`linkedin`](https://www.linkedin.com/in/alextzhao), [`ORCID`](https://orcid.org/0000-0001-6745-5980)\n",
    "\n",
    "\n",
    "#### **Max Mintz, PhD**  \n",
    "> `mintz@cis.upenn.edu`  \n",
    "> Coordinator, Senior Thesis Program, Department of Computer and Information Science  \n",
    "> University of Pennsylvania  \n",
    "\n",
    "\n",
    "## **Thesis Advisors**\n",
    "> **Chris Callison-Burch, PhD**  \n",
    "> `ccb@upenn.edu`  \n",
    "> Department of Computer and Information Science (SEAS), University of Pennsylvania  \n",
    " \n",
    "> **Lyle Ungar, PhD**  \n",
    "> `ungar@cis.upenn.edu`  \n",
    "> Department of Computer and Information Science; additional appoints in the Departments of Bioengineering (SEAS); Genomics and Computational Biology (Penn Medicine); Operations, Informations, and Decisions (Wharton); Psychology (SAS), University of Pennsylvania  \n",
    "\n",
    "\n",
    "### **Useful Links:**\n",
    "- Thesis Related:\n",
    "    - [Thesis Master Document](https://docs.google.com/document/d/1dk1xXyfHqfdn5Tld-KZu7toiNYQeJHQv7BUlG7uSqP4/edit#)\n",
    "    - [Thesis Scratch Paper](https://docs.google.com/document/d/1BP5Z2J9tJvRJB5J-hthQIGrdSnD0Bcvctd7kHqbUUKw/edit?usp=sharing)  \n",
    "    - [Thesis Codebase](https://github.com/alextzhao/psychedelicNLP)\n",
    "- Helpful Tutorials and Tips\n",
    "     - Excellent Series from `Towards Data Science`:\n",
    "         - [A Practitioner's Guide to Natural Language Processing (Part I): Processing & Understanding Text](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)\n",
    "         - [Feature Engineering: Continuous Numeric Data](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "         - [Feature Engineering: Categorical Data](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "         - [Feature Engineering: Text Data Basics](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)\n",
    "         - [Feature Engineering: Text Data Advanced](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n",
    "     - [A few userful things to know about machine learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "     - [LDA: Excellent Talk by Christine Doig on Topic Models](http://chdoig.github.io/pygotham-topic-modeling/#/)\n",
    "     - [Markdown Cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#links)\n",
    "     - [Using R and Python together](https://stackoverflow.com/questions/39008069/r-and-python-in-one-jupyter-notebook); [python, R dataframe interoperability](https://rpy2.github.io/doc/latest/html/pandas.html); [R and python pipelining](https://blog.revolutionanalytics.com/2016/01/pipelining-r-python.html)\n",
    "     - [Translate dplyr to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html)\n",
    "     - [Basic pandas tutorial](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html#min)\n",
    "     - [Recommended dependencies for pandas](https://pandas.pydata.org/pandas-docs/stable/install.html#install-recommended-dependencies)\n",
    "     - [Pandas: Working with Text Data](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html)  \n",
    "     - [sklearn reference](https://scikit-learn.org/stable/modules/classes.html): handy documentation, and provides broad-strokes ontology for machine learning techniques\n",
    "     - [Introduction to Machine Learning with sklearn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
    "     - [Sample pipeline for text feature extraction and evaluation](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py): using `sklearn`\n",
    "     - [Comparison of performances of different classifiers](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py): from `sklearn`, super cool\n",
    "     - TODO: [Citing sklearn](https://scikit-learn.org/stable/about.html#citing-scikit-learn)\n",
    "     - [Contributing to sklearn](https://scikit-learn.org/stable/developers/contributing.html)\n",
    "     - [Manually Creating a table of contents](https://medium.com/@sambozek/ipython-er-jupyter-table-of-contents-69bb72cf39d3)\n",
    "     - [Automaticaly Creating Table of Contents](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html)\n",
    "     - [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook): Free Jupyter Notebooks\n",
    "     - [Configuring jupyter notebook with extensions](https://github.com/Jupyter-contrib/jupyter_nbextensions_configurator)\n",
    "- Publishing my thesis:\n",
    "    - [Jupyter Notebook Viewer](https://nbviewer.jupyter.org/)\n",
    "    - [Hiding Code Cells for Better Viewing Experience](https://chris-said.io/2016/02/13/how-to-make-polished-jupyter-presentations-with-optional-code-visibility/)\n",
    "    - [Cool Preloaders](https://icons8.com/preloaders/)\n",
    "   \n",
    "\n",
    "![Pandas Indexing Cheatsheet](./pandas_indexing_cheatsheet.png)\n",
    "- Notes to self:\n",
    "    - `~/opt/anaconda3/lib/python3.7/site-packages/jupyter_contrib_nbextensions/nbextensions/toc2` path to nbextensions toc2\n",
    "    - [fixing nbextensions](https://github.com/ipython-contrib/jupyter_contrib_nbextensions/issues/1090)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UrGwu3iCMFLu"
   },
   "source": [
    "# [ Cover Page End ]  \n",
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xQ1O5-3rMFLu"
   },
   "source": [
    "# Table of Contents\n",
    "`TODO`\n",
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYkZN-G6MFLv"
   },
   "source": [
    "# 1. Abstract\n",
    "\n",
    "Using natural language processing (NLP) techniques on approximately 30,000 reports of transcendent experiences, we examine and better understand the dimensions of non-ordinary states of consciousness. The primary data source is the Vaults of Erowid, which consists of approximately 25K psychedelic trip reports. Through this work, we identify the hidden topics of subjective experiences through topic modelling, clusters of similar experiences, and visualize the dimensions of subjective experiences through word clouds and other visualization techniques. We also demonstrate the ability to predict (with fairly high confidence) the origins (i.e.: psychedelic drug of choice, set and setting) of such transcendent experiences. Finally, we place this research in historical and academic context, and make a proposal for a more connected and blissful future.\n",
    "\n",
    "For link to the work in progress codebase, see the [public project github](https://github.com/alextzhao/psychedelicNLP)\n",
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uo-U8oO0MFLw"
   },
   "source": [
    "# 2. Background and Signifinance\n",
    "\n",
    "## 2.1 What are Psychedelics\n",
    "\n",
    "todo insert brief legal background\n",
    "\n",
    "**_Psychedelics_** (from the Greek *psyche*: mind, *delos*: make visible, reveal; together meaning ‚Äúmind manifesting‚Äù, coined in a letter from Humphrey Osmond to Aldous Huxley), are ‚Äúsubstances that induce a heightened state of consciousness characterised by a hyperconnected brain state‚Äù (todo). The most well known ‚Äúclassical psychedelics‚Äù, some of which have been used by indigenous cultures for thousands of years, activate the serotonin system, in particular by agonizing the 5HT-2A serotonin receptor subtype. These classical psychedelics include Lysergic Acid Di-amide (LSD, semi-synthetic psychedelic derived from the ergot fungi), Psilocybin (inactive prodrug to the psychoactive Psilocin, found in certain mushrooms), Di-mythyltryptamine (DMT, the active ingredient in the powerful, indigenous Amazonian brew Ayahuasca), and Mescaline (found in Peyote Cactus). Newer psychedelics such as 2-CB, 5-Methoxy-Dimethyltryptamine (5-Meo-DMT, found in multiple plant species and most famously the Sonoran Desert Toad) are also often classified under the term hallucinogen or psychedelic. Other related drugs such as 3,4-Methylenedioxymethamphetamine (MDMA) are often referred to as psychedelic drugs, but are more correctly classified as an empathogen or entactogen (todo) (meaning ‚Äútouching within‚Äù,  from the Greek en: within), the Latin tactus: touch and the Greek -gen: produce)\n",
    "\n",
    "## 2.2 Motivation: Why This Thesis Exists\n",
    "\n",
    "todo: discussion on mental health\n",
    "\n",
    "On September 4th, 2019, Johns Hopkins University launched the Johns Hopkins Center for Psychedelics and Consciousness Research, with $17 million dollars in research funding. The center is thought to be the largest of its kind in the world, and sets to study addition, PTSD, addiction, depression, anorexia nervosa, trauma, and other mental ‚Äúillnesses‚Äù with psychedelics and traditional forms of therapy. \n",
    "\n",
    "After decades of suppression and moral hazard, psychedelics are making a return to legitimate research as a novel and surprisingly effective treatment modality for multiple extremely difficult conditions, including but not limited to end of life anxiety (todo), treatment-resistant depression (todo), post traumatic stress disorder (PTSD) (todo), alcohol and nicotine addiction (todo), sexual and racial trauma, and alzheimer‚Äôs (todo). Psychedelics have also shown dramatic results for ‚Äúwell-people‚Äù, including being able to reliably induce ‚Äúmystical-type experiences‚Äù (todo), increase creativity, and is correlated with greatly enhanced feelings of religious devotion (todo). Beyond clinical studies, animal studies with octopi have demonstrated MDMA increasing feelings of social connectedness (todo); In vitro studies have shown enhanced neuroplasticity (todo Victor preprint), orchestrated brain activity; Brain imaging studies, primarily out of Imperial College London and University College London, have shown psychedelics quiet the default mode network (todo), induce hyperconnected and entropic brain states (todo), and proposed a mechanism for action for psychedelics, formalized as the Free Energy Principle and RHEBUS (todo). Selena Atasoy of Oxford University has also recently demonstrated that the brain under psychedelics has more high energy resonant states, while remaining hyper-coherent and highly orchestrated, using a novel technique called Connectome Specific Harmonic Waves (CSHW). The Qualia Research Institute (QRI) has also recently proposed the Symmetry Theory of Valence (STV), which posits that symmetries in resonant brain states as modelled by a high dimensional mathematical objects, is correlated with experiential valence.\n",
    "\n",
    "With all the exciting work being done in the psychedelics landscape (call it ‚Äúpsychedemia‚Äù), be it clinical, neuroscientific, or anthropological, little work has been done on understanding what the psychedelic trip actually consists of, and understanding qualitatively and quantitatively the dimensions of the psychedelic subjective experience. Current work by Imperial College and Oxford (todo) are showing promising results for understanding both the macro and micro phenomenology of psychedelic experiences. Using natural language as a low-dimensional, inadequately wanting proxy, we can begin to understand the hallmark subjective signatures of different substances, and potentially identify novel neuro-targets for clinical interventions to reduce suffering, as well as gain a deeper understanding of mechanisms of action for psychedelic substances, if such a mechanistic understanding is even possible. It is the sincere hope that through this work, we may begin to demystify the subjective dimensions that characterize psychedelic experiences, and lay the foundations for more informed clinical and societal support structures that may alleviate unnecessary mental suffering and enrich the lives of well-people.  \n",
    "\n",
    "This thesis seeks to extend the current work by using techniques from machine learning, and in particular natural language processing, on a corpus of psychedelic trip reports retrieved from multiple sources, including chiefly, the Vaults of Erowid. We will present the results factually, with minimal layers of interpretation and meticulous documentation for high reproducibility, clarifying our assumptions and processes whenever appropriate. It is our hope that this work may be open-sourced and beneficial to clinical researchers, psychologists, neuroscientists, machine learning engineers, social scientists, and the lay people community as a whole. If not for some deep, metaphysical takeaway, perhaps for some lighthearted, jovial pass time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9VXuSY1Csv9"
   },
   "source": [
    "## 2.3 Hacky Origins: The Real Reason Why This Thesis Exists\n",
    "\n",
    "In the spring of 2019, Victor Acero, Rahul Sood, Margaret Fleming and I cofounded Penn Society for Psychedelic Science (PSPS), scrambled together the inaugural Intercollegiate Psychedelics Summit, [IPS 2019](https://pennpsychedelics.org/ips2019), at the University of Pennsylvania, which drew about 150 students and 8 speakers, including Frederick Barrett and Manoj Doss from JHU's Center for Psychedeelics and Consciousness Research (Psychedelics Research Unit at the time) and David Nutt, president of the European Brain Council and researcher at Imperial College London's Psychedelics Research Unit. Feeling slightly burnt out from planning the summit in 4 months, we took the summer to recharge, with Victor Acero and Emily Cribas (now PSPS Operational Director) hosting 2 journal clubs over the summer and the remaining cofounders in different corners of the world, recuperating from a very intense 4 months.\n",
    "\n",
    "Come fall of 2019, we reconvened with great enthusiasm, and collectively ratified PSPS's constitution and bylaws, estalished the **Board of Directors** structure, and welcomed 6 new team members onto the **PSPS Council**, which supports the PSPS General Member Body. One of the new members was **Yev Barkalov** (`@yevbar`), who is one of the most talented software engineers and self-proclaimed hackers I know, having founded `Hack the Fog` (San Francisco's first high school hackathon) and attended 30+ hackathons just for fun. Around late September, we began working on establishing [PSPSLabs](https://pennpsychedelics.org/pspslabs), which is an innovative lab for psychedelic projects within the umbrella of PSPS. Think GoogleX, but for psychedelics and consciousness related projects. We were setting up PSPSLabs to provide a nest for potentially impactful, consciousness expanding projects to grow and blossom, and our hope was that through our work, we may enrich the experiences of those around us in safe and responsible ways. With the ethos of silicon valley and a deep, humbling belief that the responsible marrying of tech and science can potentially bring positivity and value to those around us, we began brainstorming projects we could work on for fun to share with the world. Yev and I began calling regularly for hours at a time, sharing resources, chatting about the recent developments in psychedelia, and coming up with random ideas.\n",
    "\n",
    "One of the projects that blossomed into our shared consciousness on one such call was [**EveryQualia**](https://everyqualia.org), a search engine for psychedelic trip reports. We were super curious what the experiences of others were like on psychedelic drugs. We began looking at the Johns Hopkins and Imperial College London survey studies, which had super interesting findings such as how the subjective rating of ego dissolution correlated with the quieting of the default mode network (todo), and how a single high dose psilocyin session let to significant decreaes in depressive symptons (as measured by survey results). After looking at a few studies, we began wondering ‚Äî is there more out there? The surveys were good because they were rigorous and comparable between subjects, but a few numbers and scales could hardly capture the ineffability, noetic quality, and richness of psychedelic experiences! We became extremely fasincated by free style trip reports, unburdened by the limiting and valve reducing nature of surveys. Alas, after poking around for a while, we realized that there was no good search engine for psychedelic trip reports! There were various reddit threads and personal stories scattered here and there but no centralized data source. Then we found Erowid, which contained a massive archive of about 35,000 trip reports (as of Dec 9, 2019) The Vaults of Erowid was a treasure trove of information. Indeed, Erowid is one of the oldest and most trusted sources for information about hundreds and thousands of drugs, psychedelics and otherwise. We thought we could start by scraping Erowid and saving the trip reports to a database, build a simple webapp frontend, and use something like Algolia (search as a service API) to provide smooth realtime search functionality of the massive Erowid trip report. And just like that, EveryQualia [V0](v0.everyqualia.org) was born, live with about 10K trip reports (we didn't have any money, and Algolia was limited to 10K data objects. Also: if Earth and Fire Erowid are reading this, we apologize for bombarding your serves with our scrapers, and note we totally deserved to be blocked).\n",
    "\n",
    "Yev became driving technological powerhouse behind EveryQualia while I dipped deeper into Machine Learning and Natural Language Processing techniques, having taken `CIS 350: Computational Linguistics` with Chris Callison-Burch in the spring of 2019 and now taking `CIS 520: Machine Learning` with Lyle Ungar. With version 0 of our webapp, we had real time search and a groovy interface to read trip reports, which was awesome, but we still didn't have a way to visualize any large scale patterns that were present in the corpus. Since machine learning is all about learning generalizable patterns from massive amounts of data, using machine learning, and in particular natural language processing on a psychedelics report corpus, seemed like a natural extension. Not only would writing this thesis deepen my understanding of the theoretical foundations of machine learning, it also felt like a valuable launching pad for NLP technology that we plan to integrate into EveryQualia. I will say, at this point in a time, just how grateful I am to have two mentors and past professors to sign on as advisors to this rather unconventional thesis. Thank you\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80IcYljlMFLx"
   },
   "source": [
    "## 2.3 Previous Work\n",
    "\n",
    "- [Chemical Youth: Visualizing Erowid]: As of Dec 9, 2019, their website is not live for unkonwn reasons, but a July 23, 2019 archived version can be found [here](http://web.archive.org/web/20190723023239/https://chemicalyouth.org/visualising-erowid/). The visulizations done by this team is absolutely stellar, and the reader is invited to check out their work before reading this thesis to get a better feel for the Erowid Dataset\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "## 2.4 Intended Audience\n",
    "\n",
    "There is no single mold of an audience for which this thesis is written. A reader who is familiar with the basics of linear algebra and statistics, in particular the basic concepts and notations of vectors, matrices, and probability distributionsm will find the thesis straighforward to follow especially at times when there are slight conceptual leaps. A few sections contain more advanced probability theory, but those can be skipped without sacrificing the overall narrative structure of the thesis. No prior knowledge is assumed about techniques in machine learning and natural language processing (NLP), as the foundations will be presented from first principles by first relying on intuition and supported by mathematics for those interested. The final sections of the thesis consists of musings, extensions, and bonus content for the curious reader, and contain a few non-technical elaborations intertwined with technical notes.\n",
    "\n",
    "This thesis was as much as an empirical exercise as it was a theoretical exercise, written in a rather didactic manner as an self-exercise to test my own understanding of the material and ability to convey not only what was done but also the theory of why what was done is appropriate.\n",
    "\n",
    "Perhaps more important than prior knowledge for the reader is an innate curiosity, a child-like delight in exploring new frontiers; one who is drawn to principles of foundational interconnectedness and not afraid to embrace concepts that seem at first challenging for the unitiated, yet delightful for the persistent, may find this thesis enjoyable.\n",
    "\n",
    "A note: due to scope and time constraints, a few state of the art techniques such as finetuning BERT and GPT 2 language models were researched and planned, but not fully explored. I have left those sections in the table of contents, a deference to what was conceived, and a gentle reminder to future self that even a thesis is a work in progress and doesn't have to be absolutely \"perfect\".\n",
    "\n",
    "\n",
    "## 2.5 The Machine Learning Pipeline\n",
    "\n",
    "The field of machine learning is very broad, and indeed a complete overview of what machine learning would fill a complete tome and certainly out of scope for this thesis. That being said, there are a few generalizable insights and principles that are very important to mention. They are highly inspired by Professor Pedro Domingos's excellent paper, [A Few Useful Things to Know about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) and my work in progress computer science and statistics education\n",
    "- machine learning is about generalizing from examples\n",
    "- machine learning algorithms take _data_ as inputs, and attempt to find _patterns_ in the data, in order to accomplish some task(s) of interest\n",
    "- There are a few key types of problems in machine learning, and often the first step to translating theory into practice is recognizing the type of problem you're trying so solve:\n",
    "    (1) **Regression** problems are about predicting _continuous_ outcomes **y** from a collection of continuous or categorical features **X**, such as predicting housing prices (**y**) from \\[ square footage (continuous feature), and whether it's in a good neighborhood (categorical feature) \\] (**X**); Regression is a _supervised_ problem because it requires labelled training data **y**\n",
    "    (2) **Classification** problems are about predicting discrete class labels **y** from a collection of continuous or categorical features **X**, such as classifying whether a person has a heart condition (discrete **y**) based on \\[ blood pressure (continuous feature), and smoking habits (discrete feature) \\]; Classification is a _supervised_ problem because it requires labelled training data **y**\n",
    "    (3) **Clustering** problems are about finding clusters of observations that are most ‚Äúsimilar‚Äù to each other, such as finding customers who are similar, or mushrooms specifies are most closely related to each other; Clustering is an _unsupervised_ problem because it doesn't require labelled training data **y**: the patterns are discovered directly from the observations.\n",
    "\n",
    "\n",
    "\n",
    "This thesis employs the [CRISP-DM](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.198.5133&rep=rep1&type=pdf) (Cross Industry Standard Process for Data Mining) pipeline, which is a widely used open standard by professional data scientists as a standard data mining approach\n",
    "\n",
    "![CRISP-DM Model](./images/infographic_CRISP-DM.png)\n",
    "[`CRISP-DM Model Infographic Source`](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "\n",
    "< todo: i'm getting a little too distracted here >\n",
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZglPg2jMFLy"
   },
   "source": [
    "# 3. Data Acquisition and Origins\n",
    "\n",
    "\n",
    "\n",
    "This data can be structured (like an excel spreadsheet) or unstructured (like documents). It can be in the format of survey data with numbers, checkboxes, slidings scales; the fields can be numeric (continuous numbers) or categorical with discrete levels (such as Gen I, Gen II, Gen III pokemons). The data can even be images, videos, text, or music. Virtually anything you think of can be used as data to machine learning algorithms, _provided_ they are representation in the appropriate format that is suitable for the algorithm of choice, and the problem at hand. Indeed, the **quality** and **quantity** of the data often determines the performance of the machine learning system. If your data is very biased, your results will be biased. If your dataset is small, your machine learning model\n",
    "\n",
    "\n",
    "\n",
    "`TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skPb_heUE6r9"
   },
   "source": [
    "## 3.0 Switching Between Local Jupyter and Colab Development Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "7b4Wy64SXF63",
    "outputId": "5b7568a8-f5c9-4d08-f07f-59c89fa0b935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/content/gdrive/My Drive/UPenn/UPenn_fall-2019/[EAS 499] Senior Thesis f2019'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toggle between local and Google Colab Development Environment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# if developing locally, set is_colab to False\n",
    "is_colab = True\n",
    "root_path = '/content/gdrive/My Drive/UPenn/UPenn_fall-2019/[EAS 499] Senior Thesis f2019' if is_colab else \".\"   #change dir to your project folder\n",
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "liaQqk7LKYXv",
    "outputId": "f65cdda3-2c63-47b7-a260-48ed1195b882"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1c20d6e0c445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# crashing Google Colab on purposes to get more RAM\n",
    "# (note: abusing here, sorry Google!!)\n",
    "d=[]\n",
    "while(1):\n",
    "  d.append('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xZgXG6WMFLy"
   },
   "source": [
    "## 3.1 Data Sources\n",
    "\n",
    "With v0 of EveryQualia, we downloaded the May 2012 Erowid Archive from `http://de1.erowid.org/erowid_archives/` (most recent as of Dec 9, 2019). This download contains the entire mirror of the Erowid website from May 1st, 2012, and contains 19924 experience reports, each as its own html document. `@Yevbar` maintains a mirror of all the trip report raw files on his github, at [Old Erowid Scraper](https://github.com/yevbar/old-erowid-scraper)\n",
    "\n",
    "\n",
    "One of the earliest and biggest unknowns for this thesis was whether there was going to be enough data on which to perform any meaningful analysis, since machine learning algorithms typically require a sizable amount of data. Early conversations with Lyle Ungar and Chris Callison-Burch suggested that a corpus of 200-300K reports was desirable. Since psychedelics are still, as of Dec 9, 2019, a Schedule 1 controlled substance, reports of psychedelic expereinces are not very easy to find, as every published account after the early 1970's is tantamount to admitting a federal. As such, a considerable amount of effort that is not necessarily documented here was really looking for sources for psychedelic trip reports. Feelers to the Johns Hopkins team revealed that the session reports in their clinical trials are private and not available to use for this thesis, and conversations with friends in the industry regarding an upcoming Johns Hopkins global survey study that would be the largest of its kind in the world revealed that it was still in IRB stages. Conversations with researchers at University of Oxford, who are also doing fascinating work with natural language processing on transcendent experiences accounts, revealved that their studies are based on in person interviews, which the researchers personally and painstakingly conducted, and are based on the transcriptions of in person interviews. Further details about this study cannot be shared in this thesis as their work serves as the basis of an upcoming PhD thesis, the details of which is not yet public as of this writing. And since in parallel to Penn Society for Psychedelic Science, a couple of us from Penn, Harvard, Princeton (and 14 other schools, as of Dec 9, 2019) are growing the [Intercollegiate Psychedelics Network (IPN)](https://ipnpsychedelics.org), multiple conversations were had with other student leaders about setting up an Intercollegiate Psychedelics Archive, or IPN Archives, containing not only our respective club processes but also be a central source of aggregating trip reports from students. Indeed, there were some initial efforts of making this happen, but due to legal considerations, we decideed to hold off for now until the legal climate was more amenable, for our personal safety and the wellbeing of our respective student organization.\n",
    "\n",
    "And such, efforts were directed to other online sources, such as various reddit forum, and informal messaging boards, but none of which contained structured enough data at high enough quantities as Erowid. David Yaden, a PhD candidate with Martin Seligman and Lyle Ungar, is doing fantastic work in assembling [The Varieties Corpus](https://www.varietiescorpus.com/corpus), which is not yet available to the public and researchers. A natural extension of this thesis is to incorporate David's data in future work.\n",
    "\n",
    "As my conversations with Yev progressed, he decided to try once again scraping Erowid live. Note that Erowid has prety strict guidelines for crawlers, including:\n",
    "- Run your spider only during off-peak hours (between midnight and 5am PST)\n",
    "- Limit your robot to 10 file hits per second during off-peak hours and 2 file hits per second during peak hours.\n",
    "- Robots must properly identify themselves. Archiving and spidering robots that do not identify themselves or that falsify their agent name will result in your IP(s) being blocked. Please set up your spider to do very short runs (less than 100 pages) and test it while you monitor the robot before simply launching it against our site and walking away.\n",
    "\n",
    "Running a crawler at full speed (10 pages / second) non stop, crawling all 35,000 trip reports currently live (as of Dec 9, 2019), would take a full hour. This doesn't seem like a lot, but in reality the crawling limits are much lower, including a 1000 pages / day limit, and in practice we got repeated blocked and it often took several hours to crawl 1000 pages. We entertained the idea of spinning up multiple EC2 instances and using a distributed architecture to crawl Erowid (Yev actually wrote the code, and the idea was that each crawler would have a different IP address to \"fool\" Erowid). But this would be dishonest, and having not received an email reply from Erowid + a variety of other reasons, I decided to not to proceed with scraping the entirety of Erowid. Indeed, this is an extension for further research.\n",
    "\n",
    "Some important links about using the Erowid Experience Vaults:\n",
    "- [Erowid Spider Guidelins](https://www.erowid.org/general/about/about_archives1.shtml)\n",
    "- [About the Erowid Experience Vault](https://www.erowid.org/experiences/exp_about.cgi)  ([`archive-20191208`](http://web.archive.org/web/20191208050510/https://www.erowid.org/experiences/exp_abou.cgi))\n",
    "- [Erowid Experience Vault Review Process](https://www.erowid.org/experiences/exp_info1.shtml)  ([`archive-2019-1208`](http://web.archive.org/web/20191208064512/https://www.erowid.org/experiences/exp_info1.shtml))\n",
    "- [Erowid Downloadable Archives Collection](http://de1.erowid.org/erowid_archives/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w2-CsbZNMFLz"
   },
   "source": [
    "## 3.2 Data Scraping Procedure\n",
    "\n",
    "In the end, we decided to stick with the corpus of EveryQualia v0, which was the May 1, 2012 archive of the full Erowid website from `http://de1.erowid.org/erowid_archives/`. To extract the data into a usable format, we unarchived the disk image, mounted it to disk, and navigated to the folder containing 19934 html files (see [here](https://github.com/yevbar/old-erowid-scraper/tree/master/trippy-search/experiences)). The following script was used to extract the report body, report title, and report substance(s) into a csv file:\n",
    "\n",
    "```python\n",
    "import csv\n",
    "from lxml import html\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def trip_reports_to_csv():\n",
    "    trip_csv = \"trips.csv\"\n",
    "    columns = [\"report\", \"title\", \"substance\"]\n",
    "    mypath = \"experiences\"\n",
    "    onlyfiles = [join(mypath, f) for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "    with open(trip_csv, \"w\") as trip_csv:\n",
    "        trip_writer = csv.writer(trip_csv, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        trip_writer.writerow(columns)\n",
    "        for f in onlyfiles:\n",
    "            with open(f, \"r\", encoding='utf-8', errors='ignore') as f_:\n",
    "                try:\n",
    "                    page = f_.read()\n",
    "                    tree = html.fromstring(page)\n",
    "                    report = ''.join(tree.xpath('//div[@class=\"report-text-surround\"]/text()')).strip()\n",
    "                    title = tree.xpath('//div[@class=\"title\"]/text()')[0]\n",
    "                    substance = tree.xpath('//div[@class=\"substance\"]/text()')[0]\n",
    "                    trip_writer.writerow([report, title, substance])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "trip_reports_to_csv()\n",
    "```\n",
    "\n",
    "The package [lxml.html](https://lxml.de/lxmlhtml.html) was used to match xpaths, and credits to Yev Barkalov for the xpaths that correctly matched the correct div elements for desired sections. In all, the script took about 1 hour to run on our hardware (rough estimate). It should be noted that there was certain demographics information such as gender, age, and publication date that were available, but were not included in the analysis of this thesis. These certainly hold promise for future research directions.\n",
    "\n",
    "\n",
    "\n",
    "Lessons Learned:\n",
    "- a lot of work happens organically, and cannot be forced\n",
    "- Iterative mindset is key\n",
    "- Machine learning is about finding generalizable patterns\n",
    "- The algorithms chosen can significally impact conclusions. Many papers use black box algorithms and present results as if they are facts, but initial conditions and design decisions can very quickly change \"facts\" into approximations.\n",
    "- Machine learnign is largealy about finding generalizable patterns from data\n",
    "- todo: see the 12 lessons paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gC1P39U0MFL0"
   },
   "source": [
    "## 3.3 The Data: Raw and Unfiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtgGt_FXMFL1"
   },
   "source": [
    "In total, there were 19924 substance, report having a `report` body, a `title`, and a `substance` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "_-pvViyjMFL2",
    "outputId": "69b9d07d-b3aa-40b2-b06b-a1537fcac863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19924 entries, 0 to 19923\n",
      "Data columns (total 3 columns):\n",
      "report       19915 non-null object\n",
      "title        19924 non-null object\n",
      "substance    19924 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 467.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trips_raw_filepath = '{}/trips.csv'.format(root_path)\n",
    "pd_tripReports = pd.read_csv(trips_raw_filepath)\n",
    "pd.set_option('display.width', 80)\n",
    "pd_tripReports.shape\n",
    "pd_tripReports.info()\n",
    "# TODO: Make this pretty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "Yhzm5koOMFL6",
    "outputId": "d21789a1-c037-4ba3-cd2b-6012365c4733"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>title</th>\n",
       "      <th>substance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After having had some success with other forms...</td>\n",
       "      <td>Sideways World</td>\n",
       "      <td>Salvia divinorum (5x extract)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me and a couple of my buddies decided one nigh...</td>\n",
       "      <td>Physical Wellbeing = Crucial</td>\n",
       "      <td>Mushrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My girlfriend and I had been saving the methyl...</td>\n",
       "      <td>The Artful Dodger</td>\n",
       "      <td>Methylone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just want to warn anybody taking Lithium (or...</td>\n",
       "      <td>Seizure Inducing Combo</td>\n",
       "      <td>LSD &amp; Lithium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have had several attempts before this to bre...</td>\n",
       "      <td>Enlightenment Through a Chemical Catalyst</td>\n",
       "      <td>5-MeO-DMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              report  ...                      substance\n",
       "0  After having had some success with other forms...  ...  Salvia divinorum (5x extract)\n",
       "1  Me and a couple of my buddies decided one nigh...  ...                      Mushrooms\n",
       "2  My girlfriend and I had been saving the methyl...  ...                      Methylone\n",
       "3  I just want to warn anybody taking Lithium (or...  ...                  LSD & Lithium\n",
       "4  I have had several attempts before this to bre...  ...                      5-MeO-DMT\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 6 entries of the data\n",
    "pd_tripReports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "3HqUZg4NMFL8",
    "outputId": "0ab5b587-54d5-428e-df78-a815ba499b28",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>title</th>\n",
       "      <th>substance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19919</th>\n",
       "      <td>First off - I want to answer the question, 'Do...</td>\n",
       "      <td>My Truth About Seeds...</td>\n",
       "      <td>Morning Glory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19920</th>\n",
       "      <td>It was actually my mom who got me on Ritalin. ...</td>\n",
       "      <td>The Help I Hate to Love</td>\n",
       "      <td>Methylphenidate (Ritalin)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19921</th>\n",
       "      <td>This being a hopefully useful report on low-do...</td>\n",
       "      <td>Meeting the Spirit Plus a Territorial Dog</td>\n",
       "      <td>Peruvian torch cacti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19922</th>\n",
       "      <td>This journal shall hopefully be a guide for me...</td>\n",
       "      <td>A Journey Journal</td>\n",
       "      <td>Salvia divinorum (5x extract)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19923</th>\n",
       "      <td>I ingested 12-15mg of 2ce orally.\\n\\n\\n\\nI dum...</td>\n",
       "      <td>A Late Winter's Trip</td>\n",
       "      <td>2C-E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  report  ...                      substance\n",
       "19919  First off - I want to answer the question, 'Do...  ...                  Morning Glory\n",
       "19920  It was actually my mom who got me on Ritalin. ...  ...      Methylphenidate (Ritalin)\n",
       "19921  This being a hopefully useful report on low-do...  ...           Peruvian torch cacti\n",
       "19922  This journal shall hopefully be a guide for me...  ...  Salvia divinorum (5x extract)\n",
       "19923  I ingested 12-15mg of 2ce orally.\\n\\n\\n\\nI dum...  ...                           2C-E\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last 6 entries of the data\n",
    "pd_tripReports.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "cyu-XQGuMFL_",
    "outputId": "31f3b4a0-1ef5-4409-8a34-0e7d4edc32b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: a journey journal\\n Substance: salvia divinorum (5x extract)\\n Report: this journal shall hopefully be a guide for me into better understanding the effects of salvia divinorum. like many journals i have attempted to write, they all end in lack of desire to finish them. i plan to make this journal different. it is my hope that i describe each experience in the fullest extent to better understand the unique nature of this plant. \\n\\n\\n\\n9/19/2002\\n\\n\\n\\ni received two grams of 5x extract. i was excited for my first experience and called over my friend to watch over me, since it was my first time and i did not know how i would react.\\n\\n\\n\\ni prepared my water pipe and put a miniscule amount into the bowl, and lit it up. i inhaled and held the smoke, which wasnt much. after exhaling, i did not feel much until about a minute, when i felt a mild buzz but not much else. i was disappointed but decided to try more. i put a equal amount of salvia in the bowl and lit up again. i did not feel anything much.\\n\\n\\n\\ni decided that i was not doing enough, so about ten minutes later; i filled the bowl with a decent amount and lit it up. upon exhaling, i did not feel anything right away, but then it hit me. i was amazed at this feeling and commented that i could not believe that this drug was legal. \\n\\n\\n\\nall of a sudden, my body felt as if it were covered in ants. i was very itchy and sweaty and threw off my over-shirt and was about to take off my t-shirt when i said, no, this is just the drug. i pulled my shirt back down. i began to talk in a high-pitched voice, saying nonsensical things.\\n\\n\\n\\nmy perspective radically changed. i was standing at the edge of a cliff with my back to the precipice and beyond it was nothing. nothing existed beyond the cliff. my universe shrunk till all it included was my field of vision in front of me. my universe was only 50 square feet, and that seemed fine. i was still in my room, but i did not recognize it as such. i was in a different place, not unpleasant but interesting/weird (for lack of a better word). i stayed in that universe till i came down.\\n\\n\\n\\nmy next conscious thought was that of me cleaning out my pipe in the sink in preparation for another trip. my friend brought a video camera to record this trip. i went back to my room and chilled for a few minutes and then refilled the pipe and lit up. after two inhales to clear the chamber, i did not have much perspective shifting, but mostly an extreme stoning effect similar to weed but noticeably different. i mentioned how i was amazed that my second exhale did not contain that much smoke. i also had a vague feeling that as i was talking, i was part of a childrens television show, explaining things to the viewers. i also mentioned the television feeling. i began to come down then, and i turned on my cd player and laid on my bed, attempting to have visuals, but to no avail.\\n\\n\\n\\ni was feeling mildly disappointed, so i decided to inhale some nitrous. it gave me the typical nitrous rush, and i commented how i wished i could feel like this all the time, but neither drug intensified the other. \\n\\n\\n\\nthe effects wearing off, i thought about what i had experienced. it was a nice trip, but not quite a introspective and spiritual experience. however, i do feel that i started on a good level and plan to continue exploring consciousness with salvia.\\n\\n\\n\\n9/21/2002\\n\\n\\n\\nit has been two days since my first experience with salvia. i enjoyed my first experience immensely, but i found i was a tad disappointed that i did not experience any introspective or mind-altering states.\\n\\n\\n\\nabout 3:30pm, i load the bowl almost to the top with salvia. i clear the chamber and wait. i feel the familiar itch/sweat that i expect will be common to all my salvia experiences. it is not wholly unpleasant, but it is more comfortable without a shirt and a/c.\\n\\n\\n\\ni felt the salvia begin to take hold. however, it is only a mild stoning effect and i decide that i must smoke another bowl as to achieve the desired effects. i loaded up the bowl with more and smoke it fully.\\n\\n\\n\\nthis is where my trip truly starts. when i finished exhaling, i stared at the windowsill and a feeling of dj vu came over me. i couldnt help but feel that i had done this (smoking salvia) for many years. i began to feel like i was experiencing a childhood all the way through to fatherhood in an incredibly short amount of time. i had conversations with my best friend as we grew up and played together. as we got older, we each had children of our own and reminisced about the old days and the fun we had and the fun our children were having now.\\n\\n\\n\\ni then stopped and tried to fight the experience. i said out loud, what are you doing? and other things of the like, not fully understanding what i was experiencing. i then decided not to fight it and laid down on my bed and closed my eyes.\\n\\n\\n\\nas i laid there with eyes closed, a multi-colored spiral/vortex began to swirl in my minds eye. it made one loop and started to form another loop underneath the previous one when i realized that this loop was alive and many intelligent organisms lived within it. the loop continued to move and i could hear the organisms talking.\\n\\n\\n\\na mother was explaining to her child how each loop represented the generation before them. i understood that i was witnessing new births and that i was the creator of all these beings. the mother kept explaining how old the generations before them were, but none were as old as the ancient entity. i was that entity. i was the creator, the vessel for these beings. the mother told her child that one day, she might take him to talk to the ancient entity, but that he might not talk back. as i listened to me being described as an ancient entity, i began to think of my own beginnings, when i played with my friend and my growing up. during the conversation about meeting the entity, the loop stopped spiraling and turned into a ribbon, resembling a sine wave, and continued to move in that fashion.\\n\\n\\n\\nafter a while, i began to get annoyed at these creatures, feeling that they had no right to be inside me and using me as a vessel for their own existence. i opened my eyes and grabbed at paper and pen, and with great difficulty wrote these words:\\n\\n\\n\\nafter i wrote this, i laid back down and began to come off of the plateau. i laid there for a bit, attempting to absorb what i had just experienced, but i was still heavily under the influence.\\n\\n\\n\\ni thought that this would be a perfect time for some nitrous, so i prepared a balloon and inhaled it. i did not think i inhaled enough, but then i peaked and felt as if a giant hand was bending me forward and turning me upside down. i straightened up and came off the nitrous, in a good, but exhausted mood. i wandered around my room for a bit and then called my friend, stating that i had an amazing trip, but was not yet ready to describe it.\\n\\n\\n\\ni laid back down, almost completely devoid of any effects, and thought about what i had just experienced, and realized that i had my first, and amazing, introspective trip. i felt so special and so blown away. i dozed for about a half an hour and woke up and watched some tv. i got dinner and am now writing this. i feel great and the only word that comes to my mind is wow. i have so much respect for this plant and am grateful for the insight that it has granted me.\\n\\n\\n\\n9/24/2002\\n\\n\\n\\nat 2:30, i load up and attempt to smoke but accidentally blow half the bowl out. i gathered up most of it, but i must be very careful from now on, since the salvia is such a light herb. i take two full hits.\\n\\n\\n\\nright after the first hit, i am aware of the effects and the itch/sweat starts. i strip off my shirt after the second exhale. the psychotropic effects begin.\\n\\n\\n\\ni become aware that the waterpipe is alive. it is a person and it talks to me and says that i must put it on the ground. i comply and as i do this, i realize that my entertainment system plus several other random points in space have become alive as well. they start to make fun of me and comment on what a sloppy job i did at moving the pipe to the ground. they laugh at me and i wonder what has made them act this way. i talk back at them, telling them to shut up and mind their own business. it is at this point that i realize that the drug has hold of me. \\n\\n\\n\\ni close my eyes and a wide brownish loop forms, with vague edges. it appears to sparkle with electricity, but the electricity itself is also brown. i see that the loop is actually made up of eight distinct pieces and that each piece represents each different entity living in my room (the ones that were laughing at me). all of a sudden the loop breaks towards the top at one of the junctions between two pieces. the loop becomes a snake-like thing and moves/loops away, out of my field of vision. it then comes back into my field of vision and rushes at me and goes inside my head.\\n\\n\\n\\nthe snake-like thing takes control of my brain and begins to tell me to do things. it specifically tells me to masturbate. i cannot help myself and i am aware that i quickly undo my pants and begin the deed. i kneel onto the ground, and after what seemed like an indefinitely long time, i felt a light tingling as i ejaculated into the trashcan. there was very little feeling involved and compared to other masturbation sessions, it was at the bottom of the scale.  \\n\\n\\n\\ni laid back down on the bed and listen to the music. i have huey lewis its alright playing. as i listen, i am aware that i had picked out one of the voices within the song and was listening specifically to his part. the rest of the music was there, but i could hear every note of that particular part, which i am almost sure was the baritone. i then play piano man by billy joel and i notice that i have picked out the bass guitar part, and have just been listening to that.\\n\\n\\n\\nas i come off the plateau, i sit up and realize that i have a paper due tomorrow and feel compelled to find the information regarding it. i begin searching through my notebooks and papers, finding myself unable to stop moving for five minutes. i am rapidly searching, but to no avail. i clean up all the notebooks and then check the computer and find the information.\\n\\n\\n\\nthe after-effects slowly tapering off, i wanted to clean my pipe, but my roommate was in the kitchen, so i watched tv until he was gone. i cleaned the pipe and begin typing this.\\n\\n\\n\\nas i become more experienced with salvia and use this one drug repeatedly, i begin to find certain aspects that are similar to each trip. i get the itch/sweat seconds after smoking but it quickly dissipates. my perception of reality changes at the plateau. each trip has yielded different perceptual changes, but that is to be expected and i hope that two trips will never be the same. i also realize that after the plateau, my memory is a bit fuzzy. i cannot recall very well what i did on the comedown. during the after-effects, i begin to understand what i have experienced. \\n\\n\\n\\nregarding the effects, i do not realize am i feeling them until a minute into the effect. it is almost as if two brains are working simultaneously. my conscious brain is having the trip while the unconscious brain realizes that something is happening a bit later.\\n\\n\\n\\nsalvia leaves a sort of heaviness in my brain, kind of like after a huge test. i hypothesize that this is due to the fact that my brain did work hard, attempting to understand and comprehend the world around it while under the influence of this wonderful conscious-altering drug.\\n\\n\\n\\n9/24/2002\\n\\n\\n\\nthis is not a report of a trip that i had, just my observations with others were on salvia. j, d, and r were to partake of some this evening. \\n\\n\\n\\nj went first and after a monster hit, started coughing and then it must have hit him because he started rubbing his tongue and then licking himself on the sleeve. i encouraged him to take the rest, so he did and remained in a weird state for about 15 minutes. as i was packing a new bowl, he threw an empty box of cigarette at me. it hit me in the head and he apologized profusely and with great sincerity, more than i have ever seen him do. he said he felt like he was more stoned than he ever was for about 5 minutes.\\n\\n\\n\\nd took some and just sat there, eyes glancing around. he mentioned that it felt like j was a doll and j was talking to him, which apparently seemed very weird for a doll.\\n\\n\\n\\nr smoked next and stood up with great difficulty and mentioned something about going out to the lake. he addressed us as grandma and grandpa. he also said, and j confirmed, that he felt leaden to the porch, like he couldnt move.\\n\\n\\n\\nthat was the extent of their trips to the best of my knowledge because i did not discuss it with them in length, but i now know that people have similar reactions to salvia. \\n\\n\\n\\n10/11/2002\\n\\n\\n\\ni received 5 grams of 5x extract a week back, so i prepared and smoked a large bowl of my new batch of salvia (approximately two hits). i felt the familiar itch/sweat, but nothing much else. i was a tad disappointed that i did not feel much. at one point, i think i was mildly aware that my room was a penthouse at the top of an expensive apartment complex, but i cannot be sure. mostly, i was attempting to force the effects to happen. i think i need to construct a larger, better bong so that i can take more salvia in. perhaps this batch isnt as potent as the last one. i will build a new bong and attempt to have a better trip this week.\\n\\n\\n\\n10/31/2002\\n\\n\\n\\ni had a great trip with my new bong. also had s take a few hits and he had a good trip as well. this trip was hard to understand, but i will try to describe it. i was looking out the window and my frame of view contained the lake, horizon, lava lamp, my arm, and s in the chair. i did not recognize them at for what they were, but as one giant slate as if they were painted on it. i just stared at this canvas and it seemed like my arm was connected to the horizon and an invisible energy was flowing from it into the horizon. s moved and i became upset because it felt like the canvas was ripped, so i yelled for him to stop moving around. i slowly came back to reality and did not do much reflecting, but watched s take a good couple hits, which obviously sent him somewhere because he asked wheres my brother? he thought he was back in his old house. he came back to baseline and we talked about our trips for fifteen minutes, reflecting on how pleasant the trip was and how weird of a drug salvia is.\\n\\n\\n\\n12/02/2002\\n\\n\\n\\nk and i smoke one bowl together; a hit each. no real discernable effects on my part except a heaviness in my right hand that lasted for about 20 seconds. k just stared out the window for about half a minute. i think i am the only one who can take a hit of salvia without coughing. everyone who has smoked it, minus s, has coughed, and most of them are veteran weed smokers. i am sure the lack of effects was due to the extremely small amount i smoked and not the quality of the salvia. it is a few months old, and even though i keep it in a glass jar, i am not sure how long salvinorin a stays active. when i have a good chunk of time to myself, i will do a massive salvia trip, but since finals are arriving, it will have to wait. \\n\\n\\n\\n4/25/2003\\n\\n\\n\\nit has been a while since i smoked any salvia. on this particular night, i am tripping on 3.5 grams of mushrooms and 40mg of amt. i have done about 20 nitrous hits as well. i put ein deutsches requiem on the stereo and take a nice fatty hit.\\n\\n\\n\\nit hits me right away. the typical salvia mindshift occurs and once again am staring at a foreign horizon. my entertainment center has become a mountain range, shifting in color and growing in intensity. the music begins to take me to a place of unwant, so i attempt to change it. i find that i cannot recognize the cd remote and fumble with it for a bit before falling to the floor.\\n\\n\\n\\ni begin to talk. at the time it made perfect sense to me, but after a minute of talking, i realized i was just saying gibberish. i shake my head and try to think of what i was saying but then a new phrase pops into my head: wagons. a fraction of a second later i add to words flowers for cup, so the phrase became flowers for cupwagons. what this means i have no idea. this is all i can remember form the salvia portion of the trip.\\n\\n\\n\\nperhaps the phrase was induced by a song (vangelis messages) i was listening to, and although no lyrics form the phrase, the rhythmic patterns of the song could have formed the words in my head. \\n\\n\\n\\nas i type this entry almost four months later than the experience, that phrase has crossed through my mind many of times, for i feel it is a connection to my subconscious. i will discover the true meaning of this phrase and the link i share with it and salvia. salvia divinorum is the weirdest psychedelic i have tried to date, and i feel that there is still much to be learned.\\n\\n\\n\\n7/20/2003\\n\\n\\n\\non a day off from camp, i sit on the deck with my brother smoking a bowl of nugs. we decide to smoke a small bowl of salvia each. he hits first and sits in silence as i inhale. the mindshift occurs and my own body becomes a mountain range shaped like an upright backwards c. with every word i speak, invisible energies shoot from the top of the c to the bottom, and i imagine what it would be like if i was listening to music, particularly vangelis messages (the song i feel that i share a transcendental connection with). \\n\\n\\n\\ni turn to my brother and start talking. i am talking nonsense but the words make sense. the friday wagons are coming, b. you know they areyou know they are. he laughs and mentions how fucked up he feels and that he doesnt know what the hell is going on; internal as well as external. \\n\\n\\n\\nthe next day, i realize that the word wagons is somehow connected with salvia and myself. friday and flowers are just two beginnings for the phrase.\\n\\n\\n\\ntyped on the same day as the previous entry. it feels a bit unfair to the journal and myself to be sharing these entries having months to think about the experiences. however, i share them in better understanding this strange phrase. more salvia is needed and perhaps a new prefix will surface for the wagons phrase. i must make sure that it truly surfaces and is not just forced into existence by my desire to discover.\\n\\n\\n\\ni am starting a new journey this year. i have assimilated most of my experiences from the previous year and am ready to delve deeper into myself. i know who i am. now i must go beyond knowing. i will discover what that is this year. \\n\\n\\n\\n9/1/03  12/29/03\\n\\n\\n\\nas i have gotten lazy, i did not record the salvia experiences this year after they occurred. however, numerous trips were taken and were quite similar since the dosage and source did not differ.\\n\\n\\n\\ni smoked salvia about eight times. each trip took place in my chair that although not aesthetically pleasing, serves its purpose well. they were fun but nothing spectacular. i learned that flowers for cupwagons means nothing and that alcohol and salvia do not mix. this semester sucked ass. i am tired. i am depressed.'"
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example trip report\n",
    "report19922 = pd_tripReports.loc[19922]\n",
    "report19922[\"title\"]\n",
    "\n",
    "\"Title: {}\\n Substance: {}\\n Report: {}\".format(report19922[\"title\"], report19922[\"substance\"], report19922[\"report\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPpyYInoMFMB"
   },
   "source": [
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4msJphSMFMC"
   },
   "source": [
    "# 4. Data Cleaning and Exploratory Data Analysis (EDA)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8fV7T4fMFMC"
   },
   "source": [
    "## 4.1 Machine Learning, and the Natural Language Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48jhuMzMMFMD"
   },
   "source": [
    "![The NLP Pipeline](./images/infographic_nlp-pipeline.png)\n",
    "[`Infographic Source`](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3oCmCOpMFME"
   },
   "source": [
    "Now that we have a collection of documents, i.e.: transcendent experience reports, obtained from the Erowid Experience Vaults, we can continue with the remainder of our analysis. Nautural Language Processing (NLP) is a subfield of machine learning, and deals primarily with unstructured textual data. The above graphic outlines the high level standard workflow for an NLP project, which includes:\n",
    "- `Text Pre-processing`: since the web is messy and unpredictable, we need to clean our data appropriately and perform some pre-processing to prepare our data for subsequent analysis\n",
    "- `Text Parsing & Exploratory Data Analysis`: what structures exist in the text, such as parts of speech or named entities (e.g.:  `TOM` can be an organization (`ORG`) or a person (`PER`) depending on context) - text parsing helps us construct better understand underlying structures of the text; exploratory data analysis (EDA) is the process of playing and understanding the data. It's as if the data is a person, and you're asking it questions over coffee to get to know them better. No way of conversation is particularly better than another, and data scientists can rely on tried and true techniques or get creative.\n",
    "- `Text Representation & Feature Engineering`: as mentioned in section 2.4, the chosen _representation_ of the data, as well as the features extracted from the raw data, are extremely important. Just as a landscape artist needs to choose the angle and form of the mountain he is painting, selecting the scenes and objects (features) to include to represent spring time, a data scientist needs to choose the methods of representing the text, and select features to highlight. More frequent than not, a \"feature\" in natural language processing is a vector of integeters or real numbers (e.g.: counts of words per document). \n",
    "\n",
    "- `Modeling and / or Pattern Mining`: depending on the task at hand and data (i.e.: metadata, labels etc.) available to us, we can use supervised or unsupervised techniques, topic modelling techniques, clustering to find similar groups, dimensionality reduction to visualize the data. The remainder of the thesis touch upon the above techniques, with particular emphasis on topic modelling.\n",
    "\n",
    "- `Evaluation & Deployment`: Once we have a model, we need to have a way to understand how well it performs. That is the topic of Evaluation, a crucial part of the natural language processing and machine learning pipeline. Without sound evaluation metrics, we have created a model, not a _good_ model. Indeed, unsupervised learning techniques are typically hard to evaluate, and in this thesis we propose a few metics for topic modelling, and note that improving evaluation metrics is a further area of research.\n",
    "\n",
    "\n",
    "Note that the workflow above has been drawn linearly, but the NLP pipeline in reality has many loops ‚Äî extracting features, finding patterns, and asking questions about the data do not occur in a vaccuum. We present our work below linearly for structure, but the process of getting to know the data and writing this thesis was far from linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjTBEHInMFMF"
   },
   "source": [
    "## 4.2 Text Preprocessing\n",
    "\n",
    "\n",
    "### 4.2.1 Overview of Text Preprocessing\n",
    "\n",
    "Text Preprocessing refers to a broad class of techniques that pre-processes the text before any analysis is done. It's analogous to a rough sieve filtering out the largest rocks, cleaning and normalizing the corpus on a high level. Note that there is no \"canonical process\" to perform text pre-processing. In practice, the types of preprocessing done depends on the type of data, the quantity of data, trial and error. Steps marked with `*` are those that are most frequently used.\n",
    "\n",
    "- `* Remove extra spaces`\n",
    "- `* Make all text lowercase`\n",
    "- `* Remove stopwords`\n",
    "- `* Tokenization`\n",
    "- `* Normalize accented characters`\n",
    "- `* Remove special characters`\n",
    "- `Remove rougue `html`/`xml` tags`\n",
    "- `Remove numbers`\n",
    "- `Expanding contractions`\n",
    "- `Stemming`\n",
    "- `Lemmatization`\n",
    "\n",
    "For those interested, here is some more detail on each step:\n",
    "- `* Remove extra spaces`: \n",
    "> a whitespace is not just a whitespace in NLP; they can be `space` or `\\tab`, and depending on the parse, one needs to deal with newline characters such as `\\n` or `\\r`. Stripping away unecessary whitespace to find word boundaries is a crucial step in pre-processing. This is most commonly done with `str.strip()`\n",
    "- `* Make all text lowercase`:\n",
    "> should `wow` be treated the same as `WOW`? In sentiment analysis tasks, capitalizations matter very much, as the presence of caps often connote delight. In other tasks that are more informational based, capitalization add noise and unwanted dimensionality. This is most commonly done with  `str.lower()`\n",
    "- `* Remove stopwords`: \n",
    ">words such as `a` and `the` and `and` and `is` occur so frequently in the english language that they often don't add any meaningful information to the task at hand, so we pre-empt noise by adding them to a `stopwords` list and remove them from our corpus. Care must be taken with stopwords as by adding a word to a stopwords list we are a priori claiming that word does not add value. Indeed, men tend to use `the` more than women, so a classifier that predicts gender would be well served to include the presence or counts of `the` as a feature (Lyle Ungar).\n",
    "- `* Tokenization`: \n",
    ">The process of dividing a big chunk of text into individual units. In english, this is fairly straightforward: use individual words as tokens, or individual characters if we're building a character-level model. We also have to consider whether to make puntuations tokens, and whether we include any ngrams, e.g.: a bigram such as `cat jumped` or trigram such as `you are beautiful` separately as tokens, in addition to the individual words `cat`, `jumped`, `beautiful` etc. Tokenization is considerably harder for languages that don't have spaces, such as Chinese. In practice, we use built in tokenizers such as `nltk` or `spaCy`\n",
    "- `* Normalize accented characters`:\n",
    ">e.g.: `√°` to `a`. Useful for english, but for other languages accented characters may have special meaning and usage. This is most commonly done with `unicodedata.normalize` and built in `sklearn` functionality\n",
    "- `Remove rougue `html`/`xml` tags`:\n",
    "> websites are written in `html`(`xml`), `css`, and `js`; If a website is a human being, `html` is the skeleton and meat, `css` is the makeup, and `js` provides the instruction for interactivity. As we know, sometimes bones break, but the human is still functioning ‚Äî likewise, an `html` or `xml` file might be slightly corrupted, but the browser will still render the webpage. After we've scraped the webpage, we remove those `html` tags that are extraneous, so all we are left is the meat. This is most commonly done with packages such as `Beautiful Soup`\n",
    "- `Remove numbers`:\n",
    "> If we were designing a system to answer questions like `what is the population of norway`, numbers in the original data set is crucial. Other times, number add only noise. Depending on the problem at hand, the data scientist makes an informed decision of whether to remove numbers This is most commonly done with `regex`, or regular expressions, patterns that \"match\" to types of text. For example, the `regex` that matche to a 1 or more numbers is `[0-9]+`\n",
    "- `Expanding contractions`:\n",
    "> e.g.: `don't` to `do not`. Once again, this is optional and depends on the task and availability of data. Indeed, Lyle Ungar remarks this has been highly optional in his work.\n",
    "- `* Remove special characters`:\n",
    "> do special characters such as `<`, `!`, `#` add value? Indeed, sentiment analysis algorithms often look for `!`, and if we were analyzing twitter data, `#` are important. In other cases, special characters are treated as gibberish and removed.\n",
    "- `Stemming`:\n",
    "> `Stemming` refers to the often crude procedure of removing the ends of words to find the \"word stem\"; e.g.: walks -> walk;`Porter Stemmer`, `Lovins Stemmer`, `Paice Stemmer` are three examples of commonly used stemming algorithm. Some `Porter Stemmer` rules are found below ([source](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html))\n",
    "> ![Stemming Example](./images/infographic_stemming-inflections.png)\n",
    ">[`Stemming Graphics Source`](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)\n",
    "> Here's an example of Porter Stemming rules, from the Stanford IR book\n",
    ">![Example of Porter Stemmer](images/infographic_porter-stemmer.png)\n",
    "\n",
    "- `Lemmatization`: \n",
    "> `Lemmatization`, to quote verbatim [Stanford's Information Retrival Texbook](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), \"usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma\". In other words, lemmatization returns the \"cannonical\" form of a word.\n",
    "> Some examples of lemmatization: ([source](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/))\n",
    "> - rocks : rock\n",
    "> - corpora : corpus\n",
    "> - better : good\n",
    "> In a very real sense, lemmatization is a more complex procedure and can often produce more desirable results  \n",
    "However, it is interesting to note that with sufficiently large datasets in industrial machine learning applications (such as those done at Google), stemming and lemmatization are uncommonly used (**source**: in conversation with [Lyle Ungar](https://www.cis.upenn.edu/~ungar/))\n",
    "\n",
    "\n",
    "- Finally, save cleaned text to disk: e.g.: `news_df.to_csv('news.csv', index=False, encoding='utf-8')`, OR use pickle dump, for ease of retrieval and use later\n",
    "\n",
    "Note: Text preprossing procedures draws inspiration from [here](https://towardsdatascience.com/creating-word-clouds-with-python-f2077c8de5cc) and [A Practitioner's Guide to Natural Language Processing (Part I) ‚Äî Processing & Understanding Text](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72), as well as Professor `Chris Callison-Burch's` [Computational Liguistics Class](http://computational-linguistics-class.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eL_wswq5MFMF"
   },
   "source": [
    "### 4.2.2 The Details of Text Pre-processing\n",
    "\n",
    "TODO: EDA before data cleaning to see how dirty the data is  \n",
    "TODO: Move data cleaning to this step, before we even go into word clouds!!  \n",
    "TODO: Implement the following that have not been implemented yet!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q0H3CY5dMFMG",
    "outputId": "be69ef3a-7e3d-40b5-97c2-dd0c9ce894ae",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 4.2.2 Text Pre-processing: done loading packages\n"
     ]
    }
   ],
   "source": [
    "# Heavily adapted from: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "### Getting the right packages\n",
    "\n",
    "# for Colab only\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_md\n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "# From Spacy: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.\n",
    "nlp = spacy.load('en_core_web_md', parse=True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "print(\"Section 4.2.2 Text Pre-processing: done loading packages\")\n",
    "\n",
    "##### Recall the list of preprocessing tasks\n",
    "# - `* Remove extra spaces`\n",
    "# - `* Make all text lowercase`\n",
    "# - `* Remove stopwords`\n",
    "# - `* Tokenization`\n",
    "# - `* Normalize accented characters`\n",
    "# - `Remove rougue `html`/`xml` tags`\n",
    "# - `Remove numbers`\n",
    "# - `Expanding contractions`\n",
    "# - `Stemming`\n",
    "# - `Lemmatization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2zN6axvMFMI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Option to reimport the data\n",
    "pd_tripReports = pd.read_csv(trips_raw_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "yR97tVtIMFMK",
    "outputId": "0ef37777-5a6e-4e09-8472-44fec26d53f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>title</th>\n",
       "      <th>substance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>after having had some success with other forms...</td>\n",
       "      <td>sideways world</td>\n",
       "      <td>salvia divinorum (5x extract)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>me and a couple of my buddies decided one nigh...</td>\n",
       "      <td>physical wellbeing = crucial</td>\n",
       "      <td>mushrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my girlfriend and i had been saving the methyl...</td>\n",
       "      <td>the artful dodger</td>\n",
       "      <td>methylone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i just want to warn anybody taking lithium (or...</td>\n",
       "      <td>seizure inducing combo</td>\n",
       "      <td>lsd &amp; lithium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have had several attempts before this to bre...</td>\n",
       "      <td>enlightenment through a chemical catalyst</td>\n",
       "      <td>5-meo-dmt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              report  \\\n",
       "0  after having had some success with other forms...   \n",
       "1  me and a couple of my buddies decided one nigh...   \n",
       "2  my girlfriend and i had been saving the methyl...   \n",
       "3  i just want to warn anybody taking lithium (or...   \n",
       "4  i have had several attempts before this to bre...   \n",
       "\n",
       "                                       title                      substance  \n",
       "0                             sideways world  salvia divinorum (5x extract)  \n",
       "1               physical wellbeing = crucial                      mushrooms  \n",
       "2                          the artful dodger                      methylone  \n",
       "3                     seizure inducing combo                  lsd & lithium  \n",
       "4  enlightenment through a chemical catalyst                      5-meo-dmt  "
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make text lowercase\n",
    "pd_tripReports[\"report\"] = pd_tripReports[\"report\"].str.lower()\n",
    "pd_tripReports[\"title\"] = pd_tripReports[\"title\"].str.lower()\n",
    "pd_tripReports[\"substance\"] = pd_tripReports[\"substance\"].str.lower()\n",
    "\n",
    "pd_tripReports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "TliDcqKMMFMN",
    "outputId": "ec2da1c6-d2fb-4b8a-803d-a06efa55df89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alextzhao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "', , stopwords , computer not'"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare stopwords\n",
    "nltk.download('stopwords')\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aHdZTC6cMFMP",
    "outputId": "09d3a889-334f-4990-e8d1-07ca55a69c52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# * Normalize accented characters\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "remove_accented_chars('S√≥mƒõ √Åccƒõntƒõd tƒõxt') # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cbiWUdWkMFMS",
    "outputId": "c0e8c527-3411-4ad5-d19b-30cd9e9f800e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some important text'"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rougue `html`/`xml` tags\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "strip_html_tags('<html><h2>Some important text</h2></html>') # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "asbo2_jXMFMU",
    "outputId": "603179f6-0087-4383-a0c1-c60e1c369c55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun What do you think '"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Special Characters\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "remove_special_characters(\"Well this was fun! What do you think? 123#@!\", \n",
    "                          remove_digits=True) # example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pVJEv1HhMFMW",
    "outputId": "5601dc65-0cbb-4a79-d13b-2601df72c1b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crashing! hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Stemmer based on Porter Stemmer\n",
    "# Lyle Ungar recommends not using a stemmer\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing! his crashed yesterday, ours crashes daily\") # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FKuEnHZHMFMZ",
    "outputId": "32ac2d36-d17f-40f1-ad0f-3b50a0b8682b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash ! his crashed yesterday , ours crash daily'"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize Text\n",
    "# Lyle Ungar recommends not using a lemmatizer\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\") # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVrPHApjMFMb"
   },
   "outputs": [],
   "source": [
    "# pd_tripReports.columns.str.lower().str.strip().str.lower()\n",
    "# # pd_tripReports.columns.str.lower().strip()\n",
    "# # pd_tripReports.columns = pd_tripReports.columns.str.strip().str.lower().str.replace('.', '_')\\n\",\n",
    "\n",
    "# Adapted from https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "def normalize_corpus(corpus, html_stripping=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     special_char_removal=False, text_lemmatization=False, \n",
    "                     text_stemming = False, stopword_removal=True, \n",
    "                     remove_digits=True, contraction_expansion = False):\n",
    "    \"\"\"\n",
    "    Input: corpus, list of documents\n",
    "    Output: normalized_corpus, list of normalized documents that have been preprocessed\n",
    "    \"\"\"\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # stem text: caution when stemming + lemmatizing text at the same time\n",
    "        if text_stemming:\n",
    "            doc = simple_stemmer(text)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7aioWOgNMFMd"
   },
   "outputs": [],
   "source": [
    "## Preprocess our corpus\n",
    "## Long run time: ~ 2 min\n",
    "\n",
    "# pd_tripReports.info()\n",
    "pd_tripReports.dtypes\n",
    "pd_tripReports[\"report\"] = pd_tripReports[\"report\"].astype('str')\n",
    "pd_tripReports[\"title\"] = pd_tripReports[\"title\"].astype('str')\n",
    "pd_tripReports[\"substance\"] = pd_tripReports[\"substance\"].astype('str')\n",
    "\n",
    "corpus_list = pd_tripReports[\"report\"].to_list()\n",
    "corpus_list_normalized = normalize_corpus(corpus_list)\n",
    "\n",
    "pd_tripReports_normalized = pd_tripReports\n",
    "pd_tripReports_normalized[\"report\"] = pd.Series(corpus_list_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "XJuAUGH2MFMg",
    "outputId": "adc328ce-878d-41fa-8314-9d4a64eb7233"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'journal shall hopefully guide better understanding effects salvia divinorum. like many journals attempted write , end lack desire finish them. plan make journal different. hope describe experience fullest extent better understand unique nature plant. 9/19/2002 received two grams 5x extract. excited first experience called friend watch , since first time not know would react. prepared water pipe put miniscule amount bowl , lit up. inhaled held smoke , wasnt much. exhaling , not feel much minute , felt mild buzz not much else. disappointed decided try more. put equal amount salvia bowl lit again. not feel anything much. decided not enough , ten minutes later ; filled bowl decent amount lit up. upon exhaling , not feel anything right away , hit me. amazed feeling commented could not believe drug legal. sudden , body felt covered ants. itchy sweaty threw over-shirt take t-shirt said , no , drug. pulled shirt back down. began talk high-pitched voice , saying nonsensical things. perspective radically changed. standing edge cliff back precipice beyond nothing. nothing existed beyond cliff. universe shrunk till included field vision front me. universe 50 square feet , seemed fine. still room , not recognize such. different place , not unpleasant interesting/weird ( lack better word ) . stayed universe till came down. next conscious thought cleaning pipe sink preparation another trip. friend brought video camera record trip. went back room chilled minutes refilled pipe lit up. two inhales clear chamber , not much perspective shifting , mostly extreme stoning effect similar weed noticeably different. mentioned amazed second exhale not contain much smoke. also vague feeling talking , part childrens television show , explaining things viewers. also mentioned television feeling. began come , turned cd player laid bed , attempting visuals , no avail. feeling mildly disappointed , decided inhale nitrous. gave typical nitrous rush , commented wished could feel like time , neither drug intensified other. effects wearing , thought experienced. nice trip , not quite introspective spiritual experience. however , feel started good level plan continue exploring consciousness salvia. 9/21/2002 two days since first experience salvia. enjoyed first experience immensely , found tad disappointed not experience introspective mind-altering states. 3 : 30pm , load bowl almost top salvia. clear chamber wait. feel familiar itch/sweat expect common salvia experiences. not wholly unpleasant , comfortable without shirt a/c. felt salvia begin take hold. however , mild stoning effect decide must smoke another bowl achieve desired effects. loaded bowl smoke fully. trip truly starts. finished exhaling , stared windowsill feeling dj vu came me. couldnt help feel done ( smoking salvia ) many years. began feel like experiencing childhood way fatherhood incredibly short amount time. conversations best friend grew played together. got older , children reminisced old days fun fun children now. stopped tried fight experience. said loud , ? things like , not fully understanding experiencing. decided not fight laid bed closed eyes. laid eyes closed , multi-colored spiral/vortex began swirl minds eye. made one loop started form another loop underneath previous one realized loop alive many intelligent organisms lived within it. loop continued move could hear organisms talking. mother explaining child loop represented generation them. understood witnessing new births creator beings. mother kept explaining old generations , none old ancient entity. entity. creator , vessel beings. mother told child one day , might take talk ancient entity , might not talk back. listened described ancient entity , began think beginnings , played friend growing up. conversation meeting entity , loop stopped spiraling turned ribbon , resembling sine wave , continued move fashion. , began get annoyed creatures , feeling no right inside using vessel existence. opened eyes grabbed paper pen , great difficulty wrote words : wrote , laid back began come plateau. laid bit , attempting absorb experienced , still heavily influence. thought would perfect time nitrous , prepared balloon inhaled it. not think inhaled enough , peaked felt giant hand bending forward turning upside down. straightened came nitrous , good , exhausted mood. wandered around room bit called friend , stating amazing trip , not yet ready describe it. laid back , almost completely devoid effects , thought experienced , realized first , amazing , introspective trip. felt special blown away. dozed half hour woke watched tv. got dinner writing this. feel great word comes mind wow. much respect plant grateful insight granted me. 9/24/2002 2 : 30 , load attempt smoke accidentally blow half bowl out. gathered , must careful , since salvia light herb. take two full hits. right first hit , aware effects itch/sweat starts. strip shirt second exhale. psychotropic effects begin. become aware waterpipe alive. person talks says must put ground. comply , realize entertainment system plus several random points space become alive well. start make fun comment sloppy job moving pipe ground. laugh wonder made act way. talk back , telling shut mind business. point realize drug hold me. close eyes wide brownish loop forms , vague edges. appears sparkle electricity , electricity also brown. see loop actually made eight distinct pieces piece represents different entity living room ( ones laughing ) . sudden loop breaks towards top one junctions two pieces. loop becomes snake-like thing moves/loops away , field vision. comes back field vision rushes goes inside head. snake-like thing takes control brain begins tell things. specifically tells masturbate. cannot help aware quickly undo pants begin deed. kneel onto ground , seemed like indefinitely long time , felt light tingling ejaculated trashcan. little feeling involved compared masturbation sessions , bottom scale. laid back bed listen music. huey lewis alright playing. listen , aware picked one voices within song listening specifically part. rest music , could hear every note particular part , almost sure baritone. play piano man billy joel notice picked bass guitar part , listening that. come plateau , sit realize paper due tomorrow feel compelled find information regarding it. begin searching notebooks papers , finding unable stop moving five minutes. rapidly searching , no avail. clean notebooks check computer find information. after-effects slowly tapering , wanted clean pipe , roommate kitchen , watched tv gone. cleaned pipe begin typing this. become experienced salvia use one drug repeatedly , begin find certain aspects similar trip. get itch/sweat seconds smoking quickly dissipates. perception reality changes plateau. trip yielded different perceptual changes , expected hope two trips never same. also realize plateau , memory bit fuzzy. cannot recall well comedown. after-effects , begin understand experienced. regarding effects , not realize feeling minute effect. almost two brains working simultaneously. conscious brain trip unconscious brain realizes something happening bit later. salvia leaves sort heaviness brain , kind like huge test. hypothesize due fact brain work hard , attempting understand comprehend world around influence wonderful conscious-altering drug. 9/24/2002 not report trip , observations others salvia. j , , r partake evening. j went first monster hit , started coughing must hit started rubbing tongue licking sleeve. encouraged take rest , remained weird state 15 minutes. packing new bowl , threw empty box cigarette me. hit head apologized profusely great sincerity , ever seen do. said felt like stoned ever 5 minutes. took sat , eyes glancing around. mentioned felt like j doll j talking , apparently seemed weird doll. r smoked next stood great difficulty mentioned something going lake. addressed us grandma grandpa. also said , j confirmed , felt leaden porch , like couldnt move. extent trips best knowledge not discuss length , know people similar reactions salvia. 10/11/2002 received 5 grams 5x extract week back , prepared smoked large bowl new batch salvia ( approximately two hits ) . felt familiar itch/sweat , nothing much else. tad disappointed not feel much. one point , think mildly aware room penthouse top expensive apartment complex , cannot sure. mostly , attempting force effects happen. think need construct larger , better bong take salvia in. perhaps batch isnt potent last one. build new bong attempt better trip week. 10/31/2002 great trip new bong. also take hits good trip well. trip hard understand , try describe it. looking window frame view contained lake , horizon , lava lamp , arm , chair. not recognize , one giant slate painted it. stared canvas seemed like arm connected horizon invisible energy flowing horizon. moved became upset felt like canvas ripped , yelled stop moving around. slowly came back reality not much reflecting , watched take good couple hits , obviously sent somewhere asked wheres brother ? thought back old house. came back baseline talked trips fifteen minutes , reflecting pleasant trip weird drug salvia is. 12/02/2002 k smoke one bowl together ; hit each. no real discernable effects part except heaviness right hand lasted 20 seconds. k stared window half minute. think one take hit salvia without coughing. everyone smoked , minus , coughed , veteran weed smokers. sure lack effects due extremely small amount smoked not quality salvia. months old , even though keep glass jar , not sure long salvinorin stays active. good chunk time , massive salvia trip , since finals arriving , wait. 4/25/2003 since smoked salvia. particular night , tripping 3.5 grams mushrooms 40mg amt. done 20 nitrous hits well. put ein deutsches requiem stereo take nice fatty hit. hits right away. typical salvia mindshift occurs staring foreign horizon. entertainment center become mountain range , shifting color growing intensity. music begins take place unwant , attempt change it. find cannot recognize cd remote fumble bit falling floor. begin talk. time made perfect sense , minute talking , realized saying gibberish. shake head try think saying new phrase pops head : wagons. fraction second later add words flowers cup , phrase became flowers cupwagons. means no idea. remember form salvia portion trip. perhaps phrase induced song ( vangelis messages ) listening , although no lyrics form phrase , rhythmic patterns song could formed words head. type entry almost four months later experience , phrase crossed mind many times , feel connection subconscious. discover true meaning phrase link share salvia. salvia divinorum weirdest psychedelic tried date , feel still much learned. 7/20/2003 day camp , sit deck brother smoking bowl nugs. decide smoke small bowl salvia each. hits first sits silence inhale. mindshift occurs body becomes mountain range shaped like upright backwards c. every word speak , invisible energies shoot top c bottom , imagine would like listening music , particularly vangelis messages ( song feel share transcendental connection ) . turn brother start talking. talking nonsense words make sense. friday wagons coming , b. know areyou know are. laughs mentions fucked feels doesnt know hell going ; internal well external. next day , realize word wagons somehow connected salvia myself. friday flowers two beginnings phrase. typed day previous entry. feels bit unfair journal sharing entries months think experiences. however , share better understanding strange phrase. salvia needed perhaps new prefix surface wagons phrase. must make sure truly surfaces not forced existence desire discover. starting new journey year. assimilated experiences previous year ready delve deeper myself. know am. must go beyond knowing. discover year. 9/1/03 12/29/03 gotten lazy , not record salvia experiences year occurred. however , numerous trips taken quite similar since dosage source not differ. smoked salvia eight times. trip took place chair although not aesthetically pleasing , serves purpose well. fun nothing spectacular. learned flowers cupwagons means nothing alcohol salvia not mix. semester sucked ass. tired. depressed .'"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example trip report not preprocessed\n",
    "pd_tripReports.loc[19922, \"report\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "fRlgV2dPeaeL",
    "outputId": "6d78ed79-5442-4cac-dec1-84e5eab12c6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'journal shall hopefully guide better understanding effects salvia divinorum. like many journals attempted write , end lack desire finish them. plan make journal different. hope describe experience fullest extent better understand unique nature plant. 9/19/2002 received two grams 5x extract. excited first experience called friend watch , since first time not know would react. prepared water pipe put miniscule amount bowl , lit up. inhaled held smoke , wasnt much. exhaling , not feel much minute , felt mild buzz not much else. disappointed decided try more. put equal amount salvia bowl lit again. not feel anything much. decided not enough , ten minutes later ; filled bowl decent amount lit up. upon exhaling , not feel anything right away , hit me. amazed feeling commented could not believe drug legal. sudden , body felt covered ants. itchy sweaty threw over-shirt take t-shirt said , no , drug. pulled shirt back down. began talk high-pitched voice , saying nonsensical things. perspective radically changed. standing edge cliff back precipice beyond nothing. nothing existed beyond cliff. universe shrunk till included field vision front me. universe 50 square feet , seemed fine. still room , not recognize such. different place , not unpleasant interesting/weird ( lack better word ) . stayed universe till came down. next conscious thought cleaning pipe sink preparation another trip. friend brought video camera record trip. went back room chilled minutes refilled pipe lit up. two inhales clear chamber , not much perspective shifting , mostly extreme stoning effect similar weed noticeably different. mentioned amazed second exhale not contain much smoke. also vague feeling talking , part childrens television show , explaining things viewers. also mentioned television feeling. began come , turned cd player laid bed , attempting visuals , no avail. feeling mildly disappointed , decided inhale nitrous. gave typical nitrous rush , commented wished could feel like time , neither drug intensified other. effects wearing , thought experienced. nice trip , not quite introspective spiritual experience. however , feel started good level plan continue exploring consciousness salvia. 9/21/2002 two days since first experience salvia. enjoyed first experience immensely , found tad disappointed not experience introspective mind-altering states. 3 : 30pm , load bowl almost top salvia. clear chamber wait. feel familiar itch/sweat expect common salvia experiences. not wholly unpleasant , comfortable without shirt a/c. felt salvia begin take hold. however , mild stoning effect decide must smoke another bowl achieve desired effects. loaded bowl smoke fully. trip truly starts. finished exhaling , stared windowsill feeling dj vu came me. couldnt help feel done ( smoking salvia ) many years. began feel like experiencing childhood way fatherhood incredibly short amount time. conversations best friend grew played together. got older , children reminisced old days fun fun children now. stopped tried fight experience. said loud , ? things like , not fully understanding experiencing. decided not fight laid bed closed eyes. laid eyes closed , multi-colored spiral/vortex began swirl minds eye. made one loop started form another loop underneath previous one realized loop alive many intelligent organisms lived within it. loop continued move could hear organisms talking. mother explaining child loop represented generation them. understood witnessing new births creator beings. mother kept explaining old generations , none old ancient entity. entity. creator , vessel beings. mother told child one day , might take talk ancient entity , might not talk back. listened described ancient entity , began think beginnings , played friend growing up. conversation meeting entity , loop stopped spiraling turned ribbon , resembling sine wave , continued move fashion. , began get annoyed creatures , feeling no right inside using vessel existence. opened eyes grabbed paper pen , great difficulty wrote words : wrote , laid back began come plateau. laid bit , attempting absorb experienced , still heavily influence. thought would perfect time nitrous , prepared balloon inhaled it. not think inhaled enough , peaked felt giant hand bending forward turning upside down. straightened came nitrous , good , exhausted mood. wandered around room bit called friend , stating amazing trip , not yet ready describe it. laid back , almost completely devoid effects , thought experienced , realized first , amazing , introspective trip. felt special blown away. dozed half hour woke watched tv. got dinner writing this. feel great word comes mind wow. much respect plant grateful insight granted me. 9/24/2002 2 : 30 , load attempt smoke accidentally blow half bowl out. gathered , must careful , since salvia light herb. take two full hits. right first hit , aware effects itch/sweat starts. strip shirt second exhale. psychotropic effects begin. become aware waterpipe alive. person talks says must put ground. comply , realize entertainment system plus several random points space become alive well. start make fun comment sloppy job moving pipe ground. laugh wonder made act way. talk back , telling shut mind business. point realize drug hold me. close eyes wide brownish loop forms , vague edges. appears sparkle electricity , electricity also brown. see loop actually made eight distinct pieces piece represents different entity living room ( ones laughing ) . sudden loop breaks towards top one junctions two pieces. loop becomes snake-like thing moves/loops away , field vision. comes back field vision rushes goes inside head. snake-like thing takes control brain begins tell things. specifically tells masturbate. cannot help aware quickly undo pants begin deed. kneel onto ground , seemed like indefinitely long time , felt light tingling ejaculated trashcan. little feeling involved compared masturbation sessions , bottom scale. laid back bed listen music. huey lewis alright playing. listen , aware picked one voices within song listening specifically part. rest music , could hear every note particular part , almost sure baritone. play piano man billy joel notice picked bass guitar part , listening that. come plateau , sit realize paper due tomorrow feel compelled find information regarding it. begin searching notebooks papers , finding unable stop moving five minutes. rapidly searching , no avail. clean notebooks check computer find information. after-effects slowly tapering , wanted clean pipe , roommate kitchen , watched tv gone. cleaned pipe begin typing this. become experienced salvia use one drug repeatedly , begin find certain aspects similar trip. get itch/sweat seconds smoking quickly dissipates. perception reality changes plateau. trip yielded different perceptual changes , expected hope two trips never same. also realize plateau , memory bit fuzzy. cannot recall well comedown. after-effects , begin understand experienced. regarding effects , not realize feeling minute effect. almost two brains working simultaneously. conscious brain trip unconscious brain realizes something happening bit later. salvia leaves sort heaviness brain , kind like huge test. hypothesize due fact brain work hard , attempting understand comprehend world around influence wonderful conscious-altering drug. 9/24/2002 not report trip , observations others salvia. j , , r partake evening. j went first monster hit , started coughing must hit started rubbing tongue licking sleeve. encouraged take rest , remained weird state 15 minutes. packing new bowl , threw empty box cigarette me. hit head apologized profusely great sincerity , ever seen do. said felt like stoned ever 5 minutes. took sat , eyes glancing around. mentioned felt like j doll j talking , apparently seemed weird doll. r smoked next stood great difficulty mentioned something going lake. addressed us grandma grandpa. also said , j confirmed , felt leaden porch , like couldnt move. extent trips best knowledge not discuss length , know people similar reactions salvia. 10/11/2002 received 5 grams 5x extract week back , prepared smoked large bowl new batch salvia ( approximately two hits ) . felt familiar itch/sweat , nothing much else. tad disappointed not feel much. one point , think mildly aware room penthouse top expensive apartment complex , cannot sure. mostly , attempting force effects happen. think need construct larger , better bong take salvia in. perhaps batch isnt potent last one. build new bong attempt better trip week. 10/31/2002 great trip new bong. also take hits good trip well. trip hard understand , try describe it. looking window frame view contained lake , horizon , lava lamp , arm , chair. not recognize , one giant slate painted it. stared canvas seemed like arm connected horizon invisible energy flowing horizon. moved became upset felt like canvas ripped , yelled stop moving around. slowly came back reality not much reflecting , watched take good couple hits , obviously sent somewhere asked wheres brother ? thought back old house. came back baseline talked trips fifteen minutes , reflecting pleasant trip weird drug salvia is. 12/02/2002 k smoke one bowl together ; hit each. no real discernable effects part except heaviness right hand lasted 20 seconds. k stared window half minute. think one take hit salvia without coughing. everyone smoked , minus , coughed , veteran weed smokers. sure lack effects due extremely small amount smoked not quality salvia. months old , even though keep glass jar , not sure long salvinorin stays active. good chunk time , massive salvia trip , since finals arriving , wait. 4/25/2003 since smoked salvia. particular night , tripping 3.5 grams mushrooms 40mg amt. done 20 nitrous hits well. put ein deutsches requiem stereo take nice fatty hit. hits right away. typical salvia mindshift occurs staring foreign horizon. entertainment center become mountain range , shifting color growing intensity. music begins take place unwant , attempt change it. find cannot recognize cd remote fumble bit falling floor. begin talk. time made perfect sense , minute talking , realized saying gibberish. shake head try think saying new phrase pops head : wagons. fraction second later add words flowers cup , phrase became flowers cupwagons. means no idea. remember form salvia portion trip. perhaps phrase induced song ( vangelis messages ) listening , although no lyrics form phrase , rhythmic patterns song could formed words head. type entry almost four months later experience , phrase crossed mind many times , feel connection subconscious. discover true meaning phrase link share salvia. salvia divinorum weirdest psychedelic tried date , feel still much learned. 7/20/2003 day camp , sit deck brother smoking bowl nugs. decide smoke small bowl salvia each. hits first sits silence inhale. mindshift occurs body becomes mountain range shaped like upright backwards c. every word speak , invisible energies shoot top c bottom , imagine would like listening music , particularly vangelis messages ( song feel share transcendental connection ) . turn brother start talking. talking nonsense words make sense. friday wagons coming , b. know areyou know are. laughs mentions fucked feels doesnt know hell going ; internal well external. next day , realize word wagons somehow connected salvia myself. friday flowers two beginnings phrase. typed day previous entry. feels bit unfair journal sharing entries months think experiences. however , share better understanding strange phrase. salvia needed perhaps new prefix surface wagons phrase. must make sure truly surfaces not forced existence desire discover. starting new journey year. assimilated experiences previous year ready delve deeper myself. know am. must go beyond knowing. discover year. 9/1/03 12/29/03 gotten lazy , not record salvia experiences year occurred. however , numerous trips taken quite similar since dosage source not differ. smoked salvia eight times. trip took place chair although not aesthetically pleasing , serves purpose well. fun nothing spectacular. learned flowers cupwagons means nothing alcohol salvia not mix. semester sucked ass. tired. depressed .'"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example trip report preprocessed\n",
    "pd_tripReports_normalized.loc[19922, \"report\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSSV4mAqfDYK"
   },
   "outputs": [],
   "source": [
    "# save preprocessed file to disk\n",
    "pd_tripReports.to_csv('{}/trips_normed-html-accent-special-digits'.format(root_path), index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKQB52eEMFMl"
   },
   "source": [
    "Because the pre-processing, especially the removal of stopwords takes a substantial amount of time to run, we store a serialized version of the object so we may retrive it later at a substantially faster rate; see [pickle tutorial](https://www.journaldev.com/15638/python-pickle-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihvtolHwMFMm"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "## Use this cell as a template to save and load large data files; \n",
    "## replace filenames appropriately;\n",
    "## current code saves and loads `reports_text_tokenized`\n",
    "\n",
    "import pickle\n",
    "\n",
    "### Stores the tokenized reports:\n",
    "with open('', 'wb') as out_file:\n",
    "    pickle.dump(reports_text_tokenized, out_file)\n",
    "    \n",
    "### Loads the tokenized reports:\n",
    "with open('reports_tokenized_map', 'rb') as in_file:\n",
    "    reports_text_tokenized = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DheynHhTMFMq"
   },
   "source": [
    "#### A note on preparing stopwords\n",
    "\n",
    "The removal of stopwords (typically commonly occuring, low information content words) in natural language processing is in itself an art. Both automatic detection of stopwords (Wilbur & Sirotkin, 1992) and dictionary approaches have been widely used. For example, `nltk` has 179 stopwords , `wordcloud` package has 192 stopwords,`sklearn` has 318 stopwords, with 116 words shared between all three lists. Some examples of these shared stopwords are 'how', 'itself', 'an', 'ours', 'who', 'had', 'once', 'before', 'yours', and 'the', which are among the most common words in the English language.\n",
    "\n",
    "Here we (1) use `nltk` and `wordcloud` stopwords as a baseline (exclude `sklearn` stopwords because of [known issues](https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words) and (2) add all alternative spellings of drugs observed from EDA to our stopwords list; Note that the latter is an optional, concious design decision: we can leave the drug names be (in which case they would show up disproportionately in the word clouds), or we can remove all drug names from the reports and focus on non drug names that correlate with particular reports. In total, our psychedelic drug corpus has 2783 unique spellings / mispellings of drug names. In total, our stopword list consisits of 2999 unique stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "dYWLawV4MFMq",
    "outputId": "5d8a0e40-2b61-4a42-dfe7-1b7b9390a214"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b7ade4967c3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk_stopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#nltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordcloud_STOPWORDS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#wordcloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstances_all_alternative_spellings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#all substance spellings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "# Number of stopwords for each dictionary\n",
    "from sklearn.feature_extraction import stop_words\n",
    "len(stop_words.ENGLISH_STOP_WORDS) #sklearn\n",
    "len(nltk_stopwords.words('english')) #nltk\n",
    "len(wordcloud_STOPWORDS) #wordcloud\n",
    "len(substances_all_alternative_spellings) #all substance spellings\n",
    "\n",
    "# overlap between sklearn, nltk, and wordcloud stopwords\n",
    "overlapped_stopwords = []\n",
    "overlap_stopwords = [word for word in stop_words.ENGLISH_STOP_WORDS if word in nltk_stopwords.words('english') and word in wordcloud_STOPWORDS]\n",
    "print(overlap[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErrtMxPkMFMt",
    "outputId": "b3e63099-c1ba-4638-ecdd-c5e93e4449c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve all alternative spellings for substances\n",
    "# Using R, we found out how many unique substance spellings there were, and stored in `substances_unique` textfile\n",
    "with open('substances_unique') as f:\n",
    "    substances_all_alternative_spellings = f.readlines()\n",
    "substances_all_alternative_spellings = [spelling.strip().replace(\"'\", \"\") for spelling in substances_all_alternative_spellings]\n",
    "substances_all_alternative_spellings[:30]\n",
    "\n",
    "## prepare stopwords\n",
    "stopword_list = []\n",
    "stopwords_nltk = nltk_stopwords.words('english')\n",
    "stopwords_wordcloud = list(wordcloud_STOPWORDS)\n",
    "stopword_list.extend(stopwords_nltk + stopwords_wordcloud + substances_all_alternative_spellings)\n",
    "# deduplicate list\n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "\n",
    "# How many unique stopwords in total do we have?\n",
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SMI_2d7wMFMv",
    "outputId": "389499f5-d3d6-4571-ebfb-236a4cb6458f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7e1a2ea6f571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# TODO: Adapt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# TODO: Vectorize for pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordPunctTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# Code from: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "# TODO: Adapt\n",
    "# TODO: Vectorize for pandas dataframe\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSVekR2oMFM0"
   },
   "outputs": [],
   "source": [
    "# NOTE: Long Run Time\n",
    "\n",
    "# Tokenize the text\n",
    "reports_text_tokenized = {}\n",
    "\n",
    "# reports_text is {substance: text} mapping\n",
    "# NOTE: This code takes an extremely long time to run\n",
    "for substance in reports_text:\n",
    "    substance_text = reports_text[substance]\n",
    "    tokens = nltk.word_tokenize(substance_text)    \n",
    "    tokens = [token.strip().lower() for token in tokens]\n",
    "    substance_tokenized_text = ' '.join([token for token in tokens if token not in stopword_list])\n",
    "    \n",
    "    reports_text_tokenized[substance] = substance_tokenized_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yKTfJkwhMFM2"
   },
   "source": [
    "\n",
    "\n",
    "### 4.2.3 Insights from Text Pre-processing\n",
    "\n",
    "TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VO0awhnqMFM3"
   },
   "source": [
    "## 4.3 Data Wrangling and Exploratory Data Analysis (EDA)\n",
    "\n",
    "After preliminiary data preprocessing (cleaning), we perform data wrangling and Exploratory Data Analysis. Data Wrangling describes a suite of procedures to get the data into the right format, to \"wrangle\" with the data so that we can use it appropriately. Exploratory Data Analysis (EDA) refers broadly to understanding the data better by attempting to visualize it, ask it questions, and to better understand what we are working with.\n",
    "\n",
    "EDA was mostly performed in R, in particular using `dplyr` due to my familiarity with R, and was performed directly on the raw data. For reproducibility, the executible R code is also reproduced below; very similar procedures can be used with `pandas`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "otGkXu3tMFNA"
   },
   "outputs": [],
   "source": [
    "# Sets up the ability to use R in a jupyter notebook\n",
    "# https://stackoverflow.com/questions/39008069/r-and-python-in-one-jupyter-notebook\n",
    "# To use R, add %%R to the beginning of a cell, before any code and any comments\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AfdXCcvRMFNC",
    "outputId": "2a13cf28-2fd4-48e5-881b-0e9d4b446576"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: pacman\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# Installs the proper R packages\n",
    "if(!require('pacman')) {\n",
    "  install.packages('pacman', repos = \"http://cran.us.r-project.org\")\n",
    "}\n",
    "pacman::p_load(dplyr, leaps, car, tidyverse, GGally, reshape2, data.table, ggcorrplot, bestglm, glmnet, mapproj, pROC, data.tale, tm, SnowballC, wordcloud, RColorBrewer, reshape2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q0sf6ZCVMFNE",
    "outputId": "8f16bcd9-3720-499a-f545-74aff3a9bf9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 19924\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## Read in the raw trip reports\n",
    "tripReports <- fread(\"trips.csv\")\n",
    "tripReports.df <- as.data.frame(tripReports)\n",
    "# glimpse(tripReports)\n",
    "\n",
    "# Number of trip reports in our dataset: 19924\n",
    "nrow(tripReports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xxHdIEYMFNI",
    "outputId": "1dcaa9f9-5d58-40ef-ded3-20ebabb32593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        substance\n",
      " 1: Salvia divinorum (5x extract)\n",
      " 2:                     Mushrooms\n",
      " 3:                     Methylone\n",
      " 4:                 LSD & Lithium\n",
      " 5:                     5-MeO-DMT\n",
      " 6: Salvia divinorum (6x extract)\n",
      " 7:                      Cannabis\n",
      " 8:                        2C-T-2\n",
      " 9:                         6-APB\n",
      "10:                       2-C-T-2\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# Examine the substances present in the dataset, writing to output file\n",
    "tripReports %>% select(substance) %>% write.table(file = \"substances-all\", append=FALSE, col.names = FALSE, row.names = FALSE, sep = \"\\n\")\n",
    "\n",
    "# # Examine the substances present in the dataset, display\n",
    "tripReports %>% select(substance) %>% head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBg7mYbhMFNK",
    "outputId": "ca8d3f7a-80e2-4f18-96e9-e6ae7a0a2dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 19,924\n",
      "Variables: 24\n",
      "$ report                    \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"After having had some success with other f‚Ä¶\n",
      "$ title                     \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"Sideways World\", \"Physical Wellbeing = Cru‚Ä¶\n",
      "$ substance                 \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"salvia divinorum (5x extract)\", \"mushrooms‚Ä¶\n",
      "$ substance.mushrooms       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0‚Ä¶\n",
      "$ substance.lsd             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.mescaline       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.cannabis        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0‚Ä¶\n",
      "$ substance.mdma            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ayahuasca       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.nitrous_oxide   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.salvia          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.methamphetamine \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0‚Ä¶\n",
      "$ substance.dmt             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.5_meo_dmt       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.alcohol         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ketamine        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ibogaine        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.pcp             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.kava            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.kratom          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0‚Ä¶\n",
      "$ substance.morning_glory   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.syrian_rue      \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.unknown         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.UNK             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## Manipulate the dataframe to have 1 hot encoding for substances\n",
    "tripReports$substance <- tolower(tripReports$substance)\n",
    "tripReports <- as.data.frame(tripReports)\n",
    "\n",
    "# Explicity define the substance of interest\n",
    "substances.of.interest <- c(\"substance.mushrooms\", \"substance.lsd\", \"substance.mescaline\", \"substance.cannabis\", \"substance.mdma\", \"substance.ayahuasca\", \"substance.nitrous_oxide\", \"substance.salvia\", \"substance.methamphetamine\", \"substance.dmt\", \"substance.5_meo_dmt\", \"substance.alcohol\", \"substance.ketamine\", \"substance.ibogaine\", \"substance.pcp\", \"substance.kava\", \"substance.kratom\", \"substance.morning_glory\", \"substance.syrian_rue\", \"substance.unknown\", \"substance.UNK\")\n",
    "\n",
    "for (sub in substances.of.interest) {\n",
    "  tripReports[sub] = 0\n",
    "}\n",
    "\n",
    "### Adding one hot encodings to the substances\n",
    "tripReports$substance.mushrooms[grepl(\"mushroom|mushhrooms|mushooms|psilocin|psilocybin|psilocybe\", tripReports$substance)] <- 1\n",
    "tripReports$substance.lsd[grepl(\"lysergic acid|lsd\", tripReports$substance)] <- 1\n",
    "tripReports$substance.mescaline[grepl(\"mescaline|peyote\", tripReports$substance)] <- 1\n",
    "tripReports$substance.cannabis[grepl(\"cannabis|canabis|cannabbis|cannabinoid|cannabus|cannibis|cannibus| thc \", tripReports$substance)] <- 1\n",
    "tripReports$substance.mdma[grepl(\"mdma|ecstacy|ecstasy|ectasy|molly\", tripReports$substance)] <- 1\n",
    "tripReports$substance.ayahuasca[grepl(\"ayahuasca|ayahausca|ayahusca|p. viridis|p.viridis|b.caapi|b. caapi|cappi|viridis\", tripReports$substance)] <- 1\n",
    "tripReports$substance.nitrous_oxide[grepl(\"nitric|nitrites|nitrogen|nitrous|whippets\", tripReports$substance)] <- 1\n",
    "tripReports$substance.salvia[grepl(\"salia|saliva|salivia|sally|salva|salvia|salvinorin\", tripReports$substance)] <- 1\n",
    "tripReports$substance.methamphetamine[grepl(\"met|meth|methampetamine|methamphetamine|speed\", tripReports$substance)] <- 1\n",
    "# dmt\n",
    "tripReports$substance.dmt[grepl(\"nn-dmt\", tripReports$substance)] <- 1\n",
    "tripReports$substance.dmt[tripReports$substance == \"dmt\"] <- 1 # cannot just grep dmt otherwise this will include 5-meo-dmt\n",
    "#5-meo-dmt\n",
    "tripReports$substance.5_meo_dmt[grepl(\"5 meo-dmt|5-meo dmt|5-meo-dmt|5meo-dmt\", tripReports$substance)] <- 1\n",
    "tripReports$substance.alcohol[grepl(\"alchohol|alcohol\", tripReports$substance)] <- 1\n",
    "tripReports$substance.ketamine[grepl(\"ketamin|ketamine\", tripReports$substance)] <- 1\n",
    "tripReports$substance.ibogaine[grepl(\"iboga|ibogaine\", tripReports$substance)] <- 1\n",
    "tripReports$substance.pcp[grepl(\"pcp\", tripReports$substance)] <- 1\n",
    "tripReports$substance.kava[grepl(\"kava\", tripReports$substance)] <- 1\n",
    "tripReports$substance.kratom[grepl(\"kratom\", tripReports$substance)] <- 1\n",
    "tripReports$substance.morning_glory[grepl(\"glory|glories\", tripReports$substance)] <- 1\n",
    "tripReports$substance.syrian_rue[grepl(\"syrian rue|rue\", tripReports$substance)] <- 1\n",
    "tripReports$substance.unknown[grepl(\"unknown|unidentified|unkown\", tripReports$substance)] <- 1\n",
    "\n",
    "glimpse(tripReports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glNrrY1jMFNM",
    "outputId": "750d892a-adf9-495f-ec2c-8f1cf014a240"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   substance count\n",
      "1         substance.cannabis  3110\n",
      "2        substance.mushrooms  1686\n",
      "3           substance.salvia  1556\n",
      "4             substance.mdma  1188\n",
      "5              substance.lsd  1131\n",
      "6  substance.methamphetamine   929\n",
      "7          substance.alcohol   928\n",
      "8    substance.morning_glory   427\n",
      "9    substance.nitrous_oxide   300\n",
      "10       substance.5_meo_dmt   297\n",
      "11      substance.syrian_rue   293\n",
      "12        substance.ketamine   289\n",
      "13          substance.kratom   207\n",
      "14       substance.ayahuasca   172\n",
      "15             substance.dmt   167\n",
      "16            substance.kava   167\n",
      "17             substance.pcp    81\n",
      "18       substance.mescaline    73\n",
      "19         substance.unknown    47\n",
      "20        substance.ibogaine    43\n",
      "21             substance.UNK     0\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## Number of reports containing each substance: NOTE: a report might have more than one substance\n",
    "tripSubstances <- tripReports %>% select(-report, -title, -substance)\n",
    "tripReportsCount <- data.frame(substance = names(tripSubstances), count = colSums(tripSubstances))\n",
    "tripReportsCountSorted <- tripReportsCount %>% arrange(desc(count))\n",
    "tripReportsCountSorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sl876IpoMFNO",
    "outputId": "283eceba-65de-485c-b229-0aad0b870a40",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 19,924\n",
      "Variables: 25\n",
      "$ report                    \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"After having had some success with other f‚Ä¶\n",
      "$ title                     \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"Sideways World\", \"Physical Wellbeing = Cru‚Ä¶\n",
      "$ substance                 \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"salvia divinorum (5x extract)\", \"mushrooms‚Ä¶\n",
      "$ substance.mushrooms       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0‚Ä¶\n",
      "$ substance.lsd             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.mescaline       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.cannabis        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0‚Ä¶\n",
      "$ substance.mdma            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ayahuasca       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.nitrous_oxide   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.salvia          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.methamphetamine \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0‚Ä¶\n",
      "$ substance.dmt             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.5_meo_dmt       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.alcohol         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ketamine        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ibogaine        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.pcp             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.kava            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.kratom          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0‚Ä¶\n",
      "$ substance.morning_glory   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.syrian_rue      \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.unknown         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.UNK             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.unique_label    \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"substance.salvia\", \"substance.mushrooms\", ‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## Find reports with only one unique substance\n",
    "# Note: Long run time ~ 40 seconds\n",
    "\n",
    "tripReports$substance.unique_label <- \"NA\"\n",
    "# glimpse(tripReports)\n",
    "uniqueSubstanceRows <- tripReports %>% select(-report, -title, -substance, -substance.unique_label) %>% rowSums() == 1\n",
    "\n",
    "for (row in 1:nrow(tripReports)) {\n",
    "  # this row contains a unique substance\n",
    "  if (uniqueSubstanceRows[row]) {\n",
    "    for (substance in substances.of.interest) {\n",
    "      if (tripReports[row, substance] == 1) {\n",
    "        tripReports[row, ]$substance.unique_label<- substance\n",
    "      } \n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "glimpse(tripReports)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_KLZHVAMFNR"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# TEMP, SAVE: Use this code to save the encoded table for future use in multiple formats\n",
    "save(tripReports, file=\"tripReportsEncoded.Rda\")\n",
    "write.table(tripReports, file=\"tripReportsEncodedTable\", sep = \";;\", row.names = TRUE, col.names = TRUE )\n",
    "write.csv(tripReports, file=\"tripReportsEncoded.csv\", sep = \",\", row.names = TRUE, col.names = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPPhehWzMFNT",
    "outputId": "135f5d2c-734a-45d8-86f1-f70f00fac37e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 21\n",
      "Variables: 2\n",
      "$ substance \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"NA\", \"substance.cannabis\", \"substance.salvia\", \"substance.‚Ä¶\n",
      "$ count     \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m 11569, 1609, 1270, 1094, 747, 746, 696, 409, 328, 250, 171,‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Error in withVisible({ : object 'tripRerpotsCount_merged' not found\n",
      "Calls: <Anonymous> -> <Anonymous> -> withVisible\n",
      "\n",
      "R[write to console]: In addition: \n",
      "R[write to console]: Warning messages:\n",
      "\n",
      "R[write to console]: 1: \n",
      "R[write to console]: In write.csv(tripReports, file = \"tripReportsEncoded.csv\", sep = \",\",  :\n",
      "R[write to console]: \n",
      " \n",
      "R[write to console]:  attempt to set 'col.names' ignored\n",
      "\n",
      "R[write to console]: 2: \n",
      "R[write to console]: In write.csv(tripReports, file = \"tripReportsEncoded.csv\", sep = \",\",  :\n",
      "R[write to console]: \n",
      " \n",
      "R[write to console]:  attempt to set 'sep' ignored\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in withVisible({ : object 'tripRerpotsCount_merged' not found\n",
      "Calls: <Anonymous> -> <Anonymous> -> withVisible\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "#### Create a data frame that combines the count for reports containing a substance and reports that are uniquely a single substance\n",
    "tripReportsUniqueSubstanceCountSorted <- tripReports %>% group_by(substance.unique_label) %>% summarise(count = n()) %>% arrange(desc(count))\n",
    "# glimpse(tripReportsCountSorted)\n",
    "colnames(tripReportsUniqueSubstanceCountSorted) = c(\"substance\", \"count\")\n",
    "tripReportsUniqueSubstanceCountSorted %>% glimpse()\n",
    "\n",
    "tripReportsCount_merged <- merge(tripReportsCountSorted, tripReportsUniqueSubstanceCountSorted, by=\"substance\")\n",
    "colnames(tripReportsCount_merged) = c(\"substance\", \"n_includes_substance\", \"n_single_substance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8AnR8I6MFNV",
    "outputId": "79cb60c4-5df8-4eb1-c8ef-b506082e2f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   substance n_includes_substance n_single_substance\n",
      "1         substance.cannabis                 3110               1609\n",
      "2        substance.mushrooms                 1686               1094\n",
      "3           substance.salvia                 1556               1270\n",
      "4             substance.mdma                 1188                747\n",
      "5              substance.lsd                 1131                696\n",
      "6  substance.methamphetamine                  929                746\n",
      "7          substance.alcohol                  928                409\n",
      "8    substance.morning_glory                  427                328\n",
      "9    substance.nitrous_oxide                  300                155\n",
      "10       substance.5_meo_dmt                  297                250\n",
      "11      substance.syrian_rue                  293                164\n",
      "12        substance.ketamine                  289                170\n",
      "13          substance.kratom                  207                171\n",
      "14       substance.ayahuasca                  172                114\n",
      "15             substance.dmt                  167                167\n",
      "16            substance.kava                  167                131\n",
      "17             substance.pcp                   81                 38\n",
      "18       substance.mescaline                   73                 44\n",
      "19         substance.unknown                   47                 13\n",
      "20        substance.ibogaine                   43                 39\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "# EXPORT: Table of selected substances and their counts\n",
    "tripReportsCount_merged %>% arrange(desc(n_includes_substance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SzsL1wIfMFNY",
    "outputId": "90b15b2d-e7e3-40c7-c020-ada4ba80ef02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using substance as id variables\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## This data frame that encodes how many trip reports contains a substance and how many reports are uniquely such a substance'\n",
    "tripReportsCount_merged.long <- reshape2::melt(tripReportsCount_merged)\n",
    "# EXPORT: Graph of distribution of substances\n",
    "tripReportsCount_merged.long %>% ggplot( aes(x = reorder(substance, -value), y = value, fill=variable)) + geom_bar(stat=\"identity\", position = \"dodge\") + theme(axis.text.x = element_text(angle = 67.5, hjust = 1)) + ggtitle(\"Count of Substances Present in Trip Reports\") + xlab(\"substance\") + ylab(\"count\") + labs(fill = \"Number of Reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWDBLzlPMFM6"
   },
   "source": [
    "### 4.3.1 Treatment of `substance` column\n",
    "\n",
    "The `substance` column is of type string, and describes what substance(s) were associated with a particular trip report. It turns out that users often co-consume multiple substances at once, and may convey that fact differently in different formats as plaintext. For example, supposed a user has had `lsd`, `mushrooms`, and `lithium` in a single session. They may record this fact as `lsd, mushrooms, and lithium`, `lsd, lithium & mushrooms`, `mushrooms, lsd and lithium` or in another way, using freely `,`, `and`, and `&` as delimiters. As such, great care had to be taken using `grep`, regular expressions, and multiple string operations to label each report with a collection of substances in a standardized format. \n",
    "\n",
    "We also observed a tremendous number of unique substances in the `substance` column. Here we describe a few notable characteristics, a key procedures we performed:\n",
    "    - `2783` unique spellings for substances, spanning mistakes, alternative spellings, and parenthetical clarifications (e.g.: `mda? (sold as ecstasy)`. A few ubstances such as `2-cb` had no alternative spellings, while `salvia` had 130 unique spellings! (a few examples taken verbatim include: `salvia`, `sally divinorum (5x extract)`, `salvia d`, `saliva`, `salvinorin-a`)\n",
    "    - Due to the overwhelming number of alternative spellings, and mistakes, manual deduplication was necessary with expert knowledge. After manual depuplication, we had `1379` unique substance labels. It is very possible that I have missed a considerable chunck of duplicated entries, and an interesting task in itself is to further analyze the substance names.\n",
    "    - The substance names were standardized with the format `substance.<normalized-name>`\n",
    "    \n",
    "\n",
    "With these considerations, we used one hot encoding to denote the presence of a substance, and also added a `substance.unique_label` column for denoting those reports that only feature one substance.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGVD-kq9Csyk",
    "outputId": "db445c94-1ca6-479e-e35a-c097cdf0c4a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>report</th>\n",
       "      <th>title</th>\n",
       "      <th>substance</th>\n",
       "      <th>substance.mushrooms</th>\n",
       "      <th>substance.lsd</th>\n",
       "      <th>substance.mescaline</th>\n",
       "      <th>substance.cannabis</th>\n",
       "      <th>substance.mdma</th>\n",
       "      <th>substance.ayahuasca</th>\n",
       "      <th>...</th>\n",
       "      <th>substance.ketamine</th>\n",
       "      <th>substance.ibogaine</th>\n",
       "      <th>substance.pcp</th>\n",
       "      <th>substance.kava</th>\n",
       "      <th>substance.kratom</th>\n",
       "      <th>substance.morning_glory</th>\n",
       "      <th>substance.syrian_rue</th>\n",
       "      <th>substance.unknown</th>\n",
       "      <th>substance.UNK</th>\n",
       "      <th>substance.unique_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>After having had some success with other forms...</td>\n",
       "      <td>Sideways World</td>\n",
       "      <td>salvia divinorum (5x extract)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.salvia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Me and a couple of my buddies decided one nigh...</td>\n",
       "      <td>Physical Wellbeing = Crucial</td>\n",
       "      <td>mushrooms</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.mushrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>My girlfriend and I had been saving the methyl...</td>\n",
       "      <td>The Artful Dodger</td>\n",
       "      <td>methylone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.methamphetamine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I just want to warn anybody taking Lithium (or...</td>\n",
       "      <td>Seizure Inducing Combo</td>\n",
       "      <td>lsd &amp; lithium</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.lsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I have had several attempts before this to bre...</td>\n",
       "      <td>Enlightenment Through a Chemical Catalyst</td>\n",
       "      <td>5-meo-dmt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.5_meo_dmt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             report  \\\n",
       "0           1  After having had some success with other forms...   \n",
       "1           2  Me and a couple of my buddies decided one nigh...   \n",
       "2           3  My girlfriend and I had been saving the methyl...   \n",
       "3           4  I just want to warn anybody taking Lithium (or...   \n",
       "4           5  I have had several attempts before this to bre...   \n",
       "\n",
       "                                       title                      substance  \\\n",
       "0                             Sideways World  salvia divinorum (5x extract)   \n",
       "1               Physical Wellbeing = Crucial                      mushrooms   \n",
       "2                          The Artful Dodger                      methylone   \n",
       "3                     Seizure Inducing Combo                  lsd & lithium   \n",
       "4  Enlightenment Through a Chemical Catalyst                      5-meo-dmt   \n",
       "\n",
       "   substance.mushrooms  substance.lsd  substance.mescaline  \\\n",
       "0                    0              0                    0   \n",
       "1                    1              0                    0   \n",
       "2                    0              0                    0   \n",
       "3                    0              1                    0   \n",
       "4                    0              0                    0   \n",
       "\n",
       "   substance.cannabis  substance.mdma  substance.ayahuasca  ...  \\\n",
       "0                   0               0                    0  ...   \n",
       "1                   0               0                    0  ...   \n",
       "2                   0               0                    0  ...   \n",
       "3                   0               0                    0  ...   \n",
       "4                   0               0                    0  ...   \n",
       "\n",
       "   substance.ketamine  substance.ibogaine  substance.pcp  substance.kava  \\\n",
       "0                   0                   0              0               0   \n",
       "1                   0                   0              0               0   \n",
       "2                   0                   0              0               0   \n",
       "3                   0                   0              0               0   \n",
       "4                   0                   0              0               0   \n",
       "\n",
       "   substance.kratom  substance.morning_glory  substance.syrian_rue  \\\n",
       "0                 0                        0                     0   \n",
       "1                 0                        0                     0   \n",
       "2                 0                        0                     0   \n",
       "3                 0                        0                     0   \n",
       "4                 0                        0                     0   \n",
       "\n",
       "   substance.unknown  substance.UNK     substance.unique_label  \n",
       "0                  0              0           substance.salvia  \n",
       "1                  0              0        substance.mushrooms  \n",
       "2                  0              0  substance.methamphetamine  \n",
       "3                  0              0              substance.lsd  \n",
       "4                  0              0        substance.5_meo_dmt  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results of the encoding\n",
    "\n",
    "import pandas as pd\n",
    "root_path = \".\"\n",
    "pd_trips_encoded_from_R = pd.read_csv('{}/tripReportsEncoded.csv'.format(root_path))\n",
    "pd_trips_encoded_from_R.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sF1b9s4CCsyl",
    "outputId": "80c320d0-f216-4c25-ea39-a6863c3a5aea",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>report</th>\n",
       "      <th>title</th>\n",
       "      <th>substance</th>\n",
       "      <th>substance_mushrooms</th>\n",
       "      <th>substance_lsd</th>\n",
       "      <th>substance_mescaline</th>\n",
       "      <th>substance_cannabis</th>\n",
       "      <th>substance_mdma</th>\n",
       "      <th>substance_ayahuasca</th>\n",
       "      <th>...</th>\n",
       "      <th>substance_ketamine</th>\n",
       "      <th>substance_ibogaine</th>\n",
       "      <th>substance_pcp</th>\n",
       "      <th>substance_kava</th>\n",
       "      <th>substance_kratom</th>\n",
       "      <th>substance_morning_glory</th>\n",
       "      <th>substance_syrian_rue</th>\n",
       "      <th>substance_unknown</th>\n",
       "      <th>substance_UNK</th>\n",
       "      <th>substance_unique_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>success forms legal highs , decided experiment...</td>\n",
       "      <td>Sideways World</td>\n",
       "      <td>salvia divinorum (5x extract)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.salvia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>couple buddies decided one night past february...</td>\n",
       "      <td>Physical Wellbeing = Crucial</td>\n",
       "      <td>mushrooms</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.mushrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>girlfriend saving methylone special occasion ,...</td>\n",
       "      <td>The Artful Dodger</td>\n",
       "      <td>methylone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.methamphetamine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>want warn anybody taking lithium ( probably ps...</td>\n",
       "      <td>Seizure Inducing Combo</td>\n",
       "      <td>lsd &amp; lithium</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.lsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>several attempts breakthrough 5-meo-dmt. belie...</td>\n",
       "      <td>Enlightenment Through a Chemical Catalyst</td>\n",
       "      <td>5-meo-dmt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.5_meo_dmt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             report  \\\n",
       "0           1  success forms legal highs , decided experiment...   \n",
       "1           2  couple buddies decided one night past february...   \n",
       "2           3  girlfriend saving methylone special occasion ,...   \n",
       "3           4  want warn anybody taking lithium ( probably ps...   \n",
       "4           5  several attempts breakthrough 5-meo-dmt. belie...   \n",
       "\n",
       "                                       title                      substance  \\\n",
       "0                             Sideways World  salvia divinorum (5x extract)   \n",
       "1               Physical Wellbeing = Crucial                      mushrooms   \n",
       "2                          The Artful Dodger                      methylone   \n",
       "3                     Seizure Inducing Combo                  lsd & lithium   \n",
       "4  Enlightenment Through a Chemical Catalyst                      5-meo-dmt   \n",
       "\n",
       "   substance_mushrooms  substance_lsd  substance_mescaline  \\\n",
       "0                    0              0                    0   \n",
       "1                    1              0                    0   \n",
       "2                    0              0                    0   \n",
       "3                    0              1                    0   \n",
       "4                    0              0                    0   \n",
       "\n",
       "   substance_cannabis  substance_mdma  substance_ayahuasca  ...  \\\n",
       "0                   0               0                    0  ...   \n",
       "1                   0               0                    0  ...   \n",
       "2                   0               0                    0  ...   \n",
       "3                   0               0                    0  ...   \n",
       "4                   0               0                    0  ...   \n",
       "\n",
       "   substance_ketamine  substance_ibogaine  substance_pcp  substance_kava  \\\n",
       "0                   0                   0              0               0   \n",
       "1                   0                   0              0               0   \n",
       "2                   0                   0              0               0   \n",
       "3                   0                   0              0               0   \n",
       "4                   0                   0              0               0   \n",
       "\n",
       "   substance_kratom  substance_morning_glory  substance_syrian_rue  \\\n",
       "0                 0                        0                     0   \n",
       "1                 0                        0                     0   \n",
       "2                 0                        0                     0   \n",
       "3                 0                        0                     0   \n",
       "4                 0                        0                     0   \n",
       "\n",
       "   substance_unknown  substance_UNK     substance_unique_label  \n",
       "0                  0              0           substance.salvia  \n",
       "1                  0              0        substance.mushrooms  \n",
       "2                  0              0  substance.methamphetamine  \n",
       "3                  0              0              substance.lsd  \n",
       "4                  0              0        substance.5_meo_dmt  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CAUTION: Long Run Time\n",
    "# EXPORT\n",
    "# pre-process the encoded data frame to be the \"master data frame\" for subsequent analysis\n",
    "pd_trips_encoded_from_R.dtypes\n",
    "pd_trips_encoded_from_R[\"report\"] = pd_trips_encoded_from_R[\"report\"].astype('str')\n",
    "pd_trips_encoded_from_R[\"title\"] = pd_trips_encoded_from_R[\"title\"].astype('str')\n",
    "pd_trips_encoded_from_R[\"substance\"] = pd_trips_encoded_from_R[\"substance\"].astype('str')\n",
    "\n",
    "corpus_list_encoded = pd_trips_encoded_from_R[\"report\"].to_list()\n",
    "corpus_list_encoded_normalized = normalize_corpus(corpus_list_encoded)\n",
    "\n",
    "pd_trips_encoded_normalized = pd_trips_encoded_from_R\n",
    "pd_trips_encoded_normalized[\"report\"] = pd.Series(corpus_list_encoded_normalized)\n",
    "\n",
    "# change column names to better fit pandas workflow\n",
    "pd_trips_encoded_normalized.columns = pd_trips_encoded_normalized.columns.str.replace('.', '_')\n",
    "\n",
    "# encoded and pre-processed data frame\n",
    "pd_trips_encoded_normalized.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQeY7facMFM7"
   },
   "source": [
    "### 4.3.2 Facts about the Data\n",
    "\n",
    "TODO\n",
    "\n",
    "- average length of trip reports, character level + word level\n",
    "- mistribution of the length of trip reports\n",
    "- most frequently co-consumed substances\n",
    "- distribution of the number of labels for different reports\n",
    "- distribution of the number of labels for each substance: e.g.: are reports with `cannabis` frequently not just cannabis? normalize using proportion of trip reports;\n",
    "- think more critically about treatment of longtail; do we simply label substances not within the top 20 substances as `<UNK>`? If so, how do we treat them in bayesian methods? My intuition is telling me that I should include all of them in trip report generation, but not in classification tasks;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ji6UVIgVMFM8"
   },
   "source": [
    "### 4.3.3 Addressing Substance co-occurence\n",
    "\n",
    "TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgS72yzBMFM9"
   },
   "source": [
    "### 4.3.4 Text Parsing: Language Syntax and Structure\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "[Referece](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)\n",
    "- POS tagging: What are the top nouns, verbs, adjectives etc. associated with each substance?\n",
    "- Shallow parsing or chunking\n",
    "- Constituency Parsing\n",
    "- Dependency Parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCF-7LBMMFM-"
   },
   "source": [
    "### 4.3.5 Text Parsing: Named Entity Recognition\n",
    "TODO\n",
    "\n",
    "- TODO: This can absolutely be part of EDA!!\n",
    "\n",
    "![Named Entity Recognition Example](./images/infographic_NER.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TG9ZKDodCsyq"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "sentence = str(news_df.iloc[1].full_text)\n",
    "sentence_nlp = nlp(sentence)\n",
    "\n",
    "# visualize named entities\n",
    "displacy.render(sentence_nlp, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtL_hbqDMFM-"
   },
   "source": [
    "### 4.3.6 Text Parsing: Sentiment Analysis\n",
    "- TODO: Show distribution of sentiment across all trip reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_07skQvMFM_"
   },
   "source": [
    "### 4.3.7 Treatment of the Longtail\n",
    "\n",
    "As mentioned, there were a substantial number of substance in the corpus, and it was unfeasible to analyze all the substances. In the end we decided to focus on 20 labels, which was chosen both for their pharmalogical properties (psychedelics or most closely resembling psychedelics), and also their frequencies. All the substances chosen in the following list were among the most frequently occuring substances in the corpus\n",
    "\n",
    "- substance.mushrooms\n",
    "- substance.lsd\n",
    "- substance.mescaline\n",
    "- substance.cannabis\n",
    "- substance.mdma\n",
    "- substance.ayahuasca\n",
    "- substance.nitrous_oxide\n",
    "- substance.salvia\n",
    "- substance.methamphetamine\n",
    "- substance.dmt\n",
    "- substance.5_meo_dmt\n",
    "- substance.alcohol\n",
    "- substance.ketamine\n",
    "- substance.ibogaine\n",
    "- substance.pcp\n",
    "- substance.kava\n",
    "- substance.kratom\n",
    "- substance.morning_glory\n",
    "- substance.syrian_rue\n",
    "- substance.unknown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2hZ6KUTMFNA"
   },
   "source": [
    "\n",
    "## 4.4 The Data: Clean and Filtered\n",
    "\n",
    "The following data frame represents the data that has been cleaned, filtered, and encoded, and will be used in all subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nUsjFoUjCsyy"
   },
   "outputs": [],
   "source": [
    "pd_trips_encoded_normalized.to_csv(\"pd_trips_encoded_normalized.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEw8aAZfMFNa",
    "outputId": "b3524c3c-d251-4f9f-d028-a6e3c8281631"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>report</th>\n",
       "      <th>title</th>\n",
       "      <th>substance</th>\n",
       "      <th>substance_mushrooms</th>\n",
       "      <th>substance_lsd</th>\n",
       "      <th>substance_mescaline</th>\n",
       "      <th>substance_cannabis</th>\n",
       "      <th>substance_mdma</th>\n",
       "      <th>substance_ayahuasca</th>\n",
       "      <th>...</th>\n",
       "      <th>substance_ketamine</th>\n",
       "      <th>substance_ibogaine</th>\n",
       "      <th>substance_pcp</th>\n",
       "      <th>substance_kava</th>\n",
       "      <th>substance_kratom</th>\n",
       "      <th>substance_morning_glory</th>\n",
       "      <th>substance_syrian_rue</th>\n",
       "      <th>substance_unknown</th>\n",
       "      <th>substance_UNK</th>\n",
       "      <th>substance_unique_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>success forms legal highs , decided experiment...</td>\n",
       "      <td>Sideways World</td>\n",
       "      <td>salvia divinorum (5x extract)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.salvia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>couple buddies decided one night past february...</td>\n",
       "      <td>Physical Wellbeing = Crucial</td>\n",
       "      <td>mushrooms</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.mushrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>girlfriend saving methylone special occasion ,...</td>\n",
       "      <td>The Artful Dodger</td>\n",
       "      <td>methylone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.methamphetamine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>want warn anybody taking lithium ( probably ps...</td>\n",
       "      <td>Seizure Inducing Combo</td>\n",
       "      <td>lsd &amp; lithium</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.lsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>several attempts breakthrough 5-meo-dmt. belie...</td>\n",
       "      <td>Enlightenment Through a Chemical Catalyst</td>\n",
       "      <td>5-meo-dmt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.5_meo_dmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>interesting experience salvia nights ago want ...</td>\n",
       "      <td>Park Overhangs and Dog Hair</td>\n",
       "      <td>salvia divinorum (6x extract)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.salvia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>first time tripped ganja 3 years ago smoked pu...</td>\n",
       "      <td>Just Feeling Everything So Much</td>\n",
       "      <td>cannabis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.cannabis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>two dxm dont ask doses , toting head full 30mg...</td>\n",
       "      <td>Mormon Lake</td>\n",
       "      <td>2c-t-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>first , suggested title shows , totally clear ...</td>\n",
       "      <td>Bad Vendors Sold Lies w/ Most Experiences Here</td>\n",
       "      <td>6-apb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>brief background order : two adult males , wei...</td>\n",
       "      <td>A Word of Caution</td>\n",
       "      <td>2-c-t-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>told friend problems excessive marijuana smoki...</td>\n",
       "      <td>Luck of the Irish</td>\n",
       "      <td>acacia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>enthusiest psychodelic experience , feel oblig...</td>\n",
       "      <td>Visions of Terror</td>\n",
       "      <td>mushrooms - p. cubensis &amp; cannabis</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>bought resin net disappointed find could find ...</td>\n",
       "      <td>Stepping Up Dose of Resin Tea</td>\n",
       "      <td>kratom (extract)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.kratom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>took 1 ( 750mg ) late-mid day , felt effects s...</td>\n",
       "      <td>Nearly Useless</td>\n",
       "      <td>methocarbamol (robaxin)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.methamphetamine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>though not chemical imbibing psychonaut seem ,...</td>\n",
       "      <td>16 Hour Trip Through Heart Pounding Hell.</td>\n",
       "      <td>h.b. woodrose</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>thai friend lives southern thailand quite busi...</td>\n",
       "      <td>Southern Thailand Breakfast of Champions</td>\n",
       "      <td>kratom (fresh leaves)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.kratom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>pretty brief report , abnormal interesting rel...</td>\n",
       "      <td>An Odd Occurrence</td>\n",
       "      <td>cannabis &amp; kratom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>substance : alcohol 2c-i setting : club home e...</td>\n",
       "      <td>Too Much</td>\n",
       "      <td>2c-i &amp; alcohol</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.alcohol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>completely inexperienced regards drugs never t...</td>\n",
       "      <td>Sore Throat Reaction</td>\n",
       "      <td>morning glory</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.morning_glory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>tend consider someone fairly experienced realm...</td>\n",
       "      <td>Very Similiar to Shrooms</td>\n",
       "      <td>4-ho-met</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.methamphetamine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                             report  \\\n",
       "0            1  success forms legal highs , decided experiment...   \n",
       "1            2  couple buddies decided one night past february...   \n",
       "2            3  girlfriend saving methylone special occasion ,...   \n",
       "3            4  want warn anybody taking lithium ( probably ps...   \n",
       "4            5  several attempts breakthrough 5-meo-dmt. belie...   \n",
       "5            6  interesting experience salvia nights ago want ...   \n",
       "6            7  first time tripped ganja 3 years ago smoked pu...   \n",
       "7            8  two dxm dont ask doses , toting head full 30mg...   \n",
       "8            9  first , suggested title shows , totally clear ...   \n",
       "9           10  brief background order : two adult males , wei...   \n",
       "10          11  told friend problems excessive marijuana smoki...   \n",
       "11          12  enthusiest psychodelic experience , feel oblig...   \n",
       "12          13  bought resin net disappointed find could find ...   \n",
       "13          14  took 1 ( 750mg ) late-mid day , felt effects s...   \n",
       "14          15  though not chemical imbibing psychonaut seem ,...   \n",
       "15          16  thai friend lives southern thailand quite busi...   \n",
       "16          17  pretty brief report , abnormal interesting rel...   \n",
       "17          18  substance : alcohol 2c-i setting : club home e...   \n",
       "18          19  completely inexperienced regards drugs never t...   \n",
       "19          20  tend consider someone fairly experienced realm...   \n",
       "\n",
       "                                             title  \\\n",
       "0                                   Sideways World   \n",
       "1                     Physical Wellbeing = Crucial   \n",
       "2                                The Artful Dodger   \n",
       "3                           Seizure Inducing Combo   \n",
       "4        Enlightenment Through a Chemical Catalyst   \n",
       "5                      Park Overhangs and Dog Hair   \n",
       "6                  Just Feeling Everything So Much   \n",
       "7                                      Mormon Lake   \n",
       "8   Bad Vendors Sold Lies w/ Most Experiences Here   \n",
       "9                                A Word of Caution   \n",
       "10                               Luck of the Irish   \n",
       "11                               Visions of Terror   \n",
       "12                   Stepping Up Dose of Resin Tea   \n",
       "13                                  Nearly Useless   \n",
       "14       16 Hour Trip Through Heart Pounding Hell.   \n",
       "15        Southern Thailand Breakfast of Champions   \n",
       "16                               An Odd Occurrence   \n",
       "17                                        Too Much   \n",
       "18                            Sore Throat Reaction   \n",
       "19                        Very Similiar to Shrooms   \n",
       "\n",
       "                             substance  substance_mushrooms  substance_lsd  \\\n",
       "0        salvia divinorum (5x extract)                    0              0   \n",
       "1                            mushrooms                    1              0   \n",
       "2                            methylone                    0              0   \n",
       "3                        lsd & lithium                    0              1   \n",
       "4                            5-meo-dmt                    0              0   \n",
       "5        salvia divinorum (6x extract)                    0              0   \n",
       "6                             cannabis                    0              0   \n",
       "7                               2c-t-2                    0              0   \n",
       "8                                6-apb                    0              0   \n",
       "9                              2-c-t-2                    0              0   \n",
       "10                              acacia                    0              0   \n",
       "11  mushrooms - p. cubensis & cannabis                    1              0   \n",
       "12                    kratom (extract)                    0              0   \n",
       "13             methocarbamol (robaxin)                    0              0   \n",
       "14                       h.b. woodrose                    0              0   \n",
       "15               kratom (fresh leaves)                    0              0   \n",
       "16                   cannabis & kratom                    0              0   \n",
       "17                      2c-i & alcohol                    0              0   \n",
       "18                       morning glory                    0              0   \n",
       "19                            4-ho-met                    0              0   \n",
       "\n",
       "    substance_mescaline  substance_cannabis  substance_mdma  \\\n",
       "0                     0                   0               0   \n",
       "1                     0                   0               0   \n",
       "2                     0                   0               0   \n",
       "3                     0                   0               0   \n",
       "4                     0                   0               0   \n",
       "5                     0                   0               0   \n",
       "6                     0                   1               0   \n",
       "7                     0                   0               0   \n",
       "8                     0                   0               0   \n",
       "9                     0                   0               0   \n",
       "10                    0                   0               0   \n",
       "11                    0                   1               0   \n",
       "12                    0                   0               0   \n",
       "13                    0                   0               0   \n",
       "14                    0                   0               0   \n",
       "15                    0                   0               0   \n",
       "16                    0                   1               0   \n",
       "17                    0                   0               0   \n",
       "18                    0                   0               0   \n",
       "19                    0                   0               0   \n",
       "\n",
       "    substance_ayahuasca  ...  substance_ketamine  substance_ibogaine  \\\n",
       "0                     0  ...                   0                   0   \n",
       "1                     0  ...                   0                   0   \n",
       "2                     0  ...                   0                   0   \n",
       "3                     0  ...                   0                   0   \n",
       "4                     0  ...                   0                   0   \n",
       "5                     0  ...                   0                   0   \n",
       "6                     0  ...                   0                   0   \n",
       "7                     0  ...                   0                   0   \n",
       "8                     0  ...                   0                   0   \n",
       "9                     0  ...                   0                   0   \n",
       "10                    0  ...                   0                   0   \n",
       "11                    0  ...                   0                   0   \n",
       "12                    0  ...                   0                   0   \n",
       "13                    0  ...                   0                   0   \n",
       "14                    0  ...                   0                   0   \n",
       "15                    0  ...                   0                   0   \n",
       "16                    0  ...                   0                   0   \n",
       "17                    0  ...                   0                   0   \n",
       "18                    0  ...                   0                   0   \n",
       "19                    0  ...                   0                   0   \n",
       "\n",
       "    substance_pcp  substance_kava  substance_kratom  substance_morning_glory  \\\n",
       "0               0               0                 0                        0   \n",
       "1               0               0                 0                        0   \n",
       "2               0               0                 0                        0   \n",
       "3               0               0                 0                        0   \n",
       "4               0               0                 0                        0   \n",
       "5               0               0                 0                        0   \n",
       "6               0               0                 0                        0   \n",
       "7               0               0                 0                        0   \n",
       "8               0               0                 0                        0   \n",
       "9               0               0                 0                        0   \n",
       "10              0               0                 0                        0   \n",
       "11              0               0                 0                        0   \n",
       "12              0               0                 1                        0   \n",
       "13              0               0                 0                        0   \n",
       "14              0               0                 0                        0   \n",
       "15              0               0                 1                        0   \n",
       "16              0               0                 1                        0   \n",
       "17              0               0                 0                        0   \n",
       "18              0               0                 0                        1   \n",
       "19              0               0                 0                        0   \n",
       "\n",
       "    substance_syrian_rue  substance_unknown  substance_UNK  \\\n",
       "0                      0                  0              0   \n",
       "1                      0                  0              0   \n",
       "2                      0                  0              0   \n",
       "3                      0                  0              0   \n",
       "4                      0                  0              0   \n",
       "5                      0                  0              0   \n",
       "6                      0                  0              0   \n",
       "7                      0                  0              0   \n",
       "8                      0                  0              0   \n",
       "9                      0                  0              0   \n",
       "10                     0                  0              0   \n",
       "11                     0                  0              0   \n",
       "12                     0                  0              0   \n",
       "13                     0                  0              0   \n",
       "14                     0                  0              0   \n",
       "15                     0                  0              0   \n",
       "16                     0                  0              0   \n",
       "17                     0                  0              0   \n",
       "18                     0                  0              0   \n",
       "19                     0                  0              0   \n",
       "\n",
       "       substance_unique_label  \n",
       "0            substance.salvia  \n",
       "1         substance.mushrooms  \n",
       "2   substance.methamphetamine  \n",
       "3               substance.lsd  \n",
       "4         substance.5_meo_dmt  \n",
       "5            substance.salvia  \n",
       "6          substance.cannabis  \n",
       "7                         NaN  \n",
       "8                         NaN  \n",
       "9                         NaN  \n",
       "10                        NaN  \n",
       "11                        NaN  \n",
       "12           substance.kratom  \n",
       "13  substance.methamphetamine  \n",
       "14                        NaN  \n",
       "15           substance.kratom  \n",
       "16                        NaN  \n",
       "17          substance.alcohol  \n",
       "18    substance.morning_glory  \n",
       "19  substance.methamphetamine  \n",
       "\n",
       "[20 rows x 26 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 20 rows of the cleaned data\n",
    "pd_trips_encoded_normalized.head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKaS38QhMFNb"
   },
   "source": [
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xl-VjIXkMFNc"
   },
   "source": [
    "\n",
    "\n",
    "# 5. Text Presentation and Feature Engineering\n",
    "\n",
    "- TODO: Bag of words is SO easy with `sklearn` and `pandas`(mostly for display purposes)  \n",
    "- TODO: Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fM6_hS1AMFNd"
   },
   "source": [
    "## 5.1 Word as Vectors\n",
    "\n",
    "For those unfamiliar with natural language processing, we introduce a simple but very central and powerful idea: **_`words` can be represented as a vector of real numbers_**. This may seem counterintuitive, but the ability to represent words as vectors is absolutely key to our ability to perform analysis on unstructured text because (1) almost all machine learning algorithms take vectors or matrixes as inputs and (2) it opens us to a world of vector and matrix operations such as dot products and cosine similarity, which allow us to ask (and answer) questions such as _`how **similar** are two words?`_ intelligently and precisely.\n",
    "\n",
    "![word as vectors intuition](./images/infographic_word-as-vectors-overview.png)\n",
    "[`Infographic Source`](https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf)\n",
    "\n",
    "In the above infographic, each word is represented by 4 numbers, or 4 dimensions, representing a word's `animal`-ness, `domesticated`-ness, `pet`-ness, `fluffy`-ness. With this, we can ask whether a monkey is more similar to an elephant or more similar to a cheetah using cosine similiarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_qOR6vgCsy4",
    "outputId": "f586b784-3cd6-4d79-c425-6e7f53378a80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8251926299381945"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Illustration of cosine similarity\n",
    "# It looks like a monkey is more similar to a cheetah than it is to an elephant!\n",
    "monkey = np.array([-0.02, -0.67, -0.21, -0.48])\n",
    "elephant = np.array([-0.04, -0.09, 0.11, -0.06])\n",
    "cheetah = np.array([0.27, -0.28, -0.2, -0.43])\n",
    "\n",
    "# cosine similarity relies on the intuition that the smaller the angle between two vectors, the more similar they are\n",
    "def cos_sim(v1, v2):\n",
    "    sim = np.dot(v1, v2) / (np.sqrt(sum(v1**2)) * np.sqrt(sum(v2**2)))\n",
    "    return sim\n",
    "\n",
    "cos_sim(monkey, elephant) # 0.4926634171010775\n",
    "cos_sim(monkey, cheetah) # 0.8251926299381945"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8DbzXiXgCsy6"
   },
   "source": [
    "**Words are represented as vectors mainly in two ways (see [here](https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf) also):**\n",
    "1. count of word / context co-occurences\n",
    "2. vectors that naturally arise from trying to predict the context of a word (GloVE, Skipgram models)\n",
    "\n",
    "\n",
    "The first class of methods relies on the distributional hypothesis in linguistic (todo), which formalized the intuition that _words that co-occur together in the same contexts tend to be more similar_. There are two main ways to construct vectors this way:\n",
    "- **`term-document matrix`**: Very briefly, supposed we had a vocabulary (unique tokens) of size V, and a collection of documents (such as wikipedia pages) of size D. We can make a V x D matrix `TDM`, with words as the rows and documents as the columns. The position M(i, j) would then how many times word i occurs in document j. Just like that, we have created a *term-document matrix*, one of the most important building blocks in natural language processing. Recall that each row represents a word, and each row is a vector of dimension D. In this precise way, we have just represented each of the V words in our vocabulary as a vector of dimension D.\n",
    "- **`term-term matrix`**: now, instead of using a V X D matrix, we can make a V x V matrix `TTM`, and figure out a sensible way to populate the elements. One intuitive way is to take a \"sliding window\" with some width over the text, and count how many times some word j occurs in the context window of word i. That count would then go into TTM(i, j). In this way, each word is represented as V dimensional vector.\n",
    "\n",
    "Note that because V and D are often on the order of millions, these vector representations are absolutely huge, and many fields would be 0. For this reason, these are also called **_sparse_ vector embeddings**.\n",
    "\n",
    "\n",
    "The second class of methods rely on the intuition that if we use words as inputs to peform some machine learning task (such as predicting the most likely surrounding context words given a current word using a neural network, say), the weights and biases learned by the neural network will act as a \"good\" approximations for the vector representation of the word. \n",
    "\n",
    "The most commonly used models are `Word2Vec` (CBOW and Negative Skip Gram Sampling) by Google, `GloVe Model` from Stanford University, and `FastText Model` by Facebook. Although the three models rely on different machine learning tasks, they all rely on randomly intialized vectors converging to vector approximatiosn of words as the machine learning task trains. The exact details are beyond the scope of this thesis, but there are 3 very important takeaways: (1) The dimensions of the word vectors are arbitrary and user specified, (2) The numbers in the learned vectors don't have _intrinsic_ meaning like how the ones in the sparse representations above do. The learned vector for a word is an abstract embedding in a higher dimensional space, derived from a particular training corpus and particular algorithm. There is no _single_ vector that **_is_** a particular word. Finally, (3) Since the dimensions are user specified they are typically kept to around 100 to 300, and as such these word vectors are often called **_dense vector embeddings_**\n",
    "\n",
    "For those interested, here are three infographics that provide some context for the different algorithms, examples of embeddings learned, and links to the original papers that described the different papers. Notice here that words in the `Word2Vec` are 100-dimensional vector embeddings, and words in the `GloVe` example are 300-dimensional vector emeddings. Indeed, multiple pre trained models with different initial configurations (such as dimension of embedding and size of training corpus) are readily available for use. For a deeper yet approachable introduction to word embeddings arising from deep learning models, Diparjan Sarka's [article](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa) on Towards Data Science is an excellent place to start\n",
    "![Word2Vec Models](./images/infographic_dl-wordvec-word2vec.png)\n",
    "![GloVe Models](./images/infographic_dl-wordvec-glove.png)\n",
    "![Fasttext Models](./images/infographic_dl-wordvec-fasttext.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KTiGqA6xMFNd"
   },
   "source": [
    "## 5.2 Documents as Vectors\n",
    "\n",
    "Once the idea that words can be represented as vectors sinks in, the idea that entire documents can be represented as vectors feel very intuitive. Recall in the previous section we saw a **`term-document matrix`**, which was a V x D matrix with terms as rows and documents as columns. Taking the rows, we represented each term as a D-dimensional vector. On the flip side, if we simply took the columns, we could represent documents as V dimensional vectors. This results in a **sparse embedding** that represents a document.\n",
    "\n",
    "We can also utilize the fact that documents are a collection of words, which can be represented by vectors. As such, we can simply take the average of all the word vectors in a document, and call that our document vector. This results in a **dense embedding** that represents a document. In practice, this seemingly naive method of averaging works surprisingly well, and the intuition of averaging word vectors is agnostic to which word embedding was chosen, be it `GloVe`, `Word2Vec`, `FastText` or a DIY embeddng. One commonly used document embedding is Google's `Doc2Vec`, which, you guessed it, builds on top of `Word2Vec` word embeddings.\n",
    "\n",
    "\n",
    "\n",
    "## 5.3 Vectorizing our Corpus with Bag of Word (BOW) Features\n",
    "\n",
    "Now that we have introduced the important intuition that words and documents can be represented as vectors, we can bring this theory into practice and vectorize our corpus. Let us begin with the basics and slowly get more complex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14C-PaCjMFNe"
   },
   "source": [
    "### 5.4.1 Term-Document Counts: The Very Basics\n",
    "\n",
    "This corresponds to constructing the aforementioned **`term-document matrix`**, where we construct a V x D matrix from the corpus, where each document is represented with a V dimensional vector. `sklean` has a built in vectorzier called `CountVectorizer` which we can use directly on our normalized corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "colab_type": "code",
    "id": "zzNVwXnKCsy8",
    "outputId": "933748cf-e2d4-4d26-abff-be904525fc10",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>report</th>\n",
       "      <th>title</th>\n",
       "      <th>substance</th>\n",
       "      <th>substance_mushrooms</th>\n",
       "      <th>substance_lsd</th>\n",
       "      <th>substance_mescaline</th>\n",
       "      <th>substance_cannabis</th>\n",
       "      <th>substance_mdma</th>\n",
       "      <th>substance_ayahuasca</th>\n",
       "      <th>substance_nitrous_oxide</th>\n",
       "      <th>substance_salvia</th>\n",
       "      <th>substance_methamphetamine</th>\n",
       "      <th>substance_dmt</th>\n",
       "      <th>substance_5_meo_dmt</th>\n",
       "      <th>substance_alcohol</th>\n",
       "      <th>substance_ketamine</th>\n",
       "      <th>substance_ibogaine</th>\n",
       "      <th>substance_pcp</th>\n",
       "      <th>substance_kava</th>\n",
       "      <th>substance_kratom</th>\n",
       "      <th>substance_morning_glory</th>\n",
       "      <th>substance_syrian_rue</th>\n",
       "      <th>substance_unknown</th>\n",
       "      <th>substance_UNK</th>\n",
       "      <th>substance_unique_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>success forms legal highs , decided experiment...</td>\n",
       "      <td>Sideways World</td>\n",
       "      <td>salvia divinorum (5x extract)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.salvia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>couple buddies decided one night past february...</td>\n",
       "      <td>Physical Wellbeing = Crucial</td>\n",
       "      <td>mushrooms</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.mushrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>girlfriend saving methylone special occasion ,...</td>\n",
       "      <td>The Artful Dodger</td>\n",
       "      <td>methylone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.methamphetamine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>want warn anybody taking lithium ( probably ps...</td>\n",
       "      <td>Seizure Inducing Combo</td>\n",
       "      <td>lsd &amp; lithium</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.lsd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>several attempts breakthrough 5-meo-dmt. belie...</td>\n",
       "      <td>Enlightenment Through a Chemical Catalyst</td>\n",
       "      <td>5-meo-dmt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>substance.5_meo_dmt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...     substance_unique_label\n",
       "0           1  ...           substance.salvia\n",
       "1           2  ...        substance.mushrooms\n",
       "2           3  ...  substance.methamphetamine\n",
       "3           4  ...              substance.lsd\n",
       "4           5  ...        substance.5_meo_dmt\n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTIONAL: In case Kernel dies, use this to load back dataset\n",
    "# import pickle\n",
    "import pandas as pd\n",
    "pd_trips_encoded_normalized = pd.read_csv('{}/pd_trips_encoded_normalized.csv'.format(root_path))\n",
    "# with open('pd_trips_encoded_normalized.csv', 'rb') as in_file:\n",
    "#     pd_trips_encoded_normalized = pickle.load(in_file)\n",
    "pd_trips_encoded_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "059EGHDpCszC",
    "outputId": "0e124423-e877-40bc-8fa5-aaa97a354dc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: for some very strange reason, reading the csv file back resulted in np.nan trip reports\n",
    "pd_trips_encoded_normalized[\"report\"].isnull().sum()\n",
    "# dropna: https://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-in-a-certain-column-is-nan\n",
    "pd_trips_encoded_normalized.dropna(subset=['report'], inplace=True)\n",
    "pd_trips_encoded_normalized[\"report\"].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "colab_type": "code",
    "id": "um9Ak6lNMFNe",
    "outputId": "514b82b3-f8f4-42d3-9c16-b1f49f3132f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>00000000</th>\n",
       "      <th>00000000000000000000000000003</th>\n",
       "      <th>00005</th>\n",
       "      <th>0001</th>\n",
       "      <th>0001g</th>\n",
       "      <th>0002</th>\n",
       "      <th>0005</th>\n",
       "      <th>000ft</th>\n",
       "      <th>000mcg</th>\n",
       "      <th>000mg</th>\n",
       "      <th>000s</th>\n",
       "      <th>000ths</th>\n",
       "      <th>000x</th>\n",
       "      <th>001</th>\n",
       "      <th>0010</th>\n",
       "      <th>0015</th>\n",
       "      <th>001g</th>\n",
       "      <th>001mg</th>\n",
       "      <th>002</th>\n",
       "      <th>0025</th>\n",
       "      <th>002g</th>\n",
       "      <th>002mg</th>\n",
       "      <th>003</th>\n",
       "      <th>0030</th>\n",
       "      <th>003mg</th>\n",
       "      <th>0040</th>\n",
       "      <th>0045</th>\n",
       "      <th>004598</th>\n",
       "      <th>0047</th>\n",
       "      <th>005</th>\n",
       "      <th>0051</th>\n",
       "      <th>005g</th>\n",
       "      <th>005mg</th>\n",
       "      <th>006</th>\n",
       "      <th>00600</th>\n",
       "      <th>007</th>\n",
       "      <th>...</th>\n",
       "      <th>zwingt</th>\n",
       "      <th>zwischen</th>\n",
       "      <th>zwischendurch</th>\n",
       "      <th>zwischendursch</th>\n",
       "      <th>zwischenfall</th>\n",
       "      <th>zwischenstellung</th>\n",
       "      <th>zwischenwege</th>\n",
       "      <th>zwischenzeiten</th>\n",
       "      <th>zwischenzeitlich</th>\n",
       "      <th>zwittwerionic</th>\n",
       "      <th>zworfin</th>\n",
       "      <th>zx81</th>\n",
       "      <th>zyban</th>\n",
       "      <th>zybans</th>\n",
       "      <th>zycam</th>\n",
       "      <th>zydis</th>\n",
       "      <th>zygomatics</th>\n",
       "      <th>zygophyllaceae</th>\n",
       "      <th>zylonxxi</th>\n",
       "      <th>zynex</th>\n",
       "      <th>zyprexa</th>\n",
       "      <th>zyprexia</th>\n",
       "      <th>zyrem</th>\n",
       "      <th>zyrtec</th>\n",
       "      <th>zyvox</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzoooow</th>\n",
       "      <th>zztop</th>\n",
       "      <th>zzz</th>\n",
       "      <th>zzzrrrrmm</th>\n",
       "      <th>zzzs</th>\n",
       "      <th>zzzzang</th>\n",
       "      <th>zzzzz</th>\n",
       "      <th>zzzzzz</th>\n",
       "      <th>zzzzzzzap</th>\n",
       "      <th>zzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzuuuuurrrrrrzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19910</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19911</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19912</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19914</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19915 rows √ó 107108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  ...  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
       "0       0  ...                                                  0                                                                \n",
       "1       0  ...                                                  0                                                                \n",
       "2       0  ...                                                  0                                                                \n",
       "3       0  ...                                                  0                                                                \n",
       "4       0  ...                                                  0                                                                \n",
       "...    ..  ...                                                ...                                                                \n",
       "19910   0  ...                                                  0                                                                \n",
       "19911   0  ...                                                  0                                                                \n",
       "19912   0  ...                                                  0                                                                \n",
       "19913   0  ...                                                  0                                                                \n",
       "19914   0  ...                                                  0                                                                \n",
       "\n",
       "[19915 rows x 107108 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code adapted from: \n",
    "# https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer expects a list of documents; i.e.: [string]\n",
    "norm_corpus = pd_trips_encoded_normalized[\"report\"].to_list()\n",
    "len(norm_corpus) # 19924 reports\n",
    "\n",
    "# instead of using raw counts, we can divide matrix(i, j) by sum over column j to get a proporition\n",
    "# min_df = minimum document frequency\n",
    "# max_df = maximum document frequency\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "\n",
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)\n",
    "\n",
    "############################################\n",
    "## Ngram model extensions\n",
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "# bv = CountVectorizer(ngram_range=(2,2))\n",
    "# bv_matrix = bv.fit_transform(norm_corpus)\n",
    "\n",
    "# bv_matrix = bv_matrix.toarray()\n",
    "# vocab = bv.get_feature_names()\n",
    "# pd.DataFrame(bv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "zPBQ6_1DCszp",
    "outputId": "3ee45f8d-7d79-47da-9843-8750ed94e945"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clasify',\n",
       " 'clasp',\n",
       " 'clasped',\n",
       " 'clasping',\n",
       " 'clasps',\n",
       " 'class',\n",
       " 'classa',\n",
       " 'classed',\n",
       " 'classes',\n",
       " 'classic']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some basic properties we can learn about our data\n",
    "len(vocab) # 107108 vocabulary words, or \"tokens\", in our training corpus\n",
    "vocab[20000:20010] # example of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igozYemrCszw"
   },
   "source": [
    "Each row represents a document, and each column is a vocabulary word. We notice that the term-document matrix generated is extremely sparse (mostly zeros), and there seems to be a large number of non-sense tokens such as `00`, `000`, `0000` etc. More processing may be required to remove these tokens for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDQojCtWMFNg"
   },
   "source": [
    "### 5.4.2 TF-IDF Weighting Scheme\n",
    "\n",
    "Using just the raw word frequencies, we might overemphasize words that are shared across all trip reports, giving us little if any insight into the most significant words that is associated with each type of substance. If some word is faily rare (like \"avocado\") in general in the English language, but appeared quite few times in a single document, it makes intuitive sense to upweight the importance of that rare word with respect to that particular document. Capturing this intuition and to improve upon the raw frequency baseline, we use use the `tf-idf` for each (word, document) pair as a weighting term.\n",
    "\n",
    "`tf-idf` stands for `term frequency-inverse document frequency`, and is specified for each (word, document) pair. It is by far one of the most widely used weighting scheme in considering how important a word is to a document, and is widely used in text mining and information retrieval. For an approachable academic introduction to `tf-idf`, see Chris Callison-Burch's [lecture slides](http://computational-linguistics-class.org/slides/05-vector-semantics.pdf)  \n",
    "\n",
    "- `tf` stands for `Term Frequency` ([Luhn 1957](http://web.stanford.edu/class/linguist289/luhn57.pdf)). The term trequency of word i in document j is given by:\n",
    "$tf_{ij}$ = # of times word $i$ appears in document $j$\n",
    "\n",
    "- `idf` stands for `Inverse Document Frequency` ([Spark Jones 1972](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.8343&rep=rep1&type=pdf)). The inverse document frequency of word $i$ is given by\n",
    "\n",
    "$$idf_i = log\\left(\\cfrac{N}{df_i}\\right)$$\n",
    "\n",
    "Where $df_i$ is the `document frequency of word i`, the number of documents containing word $i$  \n",
    "\n",
    "With these definitions, we stay the tf-idf weighting of word $i$ in document $j$ is given by\n",
    "\n",
    "$$ tf.idf(i,j) = tf_{ij} \\cdot idf_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y95Cn_crMFNh"
   },
   "outputs": [],
   "source": [
    "# TFIDF Code\n",
    "# adapted from: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np #TODO: remove\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(norm_corpus)\n",
    "dtm_tfidf_nparr = dtm_tfidf.toarray()\n",
    "\n",
    "vocab = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(np.round(dtm_tfidf_nparr, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kp0vurC_Csz0"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "## Use this cell as a template to save and load large data files; \n",
    "## replace filenames appropriately;\n",
    "## current code saves and loads `reports_text_tokenized`\n",
    "\n",
    "import pickle\n",
    "\n",
    "### Stores the tokenized reports:\n",
    "with open('dtm_tfidf', 'wb') as out_file:\n",
    "    pickle.dump(dtm_tfidf, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SYmKZ_9BMFNk"
   },
   "source": [
    "\n",
    "### 5.4.3 Document Similarity matrix: Bag of Words\n",
    "\n",
    "We can use these vectorized representations of documents to ask: how similar are two documents? Or rather, how similar are any two trip reports to each other? With this question in mind, we can construct a similiarity matrix based on cosine similarity. Now, the following matrix is 19924 x 19924, and is very hard to visualize and interpret. Nevertheless, we include it here for completeness as one way to visualize the relationship between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHkteyrGMFNl",
    "outputId": "759871e7-380d-44a3-d64b-0a2dc05b529c",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19914</th>\n",
       "      <th>19915</th>\n",
       "      <th>19916</th>\n",
       "      <th>19917</th>\n",
       "      <th>19918</th>\n",
       "      <th>19919</th>\n",
       "      <th>19920</th>\n",
       "      <th>19921</th>\n",
       "      <th>19922</th>\n",
       "      <th>19923</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129429</td>\n",
       "      <td>0.090625</td>\n",
       "      <td>0.076225</td>\n",
       "      <td>0.131630</td>\n",
       "      <td>0.182618</td>\n",
       "      <td>0.127825</td>\n",
       "      <td>0.053286</td>\n",
       "      <td>0.083772</td>\n",
       "      <td>0.062685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066529</td>\n",
       "      <td>0.104080</td>\n",
       "      <td>0.102838</td>\n",
       "      <td>0.069377</td>\n",
       "      <td>0.142459</td>\n",
       "      <td>0.099818</td>\n",
       "      <td>0.108146</td>\n",
       "      <td>0.114570</td>\n",
       "      <td>0.249888</td>\n",
       "      <td>0.139532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074810</td>\n",
       "      <td>0.070011</td>\n",
       "      <td>0.112749</td>\n",
       "      <td>0.083889</td>\n",
       "      <td>0.096787</td>\n",
       "      <td>0.066591</td>\n",
       "      <td>0.069298</td>\n",
       "      <td>0.052987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045423</td>\n",
       "      <td>0.074825</td>\n",
       "      <td>0.089213</td>\n",
       "      <td>0.091410</td>\n",
       "      <td>0.079713</td>\n",
       "      <td>0.060991</td>\n",
       "      <td>0.071574</td>\n",
       "      <td>0.081623</td>\n",
       "      <td>0.134867</td>\n",
       "      <td>0.124445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.090625</td>\n",
       "      <td>0.074810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055006</td>\n",
       "      <td>0.082738</td>\n",
       "      <td>0.056040</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>0.033554</td>\n",
       "      <td>0.091089</td>\n",
       "      <td>0.075050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035420</td>\n",
       "      <td>0.086503</td>\n",
       "      <td>0.063140</td>\n",
       "      <td>0.059507</td>\n",
       "      <td>0.067507</td>\n",
       "      <td>0.061085</td>\n",
       "      <td>0.039179</td>\n",
       "      <td>0.097798</td>\n",
       "      <td>0.117534</td>\n",
       "      <td>0.095884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.076225</td>\n",
       "      <td>0.070011</td>\n",
       "      <td>0.055006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056043</td>\n",
       "      <td>0.057283</td>\n",
       "      <td>0.083659</td>\n",
       "      <td>0.043516</td>\n",
       "      <td>0.053096</td>\n",
       "      <td>0.025359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022620</td>\n",
       "      <td>0.068703</td>\n",
       "      <td>0.046551</td>\n",
       "      <td>0.032524</td>\n",
       "      <td>0.092305</td>\n",
       "      <td>0.063310</td>\n",
       "      <td>0.061366</td>\n",
       "      <td>0.061464</td>\n",
       "      <td>0.083162</td>\n",
       "      <td>0.091064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.131630</td>\n",
       "      <td>0.112749</td>\n",
       "      <td>0.082738</td>\n",
       "      <td>0.056043</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101234</td>\n",
       "      <td>0.109360</td>\n",
       "      <td>0.032017</td>\n",
       "      <td>0.077451</td>\n",
       "      <td>0.076005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039528</td>\n",
       "      <td>0.072256</td>\n",
       "      <td>0.073312</td>\n",
       "      <td>0.070553</td>\n",
       "      <td>0.100205</td>\n",
       "      <td>0.044899</td>\n",
       "      <td>0.053293</td>\n",
       "      <td>0.096512</td>\n",
       "      <td>0.153826</td>\n",
       "      <td>0.105326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19919</th>\n",
       "      <td>0.099818</td>\n",
       "      <td>0.060991</td>\n",
       "      <td>0.061085</td>\n",
       "      <td>0.063310</td>\n",
       "      <td>0.044899</td>\n",
       "      <td>0.066247</td>\n",
       "      <td>0.056920</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>0.072829</td>\n",
       "      <td>0.065669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070012</td>\n",
       "      <td>0.069409</td>\n",
       "      <td>0.039928</td>\n",
       "      <td>0.027559</td>\n",
       "      <td>0.050114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048002</td>\n",
       "      <td>0.079499</td>\n",
       "      <td>0.132459</td>\n",
       "      <td>0.082174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19920</th>\n",
       "      <td>0.108146</td>\n",
       "      <td>0.071574</td>\n",
       "      <td>0.039179</td>\n",
       "      <td>0.061366</td>\n",
       "      <td>0.053293</td>\n",
       "      <td>0.056875</td>\n",
       "      <td>0.082865</td>\n",
       "      <td>0.025507</td>\n",
       "      <td>0.059642</td>\n",
       "      <td>0.047293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025952</td>\n",
       "      <td>0.045984</td>\n",
       "      <td>0.084706</td>\n",
       "      <td>0.046487</td>\n",
       "      <td>0.046306</td>\n",
       "      <td>0.048002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069122</td>\n",
       "      <td>0.085954</td>\n",
       "      <td>0.096504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19921</th>\n",
       "      <td>0.114570</td>\n",
       "      <td>0.081623</td>\n",
       "      <td>0.097798</td>\n",
       "      <td>0.061464</td>\n",
       "      <td>0.096512</td>\n",
       "      <td>0.111946</td>\n",
       "      <td>0.101855</td>\n",
       "      <td>0.042325</td>\n",
       "      <td>0.086287</td>\n",
       "      <td>0.081434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067830</td>\n",
       "      <td>0.099294</td>\n",
       "      <td>0.078563</td>\n",
       "      <td>0.060969</td>\n",
       "      <td>0.088309</td>\n",
       "      <td>0.079499</td>\n",
       "      <td>0.069122</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141283</td>\n",
       "      <td>0.104616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19922</th>\n",
       "      <td>0.249888</td>\n",
       "      <td>0.134867</td>\n",
       "      <td>0.117534</td>\n",
       "      <td>0.083162</td>\n",
       "      <td>0.153826</td>\n",
       "      <td>0.181411</td>\n",
       "      <td>0.145943</td>\n",
       "      <td>0.060397</td>\n",
       "      <td>0.089623</td>\n",
       "      <td>0.094661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080458</td>\n",
       "      <td>0.114012</td>\n",
       "      <td>0.112815</td>\n",
       "      <td>0.082112</td>\n",
       "      <td>0.162688</td>\n",
       "      <td>0.132459</td>\n",
       "      <td>0.085954</td>\n",
       "      <td>0.141283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.154160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19923</th>\n",
       "      <td>0.139532</td>\n",
       "      <td>0.124445</td>\n",
       "      <td>0.095884</td>\n",
       "      <td>0.091064</td>\n",
       "      <td>0.105326</td>\n",
       "      <td>0.093115</td>\n",
       "      <td>0.148861</td>\n",
       "      <td>0.039393</td>\n",
       "      <td>0.085051</td>\n",
       "      <td>0.049308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054750</td>\n",
       "      <td>0.074014</td>\n",
       "      <td>0.121766</td>\n",
       "      <td>0.071626</td>\n",
       "      <td>0.091712</td>\n",
       "      <td>0.082174</td>\n",
       "      <td>0.096504</td>\n",
       "      <td>0.104616</td>\n",
       "      <td>0.154160</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19924 rows √ó 19924 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6      \\\n",
       "0      1.000000  0.129429  0.090625  0.076225  0.131630  0.182618  0.127825   \n",
       "1      0.129429  1.000000  0.074810  0.070011  0.112749  0.083889  0.096787   \n",
       "2      0.090625  0.074810  1.000000  0.055006  0.082738  0.056040  0.065651   \n",
       "3      0.076225  0.070011  0.055006  1.000000  0.056043  0.057283  0.083659   \n",
       "4      0.131630  0.112749  0.082738  0.056043  1.000000  0.101234  0.109360   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19919  0.099818  0.060991  0.061085  0.063310  0.044899  0.066247  0.056920   \n",
       "19920  0.108146  0.071574  0.039179  0.061366  0.053293  0.056875  0.082865   \n",
       "19921  0.114570  0.081623  0.097798  0.061464  0.096512  0.111946  0.101855   \n",
       "19922  0.249888  0.134867  0.117534  0.083162  0.153826  0.181411  0.145943   \n",
       "19923  0.139532  0.124445  0.095884  0.091064  0.105326  0.093115  0.148861   \n",
       "\n",
       "          7         8         9      ...     19914     19915     19916  \\\n",
       "0      0.053286  0.083772  0.062685  ...  0.066529  0.104080  0.102838   \n",
       "1      0.066591  0.069298  0.052987  ...  0.045423  0.074825  0.089213   \n",
       "2      0.033554  0.091089  0.075050  ...  0.035420  0.086503  0.063140   \n",
       "3      0.043516  0.053096  0.025359  ...  0.022620  0.068703  0.046551   \n",
       "4      0.032017  0.077451  0.076005  ...  0.039528  0.072256  0.073312   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "19919  0.045744  0.072829  0.065669  ...  0.070012  0.069409  0.039928   \n",
       "19920  0.025507  0.059642  0.047293  ...  0.025952  0.045984  0.084706   \n",
       "19921  0.042325  0.086287  0.081434  ...  0.067830  0.099294  0.078563   \n",
       "19922  0.060397  0.089623  0.094661  ...  0.080458  0.114012  0.112815   \n",
       "19923  0.039393  0.085051  0.049308  ...  0.054750  0.074014  0.121766   \n",
       "\n",
       "          19917     19918     19919     19920     19921     19922     19923  \n",
       "0      0.069377  0.142459  0.099818  0.108146  0.114570  0.249888  0.139532  \n",
       "1      0.091410  0.079713  0.060991  0.071574  0.081623  0.134867  0.124445  \n",
       "2      0.059507  0.067507  0.061085  0.039179  0.097798  0.117534  0.095884  \n",
       "3      0.032524  0.092305  0.063310  0.061366  0.061464  0.083162  0.091064  \n",
       "4      0.070553  0.100205  0.044899  0.053293  0.096512  0.153826  0.105326  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "19919  0.027559  0.050114  1.000000  0.048002  0.079499  0.132459  0.082174  \n",
       "19920  0.046487  0.046306  0.048002  1.000000  0.069122  0.085954  0.096504  \n",
       "19921  0.060969  0.088309  0.079499  0.069122  1.000000  0.141283  0.104616  \n",
       "19922  0.082112  0.162688  0.132459  0.085954  0.141283  1.000000  0.154160  \n",
       "19923  0.071626  0.091712  0.082174  0.096504  0.104616  0.154160  1.000000  \n",
       "\n",
       "[19924 rows x 19924 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Sample code from https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TvfP-UrwCsz4"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "with open('similarity-matrix_dtm_tfidf', 'wb') as out_file:\n",
    "    pickle.dump(similarity_df, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmAqQ_CAMFNo"
   },
   "source": [
    "### 5.6.4: Beyond Feature Engineering\n",
    "`TODO`\n",
    "\n",
    "- Autoencoders\n",
    "- Automatic Feature Selection\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0-9Bn6nMFNp"
   },
   "source": [
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRkZNK_qMFNr"
   },
   "source": [
    "# 6. Topic Modelilng: Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "`TODO`\n",
    "\n",
    "- Topic modelling with `Latent Dirichlet Allocation (LDA)`; see [`sklearn example`](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py) which uses `LDA` and `Non-negative matrix factorization (NMF)`; Also see [`LDA Documentation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation); Also see `gensim` package\n",
    "- As per conversation with Lyle Ungar, `LDA` can also give me document and word embeddings which can be used in wordclouds.\n",
    "- [Excellent Talk by Christine Doig on Topic Models](http://chdoig.github.io/pygotham-topic-modeling/#/)\n",
    "- [Blei 2012: Probabilistic Topic Models](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf); [local-link](./papers/Blei2012_Probabilistic-Topic-Models.pdf)\n",
    "- TODO: Latent Semantic Indexing (singular value decomposition)\n",
    "- TODO: Non-negative Matrix factorization (NMF)\n",
    "- LDA gives us two things:\n",
    "    - A document-topic matrix, which can be used as an input to wordcloud\n",
    "    - A topic-term matrix, which helps us in looking at potential topics in the corpus.\n",
    "\n",
    "- **TODO**: Explain how `Latent Dirichlet Allocation (LDA)` works; see Blei 2012 and Christine Doig's talk.\n",
    "- TODO: Investigate how to choose the number of topics, which apparently is an art in itself\n",
    "- LDA Visualizations:\n",
    "    - [LDAvis R Package](https://github.com/cpsievert/LDAvis)\n",
    "    - [LDAvis port Python Package](https://github.com/bmabey/pyLDAvis)\n",
    "- [Example LDA visualization Jupyter Notebook](https://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb#topic=0&lambda=1&term=)\n",
    "- [In depth PyLDAvis python tutorial](https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb)\n",
    "\n",
    "![LDA Pipeline](./images/infographic_lda-pipeline-chdoig.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "effsJXdgMFNs"
   },
   "source": [
    "## 6.1 Latent Dirichlet Allocation: An Introduction\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) has fancy sonuding name, and the inner workings are indeed fairly complex, so let us begin with what LDA is trying to do intuitivly: LDA attempts to (1) automatically discover topics (in an unsupervised manner) from a collection of documents, and (2) model each document in that collection as exhibiting multiple topics in diffrent proporitions (Blei 2012). Intuitively, one can imagine that a document such as a blog post, wikipedia article, or in our case, a psychedelic trip report, might consist of numerous `topics`; for example, a news article published in the New York Times might talk about extreme weather, deforestration, and climate change, when covering say, a hurricane. A psychedelic trip report might also have numerous topics, such as musings on time, experiences with nature, or feelings of bliss. So what *is* a `topic`? It should be noted that since LDA is an unsupervised technique, we don't require our input text documents have topic labels. Instead, LDA processes our collection of text documents, and extracts topics automatically in a unsupervised manner, where each `topic` is precisely a list of words that approximate that particular topic. For example, the following lists of words represent 4 topics extracted automatically from the Journal __Science__, where each topic is a list of words (Fig 2, Blei 2012):\n",
    "\n",
    "- `topic1`: human genome dna genetic genes sequence gene molecular sequencing map information genetics mapping project sequences\n",
    "- `topic2`: evolution evolutionary species organisms life origin biology groups phylogenetic living diversity group new two common\n",
    "- `topic3`: disease host bacteria diseases resistance bacterial new strains control infectious malaria parasite parasites united tuberculosis\n",
    "- `topic4`: computer models information data computers system network systems model parallel methods networks software new simulations\n",
    "\n",
    "Notice that topics discovered don't have \"names\" by themselves; by looking at the list of words associated with each topic above, it is reasonable to say `topic1` is approximately \"genetics\", `topic2` is approximately \"evoluation\", `topic 3` is approximately \"disease\", and `topic4` is approximately \"computers\"; in practice the labelling of topics can either by done by humans, or by automatically taking a word that is most \"relavent\" to that topic as the label, by some sensible definition of \"relavence\". Indeed, the above word list are sorted from most relavent to least relavent for each topic. The notion of \"relavence\" has a precise definition, which we defer to Section 6.3.\n",
    "\n",
    "\n",
    "This beautiful graphic presented by Blei 2012 helps us further the intuition of how each document can exhibit multiple topics:\n",
    "\n",
    "![LDA Intution: Each Document has Multiple Topics](./images/infographic_lda-intuition-blei2012.png)\n",
    "\n",
    "\n",
    "Now, before we dive deeper into the inner workings of LDA, let's take a look at the LDA topic modelling pipeline, lest we lose track of the big picture:\n",
    "\n",
    "![LDA Pipeline](./images/infographic_lda-pipeline-adpated-choig.png)\n",
    "\n",
    "As we can see, we have the collection of $D$ documents (i.e.: trip reports from Erowid), each with some $N_j$ words, which was obtained via processes described in *Section 3: Data Aquisision*. In *Section 4: Data Cleaning and Exploratory Data Analysis (EDA)*, we processed our documents using an array of pre-processing techniques such as tokenization and stopwords removal. In *Section 5: Text Representation and Feature Engineering*, we created vector representations of documents through a variety of methods, including the naive (but fairly effective) term-document counts, along with tf-idf weighting, both of which fall under \"Bag of Words (BOW)\" methods. And now, in this section, we use LDA to create a topic model to capture the topics representd in our collection of trip reports. It should be noted that LDA is not the only topic model available to modern data scientists. Other methods include Non-negative Matrix Fatorization (NMF) and Probabilistic Latent Semantic Analysis (pLSI), which we note here but do not dive deeply into to maintain thesis scope. For those familiar with singular value decomposition (SVD): both NMF, and pLSI relate deeply to SVD of the term-document matrix. Furthermore, \"LDA can also be seen as a type of principal component analysis (PCA) for discrete data\" (Blei 2012).\n",
    "\n",
    "Now, we are ready dive deeper into what LDA does. Let us begin with an infographic for intuition:\n",
    "\n",
    "\n",
    "![LDA Overview](./images/infographic_lda-overview-adapted-choig.png)\n",
    "\n",
    "## 6.2 The Input to LDA\n",
    "\n",
    "LDA takes as input a collection of text documents, which has been preprocessed and vectorized. Note that the format of these text documents isn't set in stone, but most commonly for LDA implementation we have a $D$ x $m$ matrix, where each row represents a document $d$, represented by a $m$-dimensional document vector embedding. We discussed many document vector embeddings in Section 5. For ease, we can use the tf-idf weighted term-document counts.\n",
    "\n",
    "For concreteness, below is the vectorized representation of the first 6 documents in our corpus:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlBr4l9BCsz9",
    "outputId": "fac3ba4e-bd61-44ff-ed6c-4c684b6ad106"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>00000000</th>\n",
       "      <th>00000000000000000000000000003</th>\n",
       "      <th>00005</th>\n",
       "      <th>0001</th>\n",
       "      <th>0001g</th>\n",
       "      <th>0002</th>\n",
       "      <th>...</th>\n",
       "      <th>zzzs</th>\n",
       "      <th>zzzzang</th>\n",
       "      <th>zzzzz</th>\n",
       "      <th>zzzzzz</th>\n",
       "      <th>zzzzzzzap</th>\n",
       "      <th>zzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzuuuuurrrrrrzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 107108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  0000  00000  00000000  00000000000000000000000000003  00005  \\\n",
       "0  0.0  0.0   0.0    0.0       0.0                            0.0    0.0   \n",
       "1  0.0  0.0   0.0    0.0       0.0                            0.0    0.0   \n",
       "2  0.0  0.0   0.0    0.0       0.0                            0.0    0.0   \n",
       "3  0.0  0.0   0.0    0.0       0.0                            0.0    0.0   \n",
       "4  0.0  0.0   0.0    0.0       0.0                            0.0    0.0   \n",
       "\n",
       "   0001  0001g  0002  ...  zzzs  zzzzang  zzzzz  zzzzzz  zzzzzzzap  zzzzzzzzz  \\\n",
       "0   0.0    0.0   0.0  ...   0.0      0.0    0.0     0.0        0.0        0.0   \n",
       "1   0.0    0.0   0.0  ...   0.0      0.0    0.0     0.0        0.0        0.0   \n",
       "2   0.0    0.0   0.0  ...   0.0      0.0    0.0     0.0        0.0        0.0   \n",
       "3   0.0    0.0   0.0  ...   0.0      0.0    0.0     0.0        0.0        0.0   \n",
       "4   0.0    0.0   0.0  ...   0.0      0.0    0.0     0.0        0.0        0.0   \n",
       "\n",
       "   zzzzzzzzzuuuuurrrrrrzzzz  zzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                       0.0                       0.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
       "0                                        0.0   \n",
       "1                                        0.0   \n",
       "2                                        0.0   \n",
       "3                                        0.0   \n",
       "4                                        0.0   \n",
       "\n",
       "   zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \n",
       "0                                                0.0                                                                  \n",
       "1                                                0.0                                                                  \n",
       "2                                                0.0                                                                  \n",
       "3                                                0.0                                                                  \n",
       "4                                                0.0                                                                  \n",
       "\n",
       "[5 rows x 107108 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 rows of our tf-idf weighted document-term matrix\n",
    "# each row represents a document\n",
    "# each column is a term, or \"features\"\n",
    "pd.DataFrame(np.round(dtm_tfidf_nparr, 2), columns=vocab).head()\n",
    "\n",
    "# wierdly enough, this following does not work; has to do with sparse vs. dense representations\n",
    "# pd.DataFrame(np.round(dtm_tfidf, 2), columns=vocab).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5gOiniQCs0G",
    "outputId": "c8647b66-4fd4-49d5-feaf-f5f9b338c0f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19915, 107108)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_tfidf_nparr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pv5Zd2vVCs0I"
   },
   "source": [
    "## 6.3 The Output of LDA\n",
    "\n",
    "\n",
    "Referring to the picture above, LDA gives us three things:\n",
    "- K topics, where each topic is a collection (cluster) of words; note that most of the time K is prespecified by the user. This is where expert knowledge of a particular field can come in very handy\n",
    "- A topic-term matrix (`ttm`), a $K$ x $V$ matrix, which specifies a word distribution for each topic k; in other words, since each topic is a collection of words, it makes sense that some words are more \"representative\" of some topic $k$ than other words. In this way, each topic has a distribution over words.\n",
    "- A document-topic matrix (`dtm`), a $D$ x $K$ matrix, which specifies a topic distribution for each document $d$; in other words, since we can understand each document as having multiple topics, it makes sense that some topics are more saliently present in a document as its \"main topics / main themes\". Since this document topic matrix is a vectorized representation of the doucments, it could also be used as an input to wordcloud, or as a feature for other machine learning algorithms, as briefly mentioned in Section 5.5.\n",
    "\n",
    "\n",
    "## 6.4 The Details of LDA\n",
    "\n",
    "For the reader interested in how LDA works, we present a more technical treatment of LDA here, which is heavily drawn from Blei 2012 and [Christine Choig's Lecture Slides](http://chdoig.github.io/pygotham-topic-modeling/#/)\n",
    "\n",
    "Firstly, we note that the topic structure of our documents is *latent* (i.e.: hidden, unobserved). We *only* observe the documents and words themselves. \n",
    "\n",
    "As such, LDA attempts to infer the hidden topic structure from the observed documents. Formally, LDA tries to compute the posterior distribution, or conditional distribution of the hidden structure given the observed documents. In Blei's words: _\"what is the hidden structure that likely generated the observed collection?\"_\n",
    "\n",
    "![LDA Overview with Details](./images/infographic_lda-details-adapted-choig-blei2012.png)\n",
    "\n",
    "To describe LDA formally, we introduce the following notation and (adpated) explanations from Blei 2012 to maintain consistency with standard notation:\n",
    "- $D$, an fixed integer, representing the number of documents in our corpus\n",
    "- $V$, a fixed integer, representing the vocaulary size, i.e.: the number of unique tokens in our corpus\n",
    "- Topics $\\beta_{1:K}$ (`K x 1` vector), where each `topic` $\\beta_k$ is a (discrete) distribution over the vocabulary. Recall that a `topic` $k$ is a list of words, some of which are more \"relavent\" than others for the given topic $k$; in this way, each topic $\\beta_k$ is a discrete distribution over the vocabulary.\n",
    "- Documents $\\theta_{1:D}$, where document $\\theta_d$ is a (discrete) topic distribution of the $d$-th document. $\\theta_{d, k}$ (`real number` $\\in [0, 1]$) represents the proportion of topic $k$ in the $d$-th document. Recall that if we have $K$ topics, each document is modelled as a mix of these K topics, each topic with some proportion between 0 and 1.\n",
    "- The topic assignments for the $d$-th document are $z_d$(`n x 1 vector`), where $z_{d,n}$ (`integer` $\\in [0, K]$) is the topic assignment for the $n$th word in document $d$; Recall that each document is thought of as a mixture of $K$ topics, each with some proportion between 0 and 1. One way to get this proportion is by conceptualizing each word $n$ in a document $d$ as having a topic $k$, and the proportion of words \"with\" topic $k$ is the proporition of topic $k$ in document $d$\n",
    "- The observed words for document $d$ are $w_d$, where $w_{d,n}$ is the $n$th word in document $d$\n",
    "\n",
    "The full joint probability distribution modelled by LDA is given by:\n",
    "\n",
    "$$ p( \\beta_{1:K}, \\theta_{1, D}, z_{1:D}, w_{1:D}) = \\prod_{i=1}^{K}p(\\beta_i)\\prod_{d=1}^D p(\\theta_d) \\left( \\prod_{n=1}^N p(z_{d, n} | \\theta_d)p(w_{d, n} | \\beta_{1:K}, z_{d, n}) \\right)$$\n",
    "\n",
    "\n",
    "Recall that LDA infers the hidden topic structure by modelling the conditional distribution of the hidden structure given the observed documents. Formally, LDA is modelling:\n",
    "\n",
    "$$p( \\beta_{1:K}, \\theta_{d, k}, z_{1:D} | w_{1:D}) = \\cfrac{p( \\beta_{1:K}, \\theta_{d, k}, z_{1:D}, w_{1:D})}{w_{1:D}}$$\n",
    "\n",
    "The numerator, or joint probability distribution over all the random variables, is straightforward to compute, but the denominator, obtained by margininalizing over all latent topic structures, is exponentially large and intractible to compute (Blei 2012). As such, LDA is approximately in two main ways, through sampling-based algorithms and variational algorithms (Blei 2012). A sampling-based algorithm constructs an approximation of the posterior, and repeatedly draws samples from this approximation so in the limit converges to the true posterior distribution, and is non-deterministic. The most popular sampling-based algorithm is Gibbs Sampling. Variational methods are deterministic algorithms. They pre-suppose that the posterior distribution is from a parametrized family of distributions (e.g.: a bell curve that's centered or off center, narrowerer or flatter, depending on the parameters of mean and variance). This makes variational methods an optimization problem as it tries to find the parameters that define a distribution closest to the posterior distribution. Here, closeness is measured by Kullback Liebler Divergence (KL-Divergence), which takes as input two distributions (order matters) and returns a real valued measure of closeness.\n",
    "\n",
    "Assumptions of LDA:\n",
    "- Bag of words assumption, that ordering of words in the document doesn't matter. We have seen two examples: term-document counts and tf-idf weighting.\n",
    "- Ordering of documents don't matter; this is fairly realistic, unless we are modelling changes in topic over time. This can be an interesting area of further research in a followup study, where we can compare trip reports of the 1960's with those of the 21st century, to examine if and how the topics present in trip reports have evolved. Indeed, one can conceive that the 1960's may be marked by hippie culture, and extending the analysis through the dimension of time may reveal cultural around psychedelic substances. See below for an example from Blei 2012 for a dynamic LDA model examining how the topics in the journal Science has evolved through time.\n",
    "- The number of topics K is assumed to be fixed. To account for chaning number of topics, one can turn to Bayesian non-parametric topic models, which is outside the scope of this thesis.\n",
    "\n",
    "Topic Model Extensions, and Future Research Areas:\n",
    "See Wallach, Griffiths et al. for models that relax the bag of words assumptions use words conditioned on previous words and LDA in conjunction with a hidden markov model (HMM), which lead to improved performance. (todo sources). Other types of topic models include (Blei 2012): correlated topic model and pachinko allocation machine, which account for correlations between topics (e.g.: machine learning and statistics may be more correlated than machine learning and penguins; the spherical topic model allows words to be unlikely in a topic (for example, \"coconuts\" is unlikely in documents about watches); sparse topic models and ‚Äúbursty‚Äù topic models respectively impose more constraints over topic distributions and more realistically model word counts (Blei 2012).\n",
    "\n",
    "For those familiar with plate models, the following graphic (Blei 2012) reminds us that LDA is a type of a broader class of algorithms: probabilistic graphical models. $D$ and $N$ represent multiplicity, i.e.: the referenced cells are repeated $D$ and $N$ times. $\\alpha$ and $\\eta$ are hyperparameters that specify the shape of two Dirichlet Distributions, used to describe the $\\theta_d$ (the topic distribution of document d) distributions and $\\beta_k$ (word distribution of topic k) distributions respectively.\n",
    "\n",
    "![Plate Model of LDA](./images/infographic_lda-plate-model-blei2012.png)\n",
    "\n",
    "\n",
    "## 6.5 Implementation of LDA on psychedelic trip reports\n",
    "\n",
    "In practice, LDA is implemented either from a Gibbs Sampling approach (e.g.: `mallet` package) or via Variational Methods (e.g.: `gensim`), and optimized using Expectation Maximization (EM) algorithms. We note without theoretical justification that Lyle Ungar has found empirically that `mallet` works better and provides more interpretable models (source: conversations). Luckily for us, packages such as `mallet` and `sklearn` are easy to use, and provide state of the art LDA implementations. We first use `sklearn` implementation of LDA, which uses an online variational Bayes algorithm, due to its ease of use and compatibility with LDA visualization packages such as `pyLDAvis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXgJcpXKMFNx",
    "outputId": "ded19574-6a14-49ef-a8e5-2b373eaff3a0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19915, 107108)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USE sklearn: https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb\n",
    "# (TODO) USING GRAPHLAB: Adapted from: https://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb#topic=0&lambda=1&term=\n",
    "\n",
    "# imports for visualizing LDA\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "# setting for jupyter notebooks\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# inports for LDA and preparing data set\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "####\n",
    "# Prepare LDA inputs: vectorization of documents \n",
    "# use the tfidf matrix we had before as the document vector represetation for input into LDA\n",
    "# our lda input is simply dtm_tfidf, the document term matrix with tfidf weighting. We take a look at the shape\n",
    "print(dtm_tfidf.shape)\n",
    "# rename the tf_idf vectorizer\n",
    "\n",
    "#### \n",
    "# Fit Latent Dirichlet Allocation models\n",
    "\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda_tfidf.fit(dtm_tfidf)\n",
    "\n",
    "# for TF DTM\n",
    "# lda_tf = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "# lda_tf.fit(dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-howZj_cCs0V"
   },
   "outputs": [],
   "source": [
    "# rename tfidf vectorizor for better readability; TODO: get it right the first time\n",
    "tfidf_vectorizer = tv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0GBAbo5Cs0X"
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Visualization of Latent Dirichlet Allocation models\n",
    "# using the tf-idf model to visualize TF-IDF\n",
    "pyLDAvis.sklearn.prepare(lda_tfidf, dtm_tfidf, tfidf_vectorizer)\n",
    "\n",
    "# pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DrMipNNBMFN0"
   },
   "source": [
    "## Below this is archived LDA code from towards data science; very solid stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MG8KMDeWMFN1",
    "outputId": "e6ef52f1-feaa-4b2f-c14a-7fa61268b0d5",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-da844e728b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdt_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'T1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'T2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'T3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_topics'"
     ]
    }
   ],
   "source": [
    "# TODO: LDA Example code\n",
    "# Source: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "# TODO: Looks like we can apply LDA on many different matrices, including count vector or tfidf vector\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=3, max_iter=10000, random_state=0)\n",
    "dt_matrix = lda.fit_transform(cv_matrix)\n",
    "features = pd.DataFrame(dt_matrix, columns=['T1', 'T2', 'T3'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yCX6qs3sMFN2"
   },
   "outputs": [],
   "source": [
    "# TODO: View the words for the topics\n",
    "# Source: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "tt_matrix = lda.components_\n",
    "for topic_weights in tt_matrix:\n",
    "    topic = [(token, weight) for token, weight in zip(vocab, topic_weights)]\n",
    "    topic = sorted(topic, key=lambda x: -x[1])\n",
    "    topic = [item for item in topic if item[1] > 0.6]\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SJsvw_WyMFN4",
    "outputId": "3dbfde3f-44ec-4648-ce78-1c5441e128ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.640s.\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 0.435s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.431s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.407s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
      "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
      "Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
      "Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 1.828s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: just people don like did know make really right think say things time look way didn ve course probably good\n",
      "Topic #1: help thanks windows know hi need using does looking anybody appreciated card mail software use info email ftp available pc\n",
      "Topic #2: does god believe know mean true christians read point jesus christian church come people fact says religion say agree bible\n",
      "Topic #3: know thanks mail interested like new just bike email edu advance want contact really list heard com post hear information\n",
      "Topic #4: 10 new 30 12 20 50 11 sale 16 15 time 14 old power ago good 100 great offer cost\n",
      "Topic #5: number 1993 data subject government new numbers provide information space following com research include large note group major time talk\n",
      "Topic #6: edu problem file com remember try soon article mike files code program sun free send think cases manager little called\n",
      "Topic #7: game year team games world fact second case won said win division play best clearly claim allow example used doesn\n",
      "Topic #8: think don drive hard need bit mac make sure read apple going comes disk computer case pretty drives software ve\n",
      "Topic #9: good just use like doesn got way don ll going does chip better doing bad key want sure bit car\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 6.229s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
      "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
      "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
      "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
      "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
      "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
      "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
      "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
      "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
      "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Example code from https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QACWDxm0MFN5"
   },
   "source": [
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZKOPZXyMFN6"
   },
   "source": [
    "# 7. Word Clouds: Getting the Gist\n",
    "\n",
    "Word Clouds are an intuitively visual way to present vast amounts of textual information, revealing the key themes conveyed in a collection of documents. Prima facie, generating word clouds feel like a simple problem, yet very quickly deeper considerations arise, including (1) using appropriate preprocessing (stop words, stemming, lemmatization), (2) whether to include documents of class $j \\neq i$ when constructing a word cloud for documents of class, and (3) how to scale tokens, given (1) and (2). In this section, we explore combinations of different approaches, and conclude that the most suitable approach depends on our philosophy in constructing these word clouds.\n",
    "\n",
    "\n",
    "## 7.1 Baseline Wordclouds\n",
    "\n",
    "`TODO`\n",
    "\n",
    "For our baselines word clouds for documents of class i, we perform basic stopwords removal, with no stemming and lemmatization. We use: \n",
    "- Token raw frequency (not considering other classes $\\neq i$)\n",
    "- TF-IDF weighting (considering other classes $j \\neq i$)\n",
    "\n",
    "Note that since each trip report may contain multiple classes, we will include a document that contains k classes in k word clouds.\n",
    "\n",
    "## 7.2 Intermediate Wordclouds\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 7.3 Log Odds Ratio, an homage to Statistical Inference\n",
    "\n",
    "`TODO`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66IMsS17MFN6"
   },
   "source": [
    "# Visualizing Erowid Corpus with Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2iF_EoiMFN6"
   },
   "source": [
    "### 1.1.1 | Retrieve the reports for different substances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zLlvnzc5MFN7",
    "outputId": "82aa52bc-5b5b-4c37-9ff0-a01eed06d114"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using report, title, substance, substance.unique_label as id variables\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 6 x 2\u001b[39m\n",
      "  title                           `n()`\n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                           \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m ::: The City of Light :::          21\n",
      "\u001b[90m2\u001b[39m ... Meh                            21\n",
      "\u001b[90m3\u001b[39m ...But I Did Everything Right!     21\n",
      "\u001b[90m4\u001b[39m ...Until the Crystal Cracked!      21\n",
      "\u001b[90m5\u001b[39m ...When I'm Closing in on Death    21\n",
      "\u001b[90m6\u001b[39m .3 PPM of Our Air                  21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0d07c5064cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'R'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-o tripReports -o tripReports.long'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'# ^ export tripReports and tripReports.long for use in python environment\\n# https://stackoverflow.com/questions/55841165/share-variables-between-r-and-python-in-jupyternotebook\\n\\n### create long data format, using report, title, substance, substance.unique_label as id variables\\n# glimpse(tripReports)\\ntripReports.long <- reshape2::melt(tripReports)\\n\\n### validate the long data format\\ntripReports.long %>% select(-report) %>% group_by(title) %>% summarize(n()) %>% head()\\n# glimpse(tripReports.long)\\n# glimpse(tripReports)\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</Users/alextzhao/opt/anaconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-130>\u001b[0m in \u001b[0;36mR\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/ipython/rmagic.py\u001b[0m in \u001b[0;36mR\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mlocalconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m                     \u001b[0moutput_ipy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobalenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_ipy\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/environments.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, item, wantfun)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \"\"\"\n\u001b[1;32m     56\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwantfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;31m# TODO: There is a design issue here. The attribute __rname__ is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# intended to store the symbol name of the R object but this is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/pandas2ri.py\u001b[0m in \u001b[0;36mrpy2py_listvector\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrpy2py_listvector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'data.frame'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy2ri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/pandas2ri.py\u001b[0m in \u001b[0;36mrpy2py_dataframe\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrpy2py_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     items = OrderedDict((k, rpy2py(v) if isinstance(v, Sexp) else v)\n\u001b[0;32m--> 219\u001b[0;31m                         for k, v in obj.items())\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPandasDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrownames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/pandas2ri.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrpy2py_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     items = OrderedDict((k, rpy2py(v) if isinstance(v, Sexp) else v)\n\u001b[0m\u001b[1;32m    219\u001b[0m                         for k, v in obj.items())\n\u001b[1;32m    220\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPandasDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/vectors.py\u001b[0m in \u001b[0;36mitems\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mit_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mit_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/vectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/pandas2ri.py\u001b[0m in \u001b[0;36mri2py_vector\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSexpVector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mri2py_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy2ri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/numpy2ri.py\u001b[0m in \u001b[0;36mrpy2py_sexp\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrpy2py_sexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_vectortypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mRTYPES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVECSXP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%R -o tripReports -o tripReports.long\n",
    "# ^ export tripReports and tripReports.long for use in python environment\n",
    "# https://stackoverflow.com/questions/55841165/share-variables-between-r-and-python-in-jupyternotebook\n",
    "\n",
    "### create long data format, using report, title, substance, substance.unique_label as id variables\n",
    "# glimpse(tripReports)\n",
    "tripReports.long <- reshape2::melt(tripReports)\n",
    "\n",
    "### validate the long data format\n",
    "tripReports.long %>% select(-report) %>% group_by(title) %>% summarize(n()) %>% head()\n",
    "# glimpse(tripReports.long)\n",
    "# glimpse(tripReports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGZRTOHkMFN9",
    "outputId": "0180b3ec-ae81-46c9-e94a-45c51bc8e15f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alextzhao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Use the wordcloud package to generate baseline wordclouds\n",
    "# https://github.com/amueller/word_cloud\n",
    "# https://www.datacamp.com/community/tutorials/wordcloud-python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from wordcloud import STOPWORDS as wordcloud_STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # utilities to interoperate between R and pandas data frames\n",
    "# # see https://rpy2.github.io/doc/latest/html/pandas.html\n",
    "# import rpy2.robjects as ro\n",
    "# from rpy2.robjects.packages import importr\n",
    "# from rpy2.robjects import pandas2ri\n",
    "# from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "# # convert the R data frame to a pandas dataframe\n",
    "# with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "#   pd_tripReports = ro.conversion.rpy2py(tripReports)\n",
    "\n",
    "# NOTE: We can just read directly from the csv file that we saved earlier\n",
    "pd_tripReports = pd.read_csv(\"tripReportsEncoded.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4vrUNeOiMFOC"
   },
   "outputs": [],
   "source": [
    "## Prepare a few substances lists for use\n",
    "\n",
    "# a list of all the substances we want word clouds for\n",
    "substances_all_list = pd_tripReports.loc[:, 'substance_mushrooms':'substance_syrian_rue'].columns.tolist()\n",
    "# [ 'mushrooms', 'lsd', 'cannabis', ... etc] for use as stopwords\n",
    "substances_all_plainstring_list = [substance.replace(\"substance_\", \"\") for substance in substances_all_list]\n",
    "\n",
    "# TODO: Prepare regexes to match with all the observed spellings of different substances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oxjG3Z0MFOD"
   },
   "outputs": [],
   "source": [
    "#### Get the text for interested substances\n",
    "# a dictionary mapping {substance_<name> : text}\n",
    "reports_text = {}\n",
    "# Concatenate with \" \" by default, TODO: Can specify separator as such: str.cat(sep=\"...\")\n",
    "for substance in substances_all_list:\n",
    "    reports_text[substance] = pd_tripReports.query(\"{} == 1\".format(substance))[\"report\"].str.cat().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8rtS2tNkMFOG",
    "outputId": "9d018895-2ef7-4aa7-b051-326bc805069f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an upper',\n",
       " 'base',\n",
       " 'bath salt',\n",
       " 'dream herbal incense)',\n",
       " 'ecstasy (unknown',\n",
       " 'indian leaf',\n",
       " 'london underground dream)',\n",
       " 'magic silver',\n",
       " 'sensory deprivation',\n",
       " 'white rock opium',\n",
       " '(15x extract)',\n",
       " '(2c-t-7',\n",
       " '(amt + mdma)',\n",
       " '(amt + methamphetamine)',\n",
       " '(hydrocodone',\n",
       " '(methyl',\n",
       " '(quetiapine) seroquel',\n",
       " '1',\n",
       " '1 soma',\n",
       " '1-4 butanediol',\n",
       " '10x',\n",
       " '10x extract)',\n",
       " '10x extracts)',\n",
       " '15x extracts)',\n",
       " '19-norandrostenedione (nor-19)',\n",
       " '2-aminoindan',\n",
       " '2-aminoindan (im)',\n",
       " '2-c-t-2',\n",
       " '2-cb',\n",
       " '2-cd']"
      ]
     },
     "execution_count": 102,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTIONAL\n",
    "# Lammatization\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "36Aije6-MFOI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFwfZxVAMFOK"
   },
   "source": [
    "### 1.1.3 | Generating Baseline Wordclouds\n",
    "\n",
    "#### 1.1.3.1 | Wordclouds Based on Word Frequency\n",
    "\n",
    "Note: Using the `wordcloud` package, we are making numerous assumptions, including:\n",
    "- Word scaling based on `frequency` only\n",
    "-  of words: the `wordcloud` package has a `relative_scaling` attribute, which specifies the importance of relative word frequencies for font-size. If `relative_scaling = 0`, the only word-ranks are considered; `relative_scaling=1` means words size is directly proportional to their frequency; Default of `relative_scaling = 0.5` considers both word frequency and word rank, and is used in our analysis. Insight retrieved from `wordcloud` [documentation](http://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud) and from [this tutorial](https://www.datacamp.com/community/tutorials/wordcloud-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGsJ7FznMFOK",
    "outputId": "e606788e-b060-4815-e0c3-756f278881a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now plotting wordclouds for 5 substances...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6c346c932534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# creating subplots: https://stackoverflow.com/questions/25239933/how-to-add-title-to-subplots-in-matplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# changing figure size: https://stackoverflow.com/questions/332289/how-do-you-change-the-size-of-figures-drawn-with-matplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Word Clouds based on word Frequencies\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#### Create Wordclouds\n",
    "# a list of the \"classical psychedelics\"\n",
    "substances_psychedelics_list = [\"substance_mushrooms\", \"substance_lsd\", \"substance_mescaline\", \"substance_5_meo_dmt\", \"substance_dmt\"]\n",
    "\n",
    "\n",
    "# NOTE: change this line of code to change which substances to plot\n",
    "substances_to_plot = substances_psychedelics_list\n",
    "num_plots = len(substances_to_plot)\n",
    "print(\"Now plotting wordclouds for {} substances...\".format(num_plots))\n",
    "\n",
    "# creating subplots: https://stackoverflow.com/questions/25239933/how-to-add-title-to-subplots-in-matplotlib\n",
    "# changing figure size: https://stackoverflow.com/questions/332289/how-do-you-change-the-size-of-figures-drawn-with-matplotlib\n",
    "fig = plt.figure(figsize=(50, 10))\n",
    "fig.suptitle(\"Word Clouds based on word Frequencies\", fontsize=16)\n",
    "\n",
    "for index, substance in enumerate(substances_to_plot):\n",
    "    wordcloud = WordCloud(stopwords=stopword_list, max_font_size=50, max_words=100, background_color=\"white\").generate(reports_text_tokenized[substance])\n",
    "    ax = plt.subplot(num_plots, 1, index+1) # note here for subplots index must be [1, num_plots]\n",
    "    ax.set_title(\"{} Wordcloud\".format(substance))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.savefig('wordclouds_naive_psychedelics.png')\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "### Plotting a single wordcloud\n",
    "# wordcloud = WordCloud(stopwords=stopwords_all, max_font_size=50, max_words=100, background_color=\"white\").generate(reports_text_tokenized[\"substance_mushrooms\"])\n",
    "# plt.figure()\n",
    "# plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTOARHJbMFON"
   },
   "source": [
    "#### 1.1.3.2 Wordclouds Based on `tf-idf`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQksxl5nMFOO"
   },
   "outputs": [],
   "source": [
    "# TODO: Replace numbers from corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8kpVxw5uMFOP"
   },
   "source": [
    "##### 1.1.3.2.1 `tfidf wordclouds` >>> Using only 19 different documents\n",
    "\n",
    "We first group all the reports for a particular substance into one document, for a total of 19 documents or `samples`. Under this schema, each substance has precisely one document, which is the concatenation of all the reports corresponding to that substance. Note that there are concerns with doing this, as `idf` typically assumes a large number of documents in the corpora; we will explore an alternative approach later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3XfgAtIMFOP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## See TFidfVectorizer docs: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.csr import csr_matrix # need this if we want to save tfidf_matrix; see https://stackoverflow.com/questions/34449127/sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "\n",
    "\n",
    "# NOTE: reports_text_tokenized stores the tokenized corpus\n",
    "# List of substances under consideration, sorted to preserve alphabetical ordering\n",
    "substances_list_sorted = sorted(list(reports_text_tokenized.keys()))\n",
    "substances_list_sorted\n",
    "\n",
    "# TfidfVectorizer expects a list of strings, so we prepare the corpus in the appropriate format\n",
    "corpus_psychedelics_sorted = []\n",
    "for substance in substances_list_sorted:\n",
    "    corpus_psychedelics_sorted.append(reports_text_tokenized[substance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F102xSaeMFOR"
   },
   "source": [
    "Notes on the specification of TfidfVectorizer; additional information found in [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "- `strip_accents=ascii` helps us strip accented characters such as `√°` to `a`\n",
    "- `lowercase=True`: automatically converts all the text to lowercase\n",
    "- `analyzer=word`: considers words as the basic unigram unit; we can also do a character level model with `char`\n",
    "- `stop_words=english`: uses built in stopwords given by `sklearn`; Follow these [instructions](https://awhan.wordpress.com/2016/06/05/scikit-learn-nlp-list-english-stopwords/) to view all the stopwords for `sklearn` and `nltk`\n",
    "- `ngram_range=(1, 1)`: setting to (1, 1) gives us unigrams. For bigrams and trigrams, we use (1, 2) and (1, 3) respectively\n",
    "- `max_df=1.0`: using the float `1.0` sets the maximum document frequency to 100\\% of the documents\n",
    "- `min_df=1`: ignores all words with document frequency less than 1\n",
    "- `max_features=None`: Don't put a cap on the number of features\n",
    "- `binary=False`: Use the tf counts, instead of setting all non-zero counts to 1\n",
    "- `dtype=float64`: Default datatype for tf-idf values\n",
    "- `norm=l2`: uses the L2 norm for each row of tf-idf values, so each row sums to 1\n",
    "- `use_idf=True`: use idf weighting (instead of just tf)\n",
    "- `smooth_idf=True`: this is akin to laplace smoothing, and helps us prevent 0 divisions\n",
    "- `sublinear_tf=False`: Do not replace tf with 1 + log(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YJB7M35FMFOR",
    "outputId": "49e27919-80f9-4c57-d212-711466d1aec4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alextzhao/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1616: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. float64 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# create and use a TfidfVectorizer\n",
    "# We specify explicitly the parameters used, even for defaults, in order to be precise with our assumptions\n",
    "# and for reproducibity\n",
    "# note: sklearn by default throws away punctuations; we accept this as we are not performing sentiment analysis\n",
    "# and related tasks where punctuations may be highly informative\n",
    "\n",
    "# https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XeLZRzJKjmE\n",
    "vectorizer_tfidf = TfidfVectorizer(strip_accents='ascii',\n",
    "                                  lowercase=True,\n",
    "                                  analyzer='word',\n",
    "                                  stop_words='english',\n",
    "                                  ngram_range=(1, 1),\n",
    "                                  max_df=1.0,\n",
    "                                  min_df=1,\n",
    "                                  max_features=None,\n",
    "                                  vocabulary=None,\n",
    "                                  binary=False,\n",
    "                                  dtype='float64',\n",
    "                                  norm='l2',\n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  sublinear_tf=False)\n",
    "\n",
    "X_vectors_tfidf = vectorizer_tfidf.fit_transform(corpus_psychedelics_sorted)\n",
    "# vectorizer_tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "TTCjIB42MFOT",
    "outputId": "98e3b199-50f3-47c9-c933-7f4b385b018d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['substance_5_meo_dmt',\n",
       " 'substance_alcohol',\n",
       " 'substance_ayahuasca',\n",
       " 'substance_cannabis',\n",
       " 'substance_dmt',\n",
       " 'substance_ibogaine',\n",
       " 'substance_kava',\n",
       " 'substance_ketamine',\n",
       " 'substance_kratom',\n",
       " 'substance_lsd',\n",
       " 'substance_mdma',\n",
       " 'substance_mescaline',\n",
       " 'substance_methamphetamine',\n",
       " 'substance_morning_glory',\n",
       " 'substance_mushrooms',\n",
       " 'substance_nitrous_oxide',\n",
       " 'substance_pcp',\n",
       " 'substance_salvia',\n",
       " 'substance_syrian_rue']"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look again at the sorted ordering of the substances\n",
    "substances_list_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMj4kXK5MFOV"
   },
   "outputs": [],
   "source": [
    "# helper functions to extract keywords from documents\n",
    "# from http://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.XeLcAzJKjmE\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9wcJFmyMFOX",
    "outputId": "e9f58737-a328-4562-9bb7-503419fde361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "\n",
      "===Keywords===\n",
      "seeds 0.381\n",
      "time 0.286\n",
      "experience 0.282\n",
      "felt 0.284\n"
     ]
    }
   ],
   "source": [
    "# Tutorial on how to extract key words based on tf-idf: http://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.XeLcAzJKjmE\n",
    "\n",
    "# Using the tf-idf frequencies\n",
    "feature_names_tfidf = vectorizer_tfidf.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sorted_items=sort_coo(X_vectors_tfidf.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names_tfidf,sorted_items,10)\n",
    " \n",
    "# now print the results\n",
    "print(\"\\n=====Doc=====\")\n",
    "# print(doc[:1000])\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])\n",
    "\n",
    "    \n",
    "# # get the first vector out (for the first document)\n",
    "# first_vector_tfidfvectorizer= X_vectors_tfidf[0]\n",
    " \n",
    "# # place tf-idf values in a pandas data frame\n",
    "# df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=feature_names_tfidf, columns=[\"tfidf\"])\n",
    "# df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7cnmnuKMFOY"
   },
   "source": [
    "##### Sidebar: Some Interesting Observations from `Tfidfvectorizer` output\n",
    "It's interesting to note that looking at the Tfidfvectorizer feature_names, we see many numbers. These numbers either stand alone or are followed by units and quantifiers.  ,  \n",
    "\n",
    "These units seem to be used to specify multiple types of quantities, such as:\n",
    "- weight (that of substance ingested or user): \n",
    "    - 'c', 'g', 'gm', 'gram', 'grams', 'lbs', 'mcg', 'mg', 'microgram', 'mics', 'o', 'oz', 'pounds', 'ug'\n",
    "- volume or length\n",
    "    - 'cc', 'cm', 'cup', 'ft', 'i', 'inch', 'iu', 'kg', 'kgs', 'meters', 'metres', 'mile', 'miles', 'ml', 'sq', 'tbs', 'tbsp', 'yards'\n",
    "- time or time period\n",
    "    - \"'s' (e.g.: in 'the 1990s')\", 'XXhXXm', 'am', 'hours', 'hr', 'min', 'minute', 'pm', 'sec'\n",
    "- user age\n",
    "    - 'day', 'y', 'yo', 'yr'\n",
    "- scales and dimensions\n",
    "    - \"'5x' (e.g.: 5x salvia extract)\", '1x15', '1x50mg'\n",
    "- addresses\n",
    "    - \"st\"\n",
    "- speed\n",
    "    - \"mph\"\n",
    "- numerical quantification\n",
    "    - \"'strips' (e.g.: number of lsd strips)\", \"'ths' (as in 100ths times)\", 'nd', 'th'\n",
    "- sound information\n",
    "    - 'bpm', 'bps', 'hz'\n",
    "\n",
    "A closer examination of this numberical data may yield very interesting information about user demographics, set and setting, and dosage. All of this may be extremely valuable since dosage, set and setting, are among the most important factors in determining one's subjective experience when consuming a drug, and demographics may be correlated with the valence of a particular experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZtlSYrTMFOY"
   },
   "outputs": [],
   "source": [
    "# TODO: Extension - extracting bigrams and trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgnFTQp7MFOa"
   },
   "source": [
    "##### 1.1.3.2.2 TODO: `tfidf wordclouds` >>> using all documents separately with multilabel\n",
    "\n",
    "TODO: How do we handle multilabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3CtZ1_FMFOb"
   },
   "source": [
    "### 1.1.4: (For Fun) Creating custom word cloud masks\n",
    "\n",
    "To make the wordclouds more interesting, we can add custom image masks to the wordclouds, as inspired by [this article](https://towardsdatascience.com/creating-word-clouds-with-python-f2077c8de5cc), [this tutorial](https://www.datacamp.com/community/tutorials/wordcloud-python), and `wordcloud` package [documentation](http://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2bX4sIatMFOb",
    "outputId": "ac15eb7f-c79d-4929-f7ac-69155d7fc298"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot wordcloud with image mask\n",
    "# Make sure white parts are 255 instead of 0, as per: https://www.datacamp.com/community/tutorials/wordcloud-python\n",
    "# TODO: Why is the wordcloud blurry, and why can I not get the mask to work appropriately?\n",
    "def transform_format(val):\n",
    "    if val == 0: return 255\n",
    "    else: return val\n",
    "    \n",
    "mask_mushroom = np.array(Image.open(\"images/util_mushroom_vector_mask.png\"))\n",
    "# vectorization of transform_format function above; invert the mask to be in the right format\n",
    "mask_mushroom[mask_mushroom > 240] = 30\n",
    "mask_mushroom[mask_mushroom == 0] = 255\n",
    "mask_mushroom\n",
    "\n",
    "#plot a mushroom wordmap about mushrooms\n",
    "#increase \"resolution\": https://stackoverflow.com/questions/28786534/increase-resolution-with-word-cloud-and-remove-empty-border\n",
    "\n",
    "# wordcloud_mushroom_masked = WordCloud(width=1000, height=10000, stopwords=stopword_list, mask=mask_mushroom, max_font_size=50, max_words=100, background_color=\"black\").generate(reports_text_tokenized[\"substance_mushrooms\"])\n",
    "wordcloud_mushroom_masked = WordCloud(max_font_size=50, max_words=100, mask=mask_mushroom, stopwords=stopword_list, background_color=\"white\").generate(reports_text_tokenized[\"substance_mushrooms\"])\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(wordcloud_mushroom_masked, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.savefig('wordcloud_naive_mushroom_masked')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qdExB2oMFOd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24LzCMPJMFOf"
   },
   "source": [
    "## 1.2: Wordclouds with Log Odds Ratio\n",
    "\n",
    "#### TODO\n",
    "\n",
    "This approach uses Informative Prior Log Odds IPLO ratio method, which is used in natural language processing to identify the differences in language usage patters between two groups. The foundations rests upon log odds ratio, a widely used technique in statistical inference and hypothesis testing. Intuitively, we ask, how much more likely is a word to belong to a particular document, as opposed to in another document. In a very deep way, log odds ratio relates to the F statistic in statistical inference, and is used widely for model selection.( // TODO: Cite ISLR / ESM)\n",
    "\n",
    "The theoretical framework is outlined in more detail [here](TODO) // TODO cite CCB's log odds paper\n",
    "\n",
    "In the case of multilabelled psychedelic trip reports, we can take a one-vs-all approach, i.e.: compare documents with the label `substance.mushrooms` and documents without the label `substance.mushrooms`. This will discover words / tokens that are particularly informative of the `substance.mushrooms` class. Note again that reports with a particular `substance.*` label are not necessarily uniquely `substance.*`, and may contain other labels since people have a tendency to consume multiple different types of drugs simultaneously.\n",
    "\n",
    "Code adapted from [john-hewitt](https://github.com/john-hewitt/iplo-analysis) and Chris Callison Burch's research group (reproduction with explicit permission from Callison-Burch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ut4lV1UTMFOf"
   },
   "outputs": [],
   "source": [
    "# TODO: Logs Odds Ratio Code Adapted from Chris Callison Burch's Lab!!!\n",
    "# TODO: Implement for psychedelic trip reprots\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ze1-nKuMFOh"
   },
   "source": [
    "## 1.3: Wordclouds with other methods\n",
    "\n",
    "### TODO: \n",
    "\n",
    "- Logistic regression weights\n",
    "- Weights from other classification methods\n",
    "- LDA document embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lU4K7L6CMFOh"
   },
   "source": [
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMR38C23MFOh"
   },
   "source": [
    "# 8. Visualizing High Dimensions: Clustering and Principle Component Analysis (PCA)\n",
    "\n",
    "`TODO`\n",
    "\n",
    "- TODO: Agglomerative Hiearchical Clustering\n",
    "- TODO: KNN (?) - Probably more of a classification algorithm\n",
    "- TODO: Search up state of the art document clustering techniques\n",
    "- TODO: K Means clustering; \"There are multiple ways to select the optimal value of k like using the Sum of Squared Errors metric, Silhouette Coefficients and the Elbow method\"\n",
    "- TODO: [Affinity Propagation](https://en.wikipedia.org/wiki/Affinity_propagation)\n",
    "- TODO: See [advanced techniques](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa) for using dense document embeddings\n",
    "- TODO: Clustering can be based on many diferent document level features:\n",
    "    - TF\n",
    "    - TF IDF\n",
    "    - LDA Term Document Matrix\n",
    "    - Dense Embeddings (average of all the word embeddings, from something like Word2Vec); finetune existing dense word embedding with something like ULMfit\n",
    "- Visualize Clusters with PCA: See [advanced tecniques](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n",
    "\n",
    "## 8.1 High Dimensionality: Everpresent and Difficult to Grok\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 8.2 Clustering\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 8.3 Principle Component Analysis\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 8.4 Other Methods\n",
    "\n",
    "`TODO`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "W_2iL30LMFOk",
    "outputId": "9a6be0ad-fd32-4401-e43e-b90dfb0dd3cd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarity_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-0cece96fdad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhierarchy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdendrogram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m pd.DataFrame(Z, columns=['Document\\Cluster 1', 'Document\\Cluster 2', \n\u001b[1;32m      8\u001b[0m                          'Distance', 'Cluster Size'], dtype='object')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'similarity_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Example of Agglomerative Hiearchical Clustering \n",
    "# Source: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(similarity_matrix, 'ward')\n",
    "pd.DataFrame(Z, columns=['Document\\Cluster 1', 'Document\\Cluster 2', \n",
    "                         'Distance', 'Cluster Size'], dtype='object')\n",
    "\n",
    "\n",
    "\n",
    "###### Visualizing the clustering process\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Data point')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(Z)\n",
    "plt.axhline(y=1.0, c='k', ls='--', lw=0.5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "u26zH32JMFOm",
    "outputId": "e1ea09e3-a37e-4b5b-ac0a-43f3ad8492cc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-37f3dcb1584c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcluster_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfcluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mcluster_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ClusterLabel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Obtain cluster levels\n",
    "# Source: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "# TODO: Compare the automatic clusters with the substance multilables\n",
    "# TODO: Can use a \"vector\" of the labels as a training label?\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "max_dist = 1.0\n",
    "\n",
    "cluster_labels = fcluster(Z, max_dist, criterion='distance')\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2I2pVgYZMFOn"
   },
   "source": [
    "# Clustering based on LDA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XI_pD3lsMFOo"
   },
   "outputs": [],
   "source": [
    "# TODO: Example code for clustering with LDA embeddings\n",
    "# Source: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3, random_state=0)\n",
    "km.fit_transform(features)\n",
    "cluster_labels = km.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGaF4ZfOMFOp"
   },
   "source": [
    "#### Other ways of Visualizing Trip reports\n",
    "\n",
    "- Clustering with ordinary `K-means` and `minibatch K-means`; see [sklearn example](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)\n",
    "- `Principle Components Analysis (PCA)` of trip reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ThzsQlmPMFOp",
    "outputId": "468f1005-2583-4574-bd0c-8d43fa0be772"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "2019-11-30 00:37:12,265 INFO Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
      "2019-11-30 00:37:12,270 INFO Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
      "  --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
      "  --no-idf              Disable Inverse Document Frequency feature weighting.\n",
      "  --use-hashing         Use a hashing feature vectorizer\n",
      "  --n-features=N_FEATURES\n",
      "                        Maximum number of features (dimensions) to extract\n",
      "                        from text.\n",
      "  --verbose             Print progress reports inside k-means algorithm.\n",
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "3387 documents\n",
      "4 categories\n",
      "\n",
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 1.086469s\n",
      "n_samples: 3387, n_features: 10000\n",
      "\n",
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, compute_labels=True, init='k-means++',\n",
      "                init_size=1000, max_iter=100, max_no_improvement=10,\n",
      "                n_clusters=4, n_init=1, random_state=None,\n",
      "                reassignment_ratio=0.01, tol=0.0, verbose=False)\n",
      "done in 0.154s\n",
      "\n",
      "Homogeneity: 0.539\n",
      "Completeness: 0.572\n",
      "V-measure: 0.555\n",
      "Adjusted Rand-Index: 0.576\n",
      "Silhouette Coefficient: 0.007\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: god people jesus say don believe christian bible com religion\n",
      "Cluster 1: graphics university image thanks ac files uk file com 3d\n",
      "Cluster 2: space com nasa access henry digex gov article pat toronto\n",
      "Cluster 3: sandvik keith sgi com livesey kent caltech apple newton solntze\n"
     ]
    }
   ],
   "source": [
    "# TODO: Example code from: https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py\n",
    "\n",
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", type=\"int\",\n",
    "              help=\"Preprocess documents with latent semantic analysis.\")\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", dest=\"minibatch\", default=True,\n",
    "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", default=False,\n",
    "              help=\"Use a hashing feature vectorizer\")\n",
    "op.add_option(\"--n-features\", type=int, default=10000,\n",
    "              help=\"Maximum number of features (dimensions)\"\n",
    "                   \" to extract from text.\")\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", dest=\"verbose\", default=False,\n",
    "              help=\"Print progress reports inside k-means algorithm.\")\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "# categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "print()\n",
    "\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(\"Extracting features from the training dataset \"\n",
    "      \"using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    if opts.use_idf:\n",
    "        # Perform an IDF normalization on the output of HashingVectorizer\n",
    "        hasher = HashingVectorizer(n_features=opts.n_features,\n",
    "                                   stop_words='english', alternate_sign=False,\n",
    "                                   norm=None, binary=False)\n",
    "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
    "                                       stop_words='english',\n",
    "                                       alternate_sign=False, norm='l2',\n",
    "                                       binary=False)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=opts.use_idf)\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "if opts.n_components:\n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "    t0 = time()\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(opts.n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "if opts.minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=opts.verbose)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "if not opts.use_hashing:\n",
    "    print(\"Top terms per cluster:\")\n",
    "\n",
    "    if opts.n_components:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pnubpyNMFOt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMcanwNSMFOv"
   },
   "source": [
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScKmmYeRMFOv"
   },
   "source": [
    "# 9: Predicting Substance Labels from Trip Reports\n",
    "\n",
    "`TODO`\n",
    "\n",
    "- Baseline: multinomial Bayes and other techniques; see sklearn [classifying text documents](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py)\n",
    "- Baseline: Logistic Regression:\n",
    "    - Coefficients returned by logistic regression can be used as weights to scale words in worclouds\n",
    "    - TODO: How do we handle the multilabel case?\n",
    "    - Appropriate regularization like LASSO / Ridge (Elastic Net) for feature selection\n",
    "- Baseline: Naive Bayes classifier\n",
    "- BERT fine-tuning\n",
    "    - [Huggingface](https://github.com/huggingface/transformers) + transformers - See Chris Callison-Burch‚Äôs recommendations\n",
    "- Handling `multilabel learning`, see [sklearn multiclass and multilabel learning](https://scikit-learn.org/stable/modules/multiclass.html#multiclass-and-multilabel-algorithms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_F-ygOdMFOw"
   },
   "source": [
    "## 9.1 The Classification Problem: Our Goals\n",
    "`TODO`\n",
    "\n",
    "## 9.2 Multilabel Considerations\n",
    "`TODO`\n",
    "\n",
    "\n",
    "Using [multilabel classification example](https://scikit-learn.org/stable/auto_examples/plot_multilabel.html#multilabel-classification) from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6urnRKHwMFOw"
   },
   "outputs": [],
   "source": [
    "## TODO: Example code from sklearn, see: https://scikit-learn.org/stable/auto_examples/plot_multilabel.html#multilabel-classification\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "\n",
    "def plot_hyperplane(clf, min_x, max_x, linestyle, label):\n",
    "    # get the separating hyperplane\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(min_x - 5, max_x + 5)  # make sure the line is long enough\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "    plt.plot(xx, yy, linestyle, label=label)\n",
    "\n",
    "\n",
    "def plot_subfigure(X, Y, subplot, title, transform):\n",
    "    if transform == \"pca\":\n",
    "        X = PCA(n_components=2).fit_transform(X)\n",
    "    elif transform == \"cca\":\n",
    "        X = CCA(n_components=2).fit(X, Y).transform(X)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "\n",
    "    classif = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "    classif.fit(X, Y)\n",
    "\n",
    "    plt.subplot(2, 2, subplot)\n",
    "    plt.title(title)\n",
    "\n",
    "    zero_class = np.where(Y[:, 0])\n",
    "    one_class = np.where(Y[:, 1])\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=40, c='gray', edgecolors=(0, 0, 0))\n",
    "    plt.scatter(X[zero_class, 0], X[zero_class, 1], s=160, edgecolors='b',\n",
    "                facecolors='none', linewidths=2, label='Class 1')\n",
    "    plt.scatter(X[one_class, 0], X[one_class, 1], s=80, edgecolors='orange',\n",
    "                facecolors='none', linewidths=2, label='Class 2')\n",
    "\n",
    "    plot_hyperplane(classif.estimators_[0], min_x, max_x, 'k--',\n",
    "                    'Boundary\\nfor class 1')\n",
    "    plot_hyperplane(classif.estimators_[1], min_x, max_x, 'k-.',\n",
    "                    'Boundary\\nfor class 2')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.xlim(min_x - .5 * max_x, max_x + .5 * max_x)\n",
    "    plt.ylim(min_y - .5 * max_y, max_y + .5 * max_y)\n",
    "    if subplot == 2:\n",
    "        plt.xlabel('First principal component')\n",
    "        plt.ylabel('Second principal component')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "X, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n",
    "                                      allow_unlabeled=True,\n",
    "                                      random_state=1)\n",
    "\n",
    "plot_subfigure(X, Y, 1, \"With unlabeled samples + CCA\", \"cca\")\n",
    "plot_subfigure(X, Y, 2, \"With unlabeled samples + PCA\", \"pca\")\n",
    "\n",
    "X, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n",
    "                                      allow_unlabeled=False,\n",
    "                                      random_state=1)\n",
    "\n",
    "plot_subfigure(X, Y, 3, \"Without unlabeled samples + CCA\", \"cca\")\n",
    "plot_subfigure(X, Y, 4, \"Without unlabeled samples + PCA\", \"pca\")\n",
    "\n",
    "plt.subplots_adjust(.04, .02, .97, .94, .09, .2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-KqeOVrpMFOx"
   },
   "source": [
    "## 9.3 Initial Attempts\n",
    "`TODO`\n",
    "\n",
    "## 9.4 The Power of Transfer Learning: State of the Art\n",
    "`TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TRIch4iTMFOx"
   },
   "source": [
    "## 9.5 NLP in Practice: Using automatic piplines for text feature extraction and classification\n",
    "See sklearn [pipline example](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py)\n",
    "\n",
    "TODO: Modify to suit multi-label learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WgjRVptMFOy"
   },
   "outputs": [],
   "source": [
    "# TODO: CODE FROM https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py\n",
    "\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "# License: BSD 3 clause\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "\n",
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(tol=1e-3)),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (20,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                               n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V4usz0cRMFO1"
   },
   "source": [
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iidWGl9WMFO2"
   },
   "source": [
    "# 10. Generation of Trip Reports\n",
    "`TODO`\n",
    "\n",
    "- Hidden Markov Model baseline\n",
    "- LSTM baselines\n",
    "    - Character Model\n",
    "    - Word model\n",
    "- Finetuning BERT to generate trip reports\n",
    "\n",
    "\n",
    "## 10.1 The Generation Problem: Our Goals\n",
    "`TODO`\n",
    "## 10.2 Hidden Markov Baseline\n",
    "`TODO`\n",
    "## 10.3 Long Short Term Memory Network Baseline\n",
    "`TODO`\n",
    "## 10.4 Fine Tuning BERT: State of the Art\n",
    "`TODO`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JS1wR3fVMFO2"
   },
   "source": [
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZwNeJWNMFO3"
   },
   "source": [
    "# 11. Feature Importance\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 11.1 The Revealing Nature of Noticing\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 11.2 LIME: State of the Art Feature Importance\n",
    "\n",
    "`TODO`\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sihQ7WgMMFO4"
   },
   "source": [
    "# 12. Meta-discussions: Neuroscience, Ethics, Mycelial Connectedness, and Future Directions - The Truly Important Stuff\n",
    "\n",
    "## 12.1 What is Language\n",
    "‚Üí model discussed with Daniel Colson\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.2 What is (Big) Data\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.3 What is Data Privacy and Security\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.4 What is Reproducibility\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.5 Open Science: A Proposal for Collaborative Research\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.6 Towards Mycelial Connectedness\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.7 The Institute of Paradise Engineering\n",
    "\n",
    "`TODO`\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOeBLfL8MFO6"
   },
   "source": [
    "# 13. Gratefulness: Acknowledgements of Foundational Connectness\n",
    "\n",
    "`TODO`\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1W_CCgXMFO7"
   },
   "source": [
    "# 14. Appendix: archived code, explanations, and miscellaneous musings\n",
    "\n",
    "## 14.1 Word Cloud of this thesis\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 14.2 Word Clouds of Pivotal Conversations\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 14.3 Original Thesis Proposal\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 14.4 Updated Thesis Proposal\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 14.5 The Mathematics of Higher Dimensions\n",
    "\n",
    "`TODO`\n",
    "\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjJcO3qEMFO7"
   },
   "source": [
    "# References\n",
    "\n",
    "- TODO: Use Bibtex  \n",
    "- TODO: Is there a way to link to citations from Jupyter Notebook? Maybe just use latex\n",
    "- [Workflow for citations in jupyter notebook](https://sylvaindeville.net/2015/07/17/writing-academic-papers-in-plain-text-with-markdown-and-jupyter-notebook/)\n",
    "- [The Automated Academic](https://sylvaindeville.net/2015/01/04/the-automated-academic/)\n",
    "- TODO: Cite scikit learn\n",
    "- [Publishing on Figshare](https://figshare.com/)\n",
    "- [Fine tuning word embeddings on smaller dataset](https://arxiv.org/abs/1801.06146)\n",
    "- TODO: [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\n",
    "- TODO: [Use sciPy pretrain transfer learning](https://spacy.io/usage/v2-1#pretraining) to improve the accuracy of model, similar to `Google`'s BERT and `fast.ai`'s ULMFiT\n",
    "- TODO: [spaCy Visualizers](https://spacy.io/usage/visualizers) SOOOO COOOL!!\n",
    "- Publishing my paper:\n",
    "    - [Binder, turn git repo into notebooks](https://mybinder.org/) with docker images\n",
    "    - [Jupyter Hub, serve notebooks to multiple users](https://jupyterhub.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SgQHbW_HMFO8",
    "outputId": "f9019ab2-35ba-4a45-e0b8-9d5631156941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['report', 'title', 'substance']\n",
      "[\"After having had some success with other forms of legal highs, I decided to experiment with salvia.  I had heard it was an intense but short acting psychedelic, which intrigued me.  Although I have loved psychedelics since I first discovered them eight years ago, I dont use them that often in part because of the time involved in tripping and analyzing the experience.  That said I have used psychedelics somewhere around three dozen times in my life.  My favorite method of tripping is LSD, but I have also used mushrooms, morning glory and baby woodrose seeds.  I have also used a variety of other drugs, including stimulants, depressants, empathogens, and deleriants.\\n\\n\\n\\nI ordered salvia online, choosing a 5x extract.  When the package arrived my husband wanted to try it immediately.  I prefer to prepare more for trips but knowing the effects werent supposed to last very long I said why not?  My husband, who uses psychedelics rarely now but used them a lot when he was younger, went first.  He took a large hit from our glass pipe.  He coughed a lot but reported no initial effects.  Only two minutes later he took two smaller but decent sized hits.  He started to smile that tripped out smile and I knew it had hit him.  He was silent but looked around like everything was captivating.  After a few minutes I asked how he felt and he said great and started to laugh.  He described a pouring feeling as though he was being poured into his body.  When he had recovered a bit more he told me it was like acid, which surprised me as the reports I read were not really acidlike.  He stumbled a bit but seemed to be mostly normal after ten minutes.  I asked him to hand me the pipe when he felt sober enough to babysit me and he handed it over five minutes later (15 minutes after smoking).\\n\\n\\n\\nHere is where I made a big mistake.  Seeing that it had required 3 hits to get my husband off, I decided to take two hits.  I had read enough reports to know better, but the extremely short duration of the trip made me feel safe.  I took the first hit slowly and smoothly (a pleasant smoke I might add), holding it in for about 20 seconds.  I blew out a huge cloud of smoke, and started to take another hit.  Halfway through the second hit (while still inhaling) I got tunnel vision.  I hardly felt the smoke leave my lungs.  The whole room was spinning and everything had trails.  I was shocked and amazed at the change in my surroundings.  It was like a cross between shroom trails and the bed spins (like from too much alcohol).  I felt two things at once.  Part of me was feeling totally disoriented and wanted everything to stop moving.  And part of me was having fun.  I realized that if I looked at the center of the room things were much better and I started to laugh really hard.  I was in ecstasy and in pain.\\n\\n\\n\\nThen the center of the room began to spin wildly too.  I felt like I was in the hold of a ship being thrashed about by massive waves.  My husband was standing half on the balcony smoking.  At this point he walked in a step and said Welcome to my world.  At that instant the whole room landed hard on its side, taking my husband and me with it.  Gravity was now pulling me in towards the kitchen, which is normally North.  South, the direction of the balcony, was up.  Both me and my husband were stuck sideways, our legs dangling in the air, inches from the kitchen counter's side, which was now the floor.  I was able to move my parts around my center of gravity, but not able to move my center of gravity.  I did not fall to the 'floor' because I was caught in the grip of two opposing forces.  One at my back and the other at my front.\\n\\n\\n\\nI was certain that I had been sucked into an alternate dimension or world, which I called sideways world.  This was what my husband meant when he said welcome to my world, or so I thought at the time. Although this new world had definite direction it was spinning about this new axis at lightening speed. I kept trying to turn, hoping that a new perspective would stop everything, but I felt as much as saw the added trails this caused.  In fact my whole body was spinning and contracting around its axis, which ran down the spine.  This was highly painful and utterly terrifying.  I have had scary moments on drugs before but nothing that comes close to this.  I didnt know that fear could be like that.  I had had one terrifying moment of clarity in a dream once, when I knew that there was no afterlife and that my consciousness would cease to exist.  This rivaled that dream in the fear department but was far more frantic.\\n\\n\\n\\nI hated my husband for damning me to this world.  I knew that it was a drug experience, and that it would end, but I had forgotten how long it would take and thought I would be like this for hours.  I was certain that he was sideways too, that he knew what this drug would do to me and let me do it anyway, so that he wouldn't be alone in this sideways world.  I looked at the door to the balcony.  The door my husband had stepped through before he turned over.  It was salvation.  The way out.  The door to the straight world.  I tried very hard to reach that door, but the world was spinning too wildly and all I could do was spin about my axis.  The door seemed to be closer at times and farther at others but I dont recall ever making progress toward it or moving away.  I yelled at my husband for putting me here, though I wasnt sure if the words came out.  I remember him talking to me but I was too frantic to listen.  His answers made no sense either.  In response to something I said he replied But Im from this world.  This struck me as utterly stupid.  He had walked into the sideways world through that door.  I saw him do it.  How could he deny it?  But as I told him this something clicked.\\n\\n\\n\\nThat door wasnt the door to the straight world!  At the same time the spinning slowed and I realized I was coming down.  Thank God!  I lay on the couch for several minutes, letting the feeling slip away.  Still terrified and in pain but now with the knowledge that it was ending.  All I had to do was wait.\\n\\n\\n\\nWhen the last of the terror had subsided I sat up and looked around.  Things were glowy and trippy like the come up of acid and I felt stoned with a mdma edge.  This lasted for about half an hour and was great.  If this was all salvia did I would smoke it all the time.  I no longer felt fear at all but a sense of awe at what the drug had done to me.  My hubby was obviously very shaken.  I knew I put him through a lot and told him several times that I was better now.  As the come down progressed I discussed my experience with him and discovered that what he saw was quite different from what I had perceived.\\n\\n\\n\\nFor his point of view, everything was fine at first.  I was laughing for at least three minutes and appeared to be enjoying myself (though in reality the fear was already there).  He stepped in after finishing a cigarette and said Welcome to my world because everything was funny to him too.  At this point I apparently jumped up and said I had to get out of this world.  I ran for the balcony door and my husband, afraid I meant to jump off the balcony, stopped me.  The next five minutes were a drag out physical struggle in which I constantly lunged for the door while he restrained me and tried to talk me down.  He said he told me a lot of things that I have no memory of.  I was also yelling and screaming at the top of my lungs.  Hubby was afraid someone would call the cops with all the ruckus I was causing.  Most of this time I was on the floor, but I fought my way up again (apparently I managed to get up several times which is surprising because my husband is much stronger than I am physically) and he put me on the couch were I lay and went limp for the first time.  This is were I came back to reality.\\n\\n\\n\\nA few days later I decided to try again, this time with less salvia.  I did not want to experience a full salvia trip again but was curious where this powerful drug might take me with more caution.  I filled the pipe bowl with a 80/20 mixture of mugwort (gives a nice stoned feeling) and salvia.  I smoked this in two hits and found myself in the happy glowy MDMA like state that I had experienced before as a come down.  I waited 5 minutes and nothing else happened so I made another bowl with a half and half mixture of the same drugs.  A hit of that put me in a deeper trip.  I experience the same feeling but with far less intensity.  This time it was the closet door that I felt I needed to go to.  I closed my eyes and buried my head in my husbands shirt until the feeling past.  I can see how many people see tunnels on salvia, since I keep focusing on doors.  After this second attempt I realized that salvia was going to keep taking me to the same place until I had a better idea of where I wanted to go and how to get there.\\n\\n\\n\\nI came away from this trip terrified of salvia but also deeply intrigued.  Someday I would like to try it again, at a smaller dose and with more preparation.  This drug has the power to take me somewhere amazing but I am not ready for it yet.\", 'Sideways World', 'Salvia divinorum (5x extract)']\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset trips.csv\n",
    "import csv\n",
    "printed = 0\n",
    "with open('trips.csv') as f:\n",
    "  csv_reader = csv.reader(f)\n",
    "  for line in csv_reader:\n",
    "    if printed < 2:\n",
    "        print(line)\n",
    "        printed += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PLwYoYTMFO_"
   },
   "source": [
    "# Epilogue: The Stories that are Not Part of the Story\n",
    "\n",
    "`TODO`\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRjDNGpjMFPA"
   },
   "source": [
    "## > Issues Encoutered, and solutions\n",
    "\n",
    "- Python not working:  \n",
    "`brew install python`  \n",
    "`brew update python`  \n",
    "- Need to update xcode command line tools after every major / minor os upgrade: `xcode-select --install`; see [here](https://stackoverflow.com/questions/52522565/git-is-not-working-after-macos-update-xcrun-error-invalid-active-developer-pa)\n",
    "\n",
    "- Issues with installing [wordcloud](https://www.datacamp.com/community/tutorials/wordcloud-python)\n",
    "- TODO: Write an article explaining managing python versions on Mac, using both `Anacoda` and `pip`\n",
    "- Cannot do `brew link <packageName>`; see [here](https://github.com/caskformula/homebrew-caskformula/issues/10)\n",
    "- Adding `autocompletion` to jupyter notebook with [this tool](https://github.com/ipython-contrib/jupyter_contrib_nbextensions)\n",
    "- Could not for the life of me get `toc2` to work\n",
    "- 5 tries... with trying to get code hiding to work; 2019-12-04, still no luck...\n",
    "- `TODO`: Putting a pin in this for now rip\n",
    "- see [this](https://gist.github.com/parente/35f5d3a9145bd3f030c8#file-nbviewer_code_toggle-ipynb)\n",
    "\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klZgi3AQMFPA"
   },
   "source": [
    "## > Fun (Optional) things discovered while working on Project\n",
    "- Add [animated progress nav](https://lab.hakim.se/progress-nav/)\n",
    "- [New York Times recommendation system](https://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine)\n",
    "- [CSS background tricks](https://tympanus.net/codrops/css_reference/background/)\n",
    "- [spaCy webapplication demo](https://explosion.ai/demos/displacy)\n",
    "- [spaCy NER visulizer demo](https://explosion.ai/demos/displacy-ent)\n",
    "- [NLP with spaCy course](https://course.spacy.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9ARq4FbMFPA",
    "outputId": "343c86dd-7be0-4ce5-9f87-0b65360fddb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdn.rawgit.com/parente/4c3e6936d0d7a46fd071/raw/65b816fb9bdd3c28b4ddf3af602bfd6015486383/code_toggle.js\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script src=\"https://cdn.rawgit.com/parente/4c3e6936d0d7a46fd071/raw/65b816fb9bdd3c28b4ddf3af602bfd6015486383/code_toggle.js\"></script>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "u8fV7T4fMFMC",
    "DheynHhTMFMq",
    "yKTfJkwhMFM2",
    "PQeY7facMFM7",
    "ji6UVIgVMFM8",
    "PgS72yzBMFM9",
    "vCF-7LBMMFM-",
    "AtL_hbqDMFM-",
    "7_07skQvMFM_",
    "T2hZ6KUTMFNA",
    "fM6_hS1AMFNd",
    "SYmKZ_9BMFNk",
    "EmAqQ_CAMFNo",
    "effsJXdgMFNs",
    "DrMipNNBMFN0",
    "I2iF_EoiMFN6",
    "nFwfZxVAMFOK",
    "LTOARHJbMFON",
    "n3CtZ1_FMFOb",
    "24LzCMPJMFOf",
    "3Ze1-nKuMFOh",
    "AGaF4ZfOMFOp",
    "R_F-ygOdMFOw",
    "-KqeOVrpMFOx",
    "TRIch4iTMFOx",
    "HRjDNGpjMFPA",
    "klZgi3AQMFPA"
   ],
   "machine_shape": "hm",
   "name": "AlexZhao_SeniorThesis_Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
