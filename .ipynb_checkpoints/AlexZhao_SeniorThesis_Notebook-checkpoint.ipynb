{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensions of Non-Ordinary Experiences: Natural Language Processing of Trascendent Experience Reports\n",
    "\n",
    "`Senior Thesis [EAS 499]`\n",
    "\n",
    "`University of Pennsylvania, Fall 2019`\n",
    "\n",
    "#### Author: **Alex Tianheng Zhao**  \n",
    "> `alexzhao@seas.upenn.edu`  \n",
    "> Department of Computer and Information, School of Engineering and Applied Science, University of Pennsylvania  \n",
    "> Department of Statistics, The Wharton School, University of Pennsylvania  \n",
    "> üåê[`personal website`](https://alextzhao.io), [`github`](https://github.com/alextzhao), [`linkedin`](https://www.linkedin.com/in/alextzhao), [`ORCID`](https://orcid.org/0000-0001-6745-5980)\n",
    "\n",
    "\n",
    "#### **Max Mintz, PhD**  \n",
    "> `mintz@cis.upenn.edu`  \n",
    "> Coordinator, Senior Thesis Program, Department of Computer and Information Science  \n",
    "> University of Pennsylvania  \n",
    "\n",
    "\n",
    "## **Thesis Advisors**\n",
    "> **Chris Callison-Burch, PhD**  \n",
    "> `ccb@upenn.edu`  \n",
    "> Department of Computer and Information Science (SEAS), University of Pennsylvania  \n",
    " \n",
    "> **Lyle Ungar, PhD**  \n",
    "> `ungar@cis.upenn.edu`  \n",
    "> Department of Computer and Information Science; additional appoints in the Departments of Bioengineering (SEAS); Genomics and Computational Biology (Penn Medicine); Operations, Informations, and Decisions (Wharton); Psychology (SAS), University of Pennsylvania  \n",
    "\n",
    "\n",
    "### **Useful Links:**\n",
    "- Thesis Related:\n",
    "    - [Thesis Master Document](https://docs.google.com/document/d/1dk1xXyfHqfdn5Tld-KZu7toiNYQeJHQv7BUlG7uSqP4/edit#)\n",
    "    - [Thesis Scratch Paper](https://docs.google.com/document/d/1BP5Z2J9tJvRJB5J-hthQIGrdSnD0Bcvctd7kHqbUUKw/edit?usp=sharing)  \n",
    "    - [Thesis Codebase](https://github.com/alextzhao/psychedelicNLP)\n",
    "- Helpful Tutorials and Tips\n",
    "     - Excellent Series from `Towards Data Science`:\n",
    "         - [A Practitioner's Guide to Natural Language Processing (Part I): Processing & Understanding Text](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)\n",
    "         - [Feature Engineering: Continuous Numeric Data](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "         - [Feature Engineering: Categorical Data](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "         - [Feature Engineering: Text Data Basics](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)\n",
    "         - [Feature Engineering: Text Data Advanced](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n",
    "     - [A few userful things to know about machine learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "     - [LDA: Excellent Talk by Christine Doig on Topic Models](http://chdoig.github.io/pygotham-topic-modeling/#/)\n",
    "     - [Markdown Cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#links)\n",
    "     - [Using R and Python together](https://stackoverflow.com/questions/39008069/r-and-python-in-one-jupyter-notebook); [python, R dataframe interoperability](https://rpy2.github.io/doc/latest/html/pandas.html); [R and python pipelining](https://blog.revolutionanalytics.com/2016/01/pipelining-r-python.html)\n",
    "     - [Translate dplyr to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html)\n",
    "     - [Basic pandas tutorial](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html#min)\n",
    "     - [Recommended dependencies for pandas](https://pandas.pydata.org/pandas-docs/stable/install.html#install-recommended-dependencies)\n",
    "     - [Pandas: Working with Text Data](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html)  \n",
    "     - [sklearn reference](https://scikit-learn.org/stable/modules/classes.html): handy documentation, and provides broad-strokes ontology for machine learning techniques\n",
    "     - [Introduction to Machine Learning with sklearn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
    "     - [Sample pipeline for text feature extraction and evaluation](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py): using `sklearn`\n",
    "     - [Comparison of performances of different classifiers](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py): from `sklearn`, super cool\n",
    "     - TODO: [Citing sklearn](https://scikit-learn.org/stable/about.html#citing-scikit-learn)\n",
    "     - [Contributing to sklearn](https://scikit-learn.org/stable/developers/contributing.html)\n",
    "     - [Manually Creating a table of contents](https://medium.com/@sambozek/ipython-er-jupyter-table-of-contents-69bb72cf39d3)\n",
    "     - [Automaticaly Creating Table of Contents](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html)\n",
    "     - [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook): Free Jupyter Notebooks\n",
    "     - [Configuring jupyter notebook with extensions](https://github.com/Jupyter-contrib/jupyter_nbextensions_configurator)\n",
    "- Publishing my thesis:\n",
    "    - [Jupyter Notebook Viewer](https://nbviewer.jupyter.org/)\n",
    "    - [Hiding Code Cells for Better Viewing Experience](https://chris-said.io/2016/02/13/how-to-make-polished-jupyter-presentations-with-optional-code-visibility/)\n",
    "    - [Cool Preloaders](https://icons8.com/preloaders/)\n",
    "   \n",
    "\n",
    "![Pandas Indexing Cheatsheet](./pandas_indexing_cheatsheet.png)\n",
    "- Notes to self:\n",
    "    - `~/opt/anaconda3/lib/python3.7/site-packages/jupyter_contrib_nbextensions/nbextensions/toc2` path to nbextensions toc2\n",
    "    - [fixing nbextensions](https://github.com/ipython-contrib/jupyter_contrib_nbextensions/issues/1090)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ Cover Page End ]  \n",
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "`TODO`\n",
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Abstract\n",
    "\n",
    "Using natural language processing (NLP) techniques on approximately 30,000 reports of transcendent experiences, we examine and better understand the dimensions of non-ordinary states of consciousness. The primary data source is the Vaults of Erowid, which consists of approximately 25K psychedelic trip reports. Through this work, we identify the hidden topics of subjective experiences through topic modelling, clusters of similar experiences, and visualize the dimensions of subjective experiences through word clouds and other visualization techniques. We also demonstrate the ability to predict (with fairly high confidence) the origins (i.e.: psychedelic drug of choice, set and setting) of such transcendent experiences. Finally, we place this research in historical and academic context, and make a proposal for a more connected and blissful future.\n",
    "\n",
    "For link to the work in progress codebase, see the [public project github](https://github.com/alextzhao/psychedelicNLP)\n",
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background and Signifinance\n",
    "\n",
    "## 2.1 What are Psychedelics\n",
    "// todo insert brief legal background\n",
    "\n",
    "**_Psychedelics_** (from the Greek *psyche*: mind, *delos*: make visible, reveal; together meaning ‚Äúmind manifesting‚Äù, coined in a letter from Humphrey Osmond to Aldous Huxley), are ‚Äúsubstances that induce a heightened state of consciousness characterised by a hyperconnected brain state‚Äù (todo). The most well known ‚Äúclassical psychedelics‚Äù, some of which have been used by indigenous cultures for thousands of years, activate the serotonin system, in particular by agonizing the 5HT-2A serotonin receptor subtype. These classical psychedelics include Lysergic Acid Di-amide (LSD, semi-synthetic psychedelic derived from the ergot fungi), Psilocybin (inactive prodrug to the psychoactive Psilocin, found in certain mushrooms), Di-mythyltryptamine (DMT, the active ingredient in the powerful, indigenous Amazonian brew Ayahuasca), and Mescaline (found in Peyote Cactus). Newer psychedelics such as 2-CB, 5-Methoxy-Dimethyltryptamine (5-Meo-DMT, found in multiple plant species and most famously the Sonoran Desert Toad) are also often classified under the term hallucinogen or psychedelic. Other related drugs such as 3,4-Methylenedioxymethamphetamine (MDMA) are often referred to as psychedelic drugs, but are more correctly classified as an empathogen or entactogen (todo) (meaning ‚Äútouching within‚Äù,  from the Greek en: within), the Latin tactus: touch and the Greek -gen: produce)\n",
    "\n",
    "## 2.2 Motivation: Why This Thesis Exists\n",
    "\n",
    "// todo: discussion on mental health\n",
    "\n",
    "On September 4th, 2019, Johns Hopkins University launched the Johns Hopkins Center for Psychedelics and Consciousness Research, with $17 million dollars in research funding. The center is thought to be the largest of its kind in the world, and sets to study addition, PTSD, addiction, depression, anorexia nervosa, trauma, and other mental ‚Äúillnesses‚Äù with psychedelics and traditional forms of therapy. \n",
    "\n",
    "After decades of suppression and moral hazard, psychedelics are making a return to legitimate research as a novel and surprisingly effective treatment modality for multiple extremely difficult conditions, including but not limited to end of life anxiety (todo), treatment-resistant depression (todo), post traumatic stress disorder (PTSD) (todo), alcohol and nicotine addiction (todo), sexual and racial trauma, and alzheimer‚Äôs (todo). Psychedelics have also shown dramatic results for ‚Äúwell-people‚Äù, including being able to reliably induce ‚Äúmystical-type experiences‚Äù (todo), increase creativity, and is correlated with greatly enhanced feelings of religious devotion (todo). Beyond clinical studies, animal studies with octopi have demonstrated MDMA increasing feelings of social connectedness (todo); In vitro studies have shown enhanced neuroplasticity (todo Victor preprint), orchestrated brain activity; Brain imaging studies, primarily out of Imperial College London and University College London, have shown psychedelics quiet the default mode network (todo), induce hyperconnected and entropic brain states (todo), and proposed a mechanism for action for psychedelics, formalized as the Free Energy Principle and RHEBUS (todo). Selena Atasoy of Oxford University has also recently demonstrated that the brain under psychedelics has more high energy resonant states, while remaining hyper-coherent and highly orchestrated, using a novel technique called Connectome Specific Harmonic Waves (CSHW). The Qualia Research Institute (QRI) has also recently proposed the Symmetry Theory of Valence (STV), which posits that symmetries in resonant brain states as modelled by a high dimensional mathematical objects, is correlated with experiential valence.\n",
    "\n",
    "With all the exciting work being done in the psychedelics landscape (call it ‚Äúpsychedemia‚Äù), be it clinical, neuroscientific, or anthropological, little work has been done on understanding what the psychedelic trip actually consists of, and understanding qualitatively and quantitatively the dimensions of the psychedelic subjective experience. Current work by Imperial College and Oxford (todo) are showing promising results for understanding both the macro and micro phenomenology of psychedelic experiences. Using natural language as a low-dimensional, inadequately wanting proxy, we can begin to understand the hallmark subjective signatures of different substances, and potentially identify novel neuro-targets for clinical interventions to reduce suffering, as well as gain a deeper understanding of mechanisms of action for psychedelic substances, if such a mechanistic understanding is even possible. It is the sincere hope that through this work, we may begin to demystify the subjective dimensions that characterize psychedelic experiences, and lay the foundations for more informed clinical and societal support structures that may alleviate unnecessary mental suffering and enrich the lives of well-people.  \n",
    "\n",
    "This thesis seeks to extend the current work by using techniques from machine learning, and in particular natural language processing, on a corpus of psychedelic trip reports retrieved from multiple sources, including chiefly, the Vaults of Erowid. We will present the results factually, with minimal layers of interpretation and meticulous documentation for high reproducibility, clarifying our assumptions and processes whenever appropriate. It is our hope that this work may be open-sourced and beneficial to clinical researchers, psychologists, neuroscientists, machine learning engineers, social scientists, and the lay people community as a whole. If not for some deep, metaphysical takeaway, perhaps for some lighthearted, jovial pass time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Intended Audience\n",
    "\n",
    "There is no single mold of an audience for which this thesis is written. A reader who is familiar with the basics of linear algebra and statistics, in particular the basic concepts and notations of vectors, matrices, and probability distributionsm will find the thesis straighforward to follow especially at times when there are slight conceptual leaps. A few sections contain more advanced probability theory, but those can be skipped without sacrificing the overall narrative structure of the thesis. No prior knowledge is assumed about techniques in machine learning and natural language processing (NLP), as the foundations will be presented from first principles by first relying on intuition and supported by mathematics for those interested. The final sections of the thesis consists of musings, extensions, and bonus content for the curious reader, and contain a few non-technical elaborations intertwined with technical notes.\n",
    "\n",
    "Perhaps more important than prior knowledge for the reader is an innate curiosity, a child-like delight in exploring new frontiers; one who is drawn to principles of foundational interconnectedness and not afraid to embrace concepts that seem at first challenging for the unitiated, yet delightful for the persistent, may find this thesis enjoyable.\n",
    "\n",
    "\n",
    "\n",
    "## 2.4 The Machine Learning Pipeline\n",
    "\n",
    "The field of machine learning is very broad, and indeed a complete overview of what machine learning would fill a complete tome and certainly out of scope for this thesis. That being said, there are a few generalizable insights and principles that are very important to mention. They are highly inspired by Professor Pedro Domingos's excellent paper, [A Few Useful Things to Know about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) and my work in progress computer science and statistics education\n",
    "- machine learning is about generalizing from examples\n",
    "- machine learning algorithms take _data_ as inputs, and attempt to find _patterns_ in the data, in order to accomplish some task(s) of interest\n",
    "- There are a few key types of problems in machine learning, and often the first step to translating theory into practice is recognizing the type of problem you're trying so solve:\n",
    "    (1) **Regression** problems are about predicting _continuous_ outcomes **y** from a collection of continuous or categorical features **X**, such as predicting housing prices (**y**) from \\[ square footage (continuous feature), and whether it's in a good neighborhood (categorical feature) \\] (**X**); Regression is a _supervised_ problem because it requires labelled training data **y**\n",
    "    (2) **Classification** problems are about predicting discrete class labels **y** from a collection of continuous or categorical features **X**, such as classifying whether a person has a heart condition (discrete **y**) based on \\[ blood pressure (continuous feature), and smoking habits (discrete feature) \\]; Classification is a _supervised_ problem because it requires labelled training data **y**\n",
    "    (3) **Clustering** problems are about finding clusters of observations that are most ‚Äúsimilar‚Äù to each other, such as finding customers who are similar, or mushrooms specifies are most closely related to each other; Clustering is an _unsupervised_ problem because it doesn't require labelled training data **y**: the patterns are discovered directly from the observations.\n",
    "\n",
    "\n",
    "\n",
    "This thesis employs the [CRISP-DM](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.198.5133&rep=rep1&type=pdf) (Cross Industry Standard Process for Data Mining) pipeline, which is a widely used open standard by professional data scientists as a standard data mining approach\n",
    "\n",
    "![CRISP-DM Model](./images/infographic_CRISP-DM.png)\n",
    "[`CRISP-DM Model Infographic Source`](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "\n",
    "< todo: i'm getting a little too distracted here >\n",
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Acquisition\n",
    "\n",
    "\n",
    "\n",
    "This data can be structured (like an excel spreadsheet) or unstructured (like documents). It can be in the format of survey data with numbers, checkboxes, slidings scales; the fields can be numeric (continuous numbers) or categorical with discrete levels (such as Gen I, Gen II, Gen III pokemons). The data can even be images, videos, text, or music. Virtually anything you think of can be used as data to machine learning algorithms, _provided_ they are representation in the appropriate format that is suitable for the algorithm of choice, and the problem at hand. Indeed, the **quality** and **quantity** of the data often determines the performance of the machine learning system. If your data is very biased, your results will be biased. If your dataset is small, your machine learning model\n",
    "\n",
    "\n",
    "\n",
    "`TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Sources\n",
    "\n",
    "`TODO`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data Scraping Procedure\n",
    "\n",
    "`TODO`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 The Data: Raw and Unfiltered\n",
    "`TODO`: Explain data source  \n",
    "`TODO`: Screenshot of Erowid  \n",
    "`TODO`: Explain scraping Erowid with Selenium and lxml  \n",
    "`TODO`: Explain extract the relavent information  \n",
    "`TODO`: Credits to Yev  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, there were 19924 substance, report having a `report` body, a `title`, and a `substance` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19924 entries, 0 to 19923\n",
      "Data columns (total 3 columns):\n",
      "report       19915 non-null object\n",
      "title        19924 non-null object\n",
      "substance    19924 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 467.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_tripReports = pd.read_csv('trips.csv')\n",
    "pd.set_option('display.width', 80)\n",
    "pd_tripReports.shape\n",
    "pd_tripReports.info()\n",
    "# TODO: Make this pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>title</th>\n",
       "      <th>substance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After having had some success with other forms...</td>\n",
       "      <td>Sideways World</td>\n",
       "      <td>Salvia divinorum (5x extract)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me and a couple of my buddies decided one nigh...</td>\n",
       "      <td>Physical Wellbeing = Crucial</td>\n",
       "      <td>Mushrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My girlfriend and I had been saving the methyl...</td>\n",
       "      <td>The Artful Dodger</td>\n",
       "      <td>Methylone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just want to warn anybody taking Lithium (or...</td>\n",
       "      <td>Seizure Inducing Combo</td>\n",
       "      <td>LSD &amp; Lithium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have had several attempts before this to bre...</td>\n",
       "      <td>Enlightenment Through a Chemical Catalyst</td>\n",
       "      <td>5-MeO-DMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              report  \\\n",
       "0  After having had some success with other forms...   \n",
       "1  Me and a couple of my buddies decided one nigh...   \n",
       "2  My girlfriend and I had been saving the methyl...   \n",
       "3  I just want to warn anybody taking Lithium (or...   \n",
       "4  I have had several attempts before this to bre...   \n",
       "\n",
       "                                       title                      substance  \n",
       "0                             Sideways World  Salvia divinorum (5x extract)  \n",
       "1               Physical Wellbeing = Crucial                      Mushrooms  \n",
       "2                          The Artful Dodger                      Methylone  \n",
       "3                     Seizure Inducing Combo                  LSD & Lithium  \n",
       "4  Enlightenment Through a Chemical Catalyst                      5-MeO-DMT  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 6 entries of the data\n",
    "pd_tripReports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>title</th>\n",
       "      <th>substance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19919</th>\n",
       "      <td>First off - I want to answer the question, 'Do...</td>\n",
       "      <td>My Truth About Seeds...</td>\n",
       "      <td>Morning Glory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19920</th>\n",
       "      <td>It was actually my mom who got me on Ritalin. ...</td>\n",
       "      <td>The Help I Hate to Love</td>\n",
       "      <td>Methylphenidate (Ritalin)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19921</th>\n",
       "      <td>This being a hopefully useful report on low-do...</td>\n",
       "      <td>Meeting the Spirit Plus a Territorial Dog</td>\n",
       "      <td>Peruvian torch cacti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19922</th>\n",
       "      <td>This journal shall hopefully be a guide for me...</td>\n",
       "      <td>A Journey Journal</td>\n",
       "      <td>Salvia divinorum (5x extract)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19923</th>\n",
       "      <td>I ingested 12-15mg of 2ce orally.\\n\\n\\n\\nI dum...</td>\n",
       "      <td>A Late Winter's Trip</td>\n",
       "      <td>2C-E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  report  \\\n",
       "19919  First off - I want to answer the question, 'Do...   \n",
       "19920  It was actually my mom who got me on Ritalin. ...   \n",
       "19921  This being a hopefully useful report on low-do...   \n",
       "19922  This journal shall hopefully be a guide for me...   \n",
       "19923  I ingested 12-15mg of 2ce orally.\\n\\n\\n\\nI dum...   \n",
       "\n",
       "                                           title  \\\n",
       "19919                    My Truth About Seeds...   \n",
       "19920                    The Help I Hate to Love   \n",
       "19921  Meeting the Spirit Plus a Territorial Dog   \n",
       "19922                          A Journey Journal   \n",
       "19923                       A Late Winter's Trip   \n",
       "\n",
       "                           substance  \n",
       "19919                  Morning Glory  \n",
       "19920      Methylphenidate (Ritalin)  \n",
       "19921           Peruvian torch cacti  \n",
       "19922  Salvia divinorum (5x extract)  \n",
       "19923                           2C-E  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last 6 entries of the data\n",
    "pd_tripReports.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This journal shall hopefully be a guide for me into better understanding the effects of Salvia Divinorum. Like many journals I have attempted to write, they all end in lack of desire to finish them. I plan to make this journal different. It is my hope that I describe each experience in the fullest extent to better understand the unique nature of this plant. \\n\\n\\n\\n9/19/2002\\n\\n\\n\\nI received two grams of 5x extract. I was excited for my first experience and called over my friend to watch over me, since it was my first time and I did not know how I would react.\\n\\n\\n\\nI prepared my water pipe and put a miniscule amount into the bowl, and lit it up. I inhaled and held the smoke, which wasnt much. After exhaling, I did not feel much until about a minute, when I felt a mild buzz but not much else. I was disappointed but decided to try more. I put a equal amount of salvia in the bowl and lit up again. I did not feel anything much.\\n\\n\\n\\nI decided that I was not doing enough, so about ten minutes later; I filled the bowl with a decent amount and lit it up. Upon exhaling, I did not feel anything right away, but then it hit me. I was amazed at this feeling and commented that I could not believe that this drug was legal. \\n\\n\\n\\nAll of a sudden, my body felt as if it were covered in ants. I was very itchy and sweaty and threw off my over-shirt and was about to take off my t-shirt when I said, No, this is just the drug. I pulled my shirt back down. I began to talk in a high-pitched voice, saying nonsensical things.\\n\\n\\n\\nMy perspective radically changed. I was standing at the edge of a cliff with my back to the precipice and beyond it was nothing. Nothing existed beyond the cliff. My universe shrunk till all it included was my field of vision in front of me. My universe was only 50 square feet, and that seemed fine. I was still in my room, but I did not recognize it as such. I was in a different place, not unpleasant but interesting/weird (for lack of a better word). I stayed in that universe till I came down.\\n\\n\\n\\nMy next conscious thought was that of me cleaning out my pipe in the sink in preparation for another trip. My friend brought a video camera to record this trip. I went back to my room and chilled for a few minutes and then refilled the pipe and lit up. After two inhales to clear the chamber, I did not have much perspective shifting, but mostly an extreme stoning effect similar to weed but noticeably different. I mentioned how I was amazed that my second exhale did not contain that much smoke. I also had a vague feeling that as I was talking, I was part of a childrens television show, explaining things to the viewers. I also mentioned the television feeling. I began to come down then, and I turned on my CD player and laid on my bed, attempting to have visuals, but to no avail.\\n\\n\\n\\nI was feeling mildly disappointed, so I decided to inhale some nitrous. It gave me the typical nitrous rush, and I commented how I wished I could feel like this all the time, but neither drug intensified the other. \\n\\n\\n\\nThe effects wearing off, I thought about what I had experienced. It was a nice trip, but not quite a introspective and spiritual experience. However, I do feel that I started on a good level and plan to continue exploring consciousness with salvia.\\n\\n\\n\\n9/21/2002\\n\\n\\n\\nIt has been two days since my first experience with salvia. I enjoyed my first experience immensely, but I found I was a tad disappointed that I did not experience any introspective or mind-altering states.\\n\\n\\n\\nAbout 3:30pm, I load the bowl almost to the top with salvia. I clear the chamber and wait. I feel the familiar itch/sweat that I expect will be common to all my salvia experiences. It is not wholly unpleasant, but it is more comfortable without a shirt and A/C.\\n\\n\\n\\nI felt the salvia begin to take hold. However, it is only a mild stoning effect and I decide that I must smoke another bowl as to achieve the desired effects. I loaded up the bowl with more and smoke it fully.\\n\\n\\n\\nThis is where my trip truly starts. When I finished exhaling, I stared at the windowsill and a feeling of dj vu came over me. I couldnt help but feel that I had done this (smoking salvia) for many years. I began to feel like I was experiencing a childhood all the way through to fatherhood in an incredibly short amount of time. I had conversations with my best friend as we grew up and played together. As we got older, we each had children of our own and reminisced about the old days and the fun we had and the fun our children were having now.\\n\\n\\n\\nI then stopped and tried to fight the experience. I said out loud, What are you doing? and other things of the like, not fully understanding what I was experiencing. I then decided not to fight it and laid down on my bed and closed my eyes.\\n\\n\\n\\nAs I laid there with eyes closed, a multi-colored spiral/vortex began to swirl in my minds eye. It made one loop and started to form another loop underneath the previous one when I realized that this loop was alive and many intelligent organisms lived within it. The loop continued to move and I could hear the organisms talking.\\n\\n\\n\\nA mother was explaining to her child how each loop represented the generation before them. I understood that I was witnessing new births and that I was the creator of all these beings. The mother kept explaining how old the generations before them were, but none were as old as the ancient entity. I was that entity. I was the creator, the vessel for these beings. The mother told her child that one day, she might take him to talk to the ancient entity, but that he might not talk back. As I listened to me being described as an ancient entity, I began to think of my own beginnings, when I played with my friend and my growing up. During the conversation about meeting the entity, the loop stopped spiraling and turned into a ribbon, resembling a sine wave, and continued to move in that fashion.\\n\\n\\n\\nAfter a while, I began to get annoyed at these creatures, feeling that they had no right to be inside me and using me as a vessel for their own existence. I opened my eyes and grabbed at paper and pen, and with great difficulty wrote these words:\\n\\n\\n\\nAfter I wrote this, I laid back down and began to come off of the plateau. I laid there for a bit, attempting to absorb what I had just experienced, but I was still heavily under the influence.\\n\\n\\n\\nI thought that this would be a perfect time for some nitrous, so I prepared a balloon and inhaled it. I did not think I inhaled enough, but then I peaked and felt as if a giant hand was bending me forward and turning me upside down. I straightened up and came off the nitrous, in a good, but exhausted mood. I wandered around my room for a bit and then called my friend, stating that I had an amazing trip, but was not yet ready to describe it.\\n\\n\\n\\nI laid back down, almost completely devoid of any effects, and thought about what I had just experienced, and realized that I had my first, and amazing, introspective trip. I felt so special and so blown away. I dozed for about a half an hour and woke up and watched some TV. I got dinner and am now writing this. I feel great and the only word that comes to my mind is WOW. I have so much respect for this plant and am grateful for the insight that it has granted me.\\n\\n\\n\\n9/24/2002\\n\\n\\n\\nAt 2:30, I load up and attempt to smoke but accidentally blow half the bowl out. I gathered up most of it, but I must be very careful from now on, since the salvia is such a light herb. I take two full hits.\\n\\n\\n\\nRight after the first hit, I am aware of the effects and the itch/sweat starts. I strip off my shirt after the second exhale. The psychotropic effects begin.\\n\\n\\n\\nI become aware that the waterpipe is alive. It is a person and it talks to me and says that I must put it on the ground. I comply and as I do this, I realize that my entertainment system plus several other random points in space have become alive as well. They start to make fun of me and comment on what a sloppy job I did at moving the pipe to the ground. They laugh at me and I wonder what has made them act this way. I talk back at them, telling them to shut up and mind their own business. It is at this point that I realize that the drug has hold of me. \\n\\n\\n\\nI close my eyes and a wide brownish loop forms, with vague edges. It appears to sparkle with electricity, but the electricity itself is also brown. I see that the loop is actually made up of eight distinct pieces and that each piece represents each different entity living in my room (the ones that were laughing at me). All of a sudden the loop breaks towards the top at one of the junctions between two pieces. The loop becomes a snake-like thing and moves/loops away, out of my field of vision. It then comes back into my field of vision and rushes at me and goes inside my head.\\n\\n\\n\\nThe snake-like thing takes control of my brain and begins to tell me to do things. It specifically tells me to masturbate. I cannot help myself and I am aware that I quickly undo my pants and begin the deed. I kneel onto the ground, and after what seemed like an indefinitely long time, I felt a light tingling as I ejaculated into the trashcan. There was very little feeling involved and compared to other masturbation sessions, it was at the bottom of the scale.  \\n\\n\\n\\nI laid back down on the bed and listen to the music. I have Huey Lewis Its Alright playing. As I listen, I am aware that I had picked out one of the voices within the song and was listening specifically to his part. The rest of the music was there, but I could hear every note of that particular part, which I am almost sure was the baritone. I then play Piano Man by Billy Joel and I notice that I have picked out the bass guitar part, and have just been listening to that.\\n\\n\\n\\nAs I come off the plateau, I sit up and realize that I have a paper due tomorrow and feel compelled to find the information regarding it. I begin searching through my notebooks and papers, finding myself unable to stop moving for five minutes. I am rapidly searching, but to no avail. I clean up all the notebooks and then check the computer and find the information.\\n\\n\\n\\nThe after-effects slowly tapering off, I wanted to clean my pipe, but my roommate was in the kitchen, so I watched TV until he was gone. I cleaned the pipe and begin typing this.\\n\\n\\n\\nAs I become more experienced with salvia and use this one drug repeatedly, I begin to find certain aspects that are similar to each trip. I get the itch/sweat seconds after smoking but it quickly dissipates. My perception of reality changes at the plateau. Each trip has yielded different perceptual changes, but that is to be expected and I hope that two trips will never be the same. I also realize that after the plateau, my memory is a bit fuzzy. I cannot recall very well what I did on the comedown. During the after-effects, I begin to understand what I have experienced. \\n\\n\\n\\nRegarding the effects, I do not realize am I feeling them until a minute into the effect. It is almost as if two brains are working simultaneously. My conscious brain is having the trip while the unconscious brain realizes that something is happening a bit later.\\n\\n\\n\\nSalvia leaves a sort of heaviness in my brain, kind of like after a huge test. I hypothesize that this is due to the fact that my brain did work hard, attempting to understand and comprehend the world around it while under the influence of this wonderful conscious-altering drug.\\n\\n\\n\\n9/24/2002\\n\\n\\n\\nThis is not a report of a trip that I had, just my observations with others were on Salvia. J, D, and R were to partake of some this evening. \\n\\n\\n\\nJ went first and after a monster hit, started coughing and then it must have hit him because he started rubbing his tongue and then licking himself on the sleeve. I encouraged him to take the rest, so he did and remained in a weird state for about 15 minutes. As I was packing a new bowl, he threw an empty box of cigarette at me. It hit me in the head and he apologized profusely and with great sincerity, more than I have ever seen him do. He said he felt like he was more stoned than he ever was for about 5 minutes.\\n\\n\\n\\nD took some and just sat there, eyes glancing around. He mentioned that it felt like J was a doll and J was talking to him, which apparently seemed very weird for a doll.\\n\\n\\n\\nR smoked next and stood up with great difficulty and mentioned something about going out to the lake. He addressed us as grandma and grandpa. He also said, and J confirmed, that he felt leaden to the porch, like he couldnt move.\\n\\n\\n\\nThat was the extent of their trips to the best of my knowledge because I did not discuss it with them in length, but I now know that people have similar reactions to salvia. \\n\\n\\n\\n10/11/2002\\n\\n\\n\\nI received 5 grams of 5x extract a week back, so I prepared and smoked a large bowl of my new batch of salvia (approximately two hits). I felt the familiar itch/sweat, but nothing much else. I was a tad disappointed that I did not feel much. At one point, I think I was mildly aware that my room was a penthouse at the top of an expensive apartment complex, but I cannot be sure. Mostly, I was attempting to force the effects to happen. I think I need to construct a larger, better bong so that I can take more salvia in. Perhaps this batch isnt as potent as the last one. I will build a new bong and attempt to have a better trip this week.\\n\\n\\n\\n10/31/2002\\n\\n\\n\\nI had a great trip with my new bong. Also had S take a few hits and he had a good trip as well. This trip was hard to understand, but I will try to describe it. I was looking out the window and my frame of view contained the lake, horizon, lava lamp, my arm, and S in the chair. I did not recognize them at for what they were, but as one giant slate as if they were painted on it. I just stared at this canvas and it seemed like my arm was connected to the horizon and an invisible energy was flowing from it into the horizon. S moved and I became upset because it felt like the canvas was ripped, so I yelled for him to stop moving around. I slowly came back to reality and did not do much reflecting, but watched S take a good couple hits, which obviously sent him somewhere because he asked Wheres my brother? He thought he was back in his old house. He came back to baseline and we talked about our trips for fifteen minutes, reflecting on how pleasant the trip was and how weird of a drug Salvia is.\\n\\n\\n\\n12/02/2002\\n\\n\\n\\nK and I smoke one bowl together; a hit each. No real discernable effects on my part except a heaviness in my right hand that lasted for about 20 seconds. K just stared out the window for about half a minute. I think I am the only one who can take a hit of Salvia without coughing. Everyone who has smoked it, minus S, has coughed, and most of them are veteran weed smokers. I am sure the lack of effects was due to the extremely small amount I smoked and not the quality of the salvia. It is a few months old, and even though I keep it in a glass jar, I am not sure how long Salvinorin A stays active. When I have a good chunk of time to myself, I will do a massive salvia trip, but since finals are arriving, it will have to wait. \\n\\n\\n\\n4/25/2003\\n\\n\\n\\nIt has been a while since I smoked any salvia. On this particular night, I am tripping on 3.5 grams of mushrooms and 40mg of AMT. I have done about 20 nitrous hits as well. I put Ein Deutsches Requiem on the stereo and take a nice fatty hit.\\n\\n\\n\\nIt hits me right away. The typical salvia mindshift occurs and once again am staring at a foreign horizon. My entertainment center has become a mountain range, shifting in color and growing in intensity. The music begins to take me to a place of unwant, so I attempt to change it. I find that I cannot recognize the CD remote and fumble with it for a bit before falling to the floor.\\n\\n\\n\\nI begin to talk. At the time it made perfect sense to me, but after a minute of talking, I realized I was just saying gibberish. I shake my head and try to think of what I was saying but then a new phrase pops into my head: wagons. A fraction of a second later I add to words Flowers for Cup, so the phrase became Flowers for Cupwagons. What this means I have no idea. This is all I can remember form the salvia portion of the trip.\\n\\n\\n\\nPerhaps the phrase was induced by a song (Vangelis Messages) I was listening to, and although no lyrics form the phrase, the rhythmic patterns of the song could have formed the words in my head. \\n\\n\\n\\nAs I type this entry almost four months later than the experience, that phrase has crossed through my mind many of times, for I feel it is a connection to my subconscious. I will discover the true meaning of this phrase and the link I share with it and salvia. Salvia Divinorum is the weirdest psychedelic I have tried to date, and I feel that there is still much to be learned.\\n\\n\\n\\n7/20/2003\\n\\n\\n\\nOn a day off from camp, I sit on the deck with my brother smoking a bowl of nugs. We decide to smoke a small bowl of salvia each. He hits first and sits in silence as I inhale. The mindshift occurs and my own body becomes a mountain range shaped like an upright backwards C. With every word I speak, invisible energies shoot from the top of the C to the bottom, and I imagine what it would be like if I was listening to music, particularly Vangelis Messages (the song I feel that I share a transcendental connection with). \\n\\n\\n\\nI turn to my brother and start talking. I am talking nonsense but the words make sense. The Friday wagons are coming, B. You know they areyou know they are. He laughs and mentions how fucked up he feels and that he doesnt know what the hell is going on; internal as well as external. \\n\\n\\n\\nThe next day, I realize that the word wagons is somehow connected with salvia and myself. Friday and Flowers are just two beginnings for the phrase.\\n\\n\\n\\nTyped on the same day as the previous entry. It feels a bit unfair to the journal and myself to be sharing these entries having months to think about the experiences. However, I share them in better understanding this strange phrase. More salvia is needed and perhaps a new prefix will surface for the wagons phrase. I must make sure that it truly surfaces and is not just forced into existence by my desire to discover.\\n\\n\\n\\nI am starting a new journey this year. I have assimilated most of my experiences from the previous year and am ready to delve deeper into myself. I know who I am. Now I must go beyond knowing. I will discover what that is this year. \\n\\n\\n\\n9/1/03  12/29/03\\n\\n\\n\\nAs I have gotten lazy, I did not record the salvia experiences this year after they occurred. However, numerous trips were taken and were quite similar since the dosage and source did not differ.\\n\\n\\n\\nI smoked salvia about eight times. Each trip took place in my chair that although not aesthetically pleasing, serves its purpose well. They were fun but nothing spectacular. I learned that flowers for cupwagons means nothing and that alcohol and salvia do not mix. This semester sucked ass. I am tired. I am depressed.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example trip report\n",
    "pd_tripReports.loc[19922, \"report\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Cleaning and Exploratory Data Analysis (EDA)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Machine Learning, and the Natural Language Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The NLP Pipeline](./images/infographic_nlp-pipeline.png)\n",
    "[`Infographic Source`](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a collection of documents, i.e.: transcendent experience reports, scraped from the web, we can continue with the remainder of our analysis. Nautural Language Processing (NLP) is a subfield of machine learning, and deals primarily with unstructured textual data. The above graphic outlines the high level standard workflow for an NLP project, which includes:\n",
    "- `Text Pre-processing`: since the web is messy and unpredictable, we need to clean our data appropriately and perform some pre-processing to prepare our data for subsequent analysis\n",
    "- `Text Parsing & Exploratory Data Analysis`: what structures exist in the text, such as parts of speech or named entities (e.g.:  `TOM` can be an organization (`ORG`) or a person (`PER`) depending on context) - text parsing helps us construct better understand underlying structures of the text; exploratory data analysis (EDA) is the process of playing and understanding the data. It's as if the data is a person, and you're asking it questions over coffee to get to know them better. No way of conversation is particularly better than another, and data scientists can rely on tried and true techniques or get creative.\n",
    "- `Text Representation & Feature Engineering`: as mentioned in section 2.4, the chosen _representation_ of the data, as well as the features extracted from the raw data, are extremely important. Just as a landscape artist needs to choose the angle and form of the mountain he is painting, selecting the scenes and objects (features) to include to represent spring time, a data scientist needs to choose the methods of representing the text, and select features to highlight. More frequent than not, a \"feature\" in natural language processing is a vector of integeters or real numbers (e.g.: counts of words per document). \n",
    "\n",
    "- `Modeling and / or Pattern Mining`: depending on the task at hand and data (i.e.: metadata, labels etc.) available to us, we can use supervised or unsupervised techniques, topic modelling techniques, clustering to find similar groups, dimensionality reduction to visualize the data. The remainder of the thesis touch upon the above techniques, with particular emphasis on topic modelling.\n",
    "\n",
    "- `Evaluation & Deployment`: Once we have a model, we need to have a way to understand how well it performs. That is the topic of Evaluation, a crucial part of the natural language processing and machine learning pipeline. Without sound evaluation metrics, we have created a model, not a _good_ model. Indeed, unsupervised learning techniques are typically hard to evaluate, and in this thesis we propose a few metics for topic modelling, and note that improving evaluation metrics is a further area of research.\n",
    "\n",
    "\n",
    "Note that the workflow above has been drawn linearly, but the NLP pipeline in reality has many loops ‚Äî extracting features, finding patterns, and asking questions about the data do not occur in a vaccuum. We present our work below linearly for structure, but the process of getting to know the data and writing this thesis was far from linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Text Preprocessing\n",
    "\n",
    "\n",
    "### 4.2.1 Overview of Text Preprocessing\n",
    "\n",
    "Text Preprocessing refers to a broad class of techniques that pre-processes the text before any analysis is done. It's analogous to a rough sieve filtering out the largest rocks, cleaning and normalizing the corpus on a high level. Note that there is no \"canonical process\" to perform text pre-processing. In practice, the types of preprocessing done depends on the type of data, the quantity of data, trial and error. Steps marked with `*` are those that are most frequently used.\n",
    "\n",
    "- `* Remove extra spaces`\n",
    "- `* Make all text lowercase`\n",
    "- `* Remove stopwords`\n",
    "- `* Tokenization`\n",
    "- `* Normalize accented characters`\n",
    "- `Remove rougue `html`/`xml` tags`\n",
    "- `Remove numbers`\n",
    "- `Expanding contractions`\n",
    "- `Stemming`\n",
    "- `Lemmatization`\n",
    "\n",
    "For those interested, here is some more detail on each step:\n",
    "- `* Remove extra spaces`: \n",
    "> a whitespace is not just a whitespace in NLP; they can be `space` or `\\tab`, and depending on the parse, one needs to deal with newline characters such as `\\n` or `\\r`. Stripping away unecessary whitespace to find word boundaries is a crucial step in pre-processing. This is most commonly done with `str.strip()`\n",
    "- `* Make all text lowercase`:\n",
    "> should `wow` be treated the same as `WOW`? In sentiment analysis tasks, capitalizations matter very much, as the presence of caps often connote delight. In other tasks that are more informational based, capitalization add noise and unwanted dimensionality. This is most commonly done with  `str.lower()`\n",
    "- `* Remove stopwords`: \n",
    ">words such as `a` and `the` and `and` and `is` occur so frequently in the english language that they often don't add any meaningful information to the task at hand, so we pre-empt noise by adding them to a `stopwords` list and remove them from our corpus. Care must be taken with stopwords as by adding a word to a stopwords list we are a priori claiming that word does not add value. Indeed, men tend to use `the` more than women, so a classifier that predicts gender would be well served to include the presence or counts of `the` as a feature (Lyle Ungar).\n",
    "- `* Tokenization`: \n",
    ">The process of dividing a big chunk of text into individual units. In english, this is fairly straightforward: use individual words as tokens, or individual characters if we're building a character-level model. We also have to consider whether to make puntuations tokens, and whether we include any ngrams, e.g.: a bigram such as `cat jumped` or trigram such as `you are beautiful` separately as tokens, in addition to the individual words `cat`, `jumped`, `beautiful` etc. Tokenization is considerably harder for languages that don't have spaces, such as Chinese. In practice, we use built in tokenizers such as `nltk` or `spaCy`\n",
    "- `* Normalize accented characters`:\n",
    ">e.g.: `√°` to `a`. Useful for english, but for other languages accented characters may have special meaning and usage. This is most commonly done with `unicodedata.normalize` and built in `sklearn` functionality\n",
    "- `Remove rougue `html`/`xml` tags`:\n",
    "> websites are written in `html`(`xml`), `css`, and `js`; If a website is a human being, `html` is the skeleton and meat, `css` is the makeup, and `js` provides the instruction for interactivity. As we know, sometimes bones break, but the human is still functioning ‚Äî likewise, an `html` or `xml` file might be slightly corrupted, but the browser will still render the webpage. After we've scraped the webpage, we remove those `html` tags that are extraneous, so all we are left is the meat. This is most commonly done with packages such as `Beautiful Soup`\n",
    "- `Remove numbers`:\n",
    "> If we were designing a system to answer questions like `what is the population of norway`, numbers in the original data set is crucial. Other times, number add only noise. Depending on the problem at hand, the data scientist makes an informed decision of whether to remove numbers This is most commonly done with `regex`, or regular expressions, patterns that \"match\" to types of text. For example, the `regex` that matche to a 1 or more numbers is `[0-9]+`\n",
    "- `Expanding contractions`:\n",
    "> e.g.: `don't` to `do not`. Once again, this is optional and depends on the task and availability of data. Indeed, Lyle Ungar remarks this has been highly optional in his work.\n",
    "- `* Remove special characters`:\n",
    "> do special characters such as `<`, `!`, `#` add value? Indeed, sentiment analysis algorithms often look for `!`, and if we were analyzing twitter data, `#` are important. In other cases, special characters are treated as gibberish and removed.\n",
    "- `Stemming`:\n",
    "> TODO\n",
    "- `Lemmatization`: \n",
    "> TODO\n",
    "![Stemming Example](./images/infographic_stemming-inflections.png)\n",
    "[`Stemming Graphics Source`](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)\n",
    "\n",
    "- Save cleaned text to disk: e.g.: `news_df.to_csv('news.csv', index=False, encoding='utf-8')`; OR use pickle dump\n",
    "\n",
    "\n",
    "Text preprossing procedures draws inspiration from [here](https://towardsdatascience.com/creating-word-clouds-with-python-f2077c8de5cc) and [A Practitioner's Guide to Natural Language Processing (Part I) ‚Äî Processing & Understanding Text](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72), as well as Professor `Chris Callison-Burch's` [Computational Liguistics Class](http://computational-linguistics-class.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 The Details of Text Pre-processing\n",
    "\n",
    "TODO: EDA before data cleaning to see how dirty the data is  \n",
    "TODO: Move data cleaning to this step, before we even go into word clouds!!  \n",
    "TODO: Implement the following that have not been implemented yet!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.1 Getting the data into the right format: R and Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>title</th>\n",
       "      <th>substance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After having had some success with other forms...</td>\n",
       "      <td>Sideways World</td>\n",
       "      <td>Salvia divinorum (5x extract)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me and a couple of my buddies decided one nigh...</td>\n",
       "      <td>Physical Wellbeing = Crucial</td>\n",
       "      <td>Mushrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My girlfriend and I had been saving the methyl...</td>\n",
       "      <td>The Artful Dodger</td>\n",
       "      <td>Methylone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just want to warn anybody taking Lithium (or...</td>\n",
       "      <td>Seizure Inducing Combo</td>\n",
       "      <td>LSD &amp; Lithium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have had several attempts before this to bre...</td>\n",
       "      <td>Enlightenment Through a Chemical Catalyst</td>\n",
       "      <td>5-MeO-DMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              report  \\\n",
       "0  After having had some success with other forms...   \n",
       "1  Me and a couple of my buddies decided one nigh...   \n",
       "2  My girlfriend and I had been saving the methyl...   \n",
       "3  I just want to warn anybody taking Lithium (or...   \n",
       "4  I have had several attempts before this to bre...   \n",
       "\n",
       "                                       title                      substance  \n",
       "0                             Sideways World  Salvia divinorum (5x extract)  \n",
       "1               Physical Wellbeing = Crucial                      Mushrooms  \n",
       "2                          The Artful Dodger                      Methylone  \n",
       "3                     Seizure Inducing Combo                  LSD & Lithium  \n",
       "4  Enlightenment Through a Chemical Catalyst                      5-MeO-DMT  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Getting to know the dataframe + pandas\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.2 Text Pre-processing pipeline\n",
    "\n",
    "#### 4.2.2.2 | Prepare Stopwords\n",
    "\n",
    "The removal of stopwords (typically commonly occuring, low information content words) in natural language processing is in itself an art. Both automatic detection of stopwords (Wilbur & Sirotkin, 1992) and dictionary approaches have been widely used. For example, `nltk` has 179 stopwords , `wordcloud` package has 192 stopwords,`sklearn` has 318 stopwords, with 116 words shared between all three lists. Some examples of these shared stopwords are 'how', 'itself', 'an', 'ours', 'who', 'had', 'once', 'before', 'yours', and 'the', which are among the most common words in the English language.\n",
    "\n",
    "Here we (1) use `nltk` and `wordcloud` stopwords as a baseline (exclude `sklearn` stopwords because of [known issues](https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words) and (2) add all alternative spellings of drugs observed from EDA to our stopwords list; Note that the latter is an optional, concious design decision: we can leave the drug names be (in which case they would show up disproportionately in the word clouds), or we can remove all drug names from the reports and focus on non drug names that correlate with particular reports. In total, our psychedelic drug corpus has 2783 unique spellings / mispellings of drug names. In total, our stopword list consisits of 2999 unique stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'itself', 'an', 'ours', 'who', 'had', 'once', 'before', 'yours', 'am', 'some', 'no', 'be', 'but', 'her', 'after', 'your', 'then', 'for', 'his']\n"
     ]
    }
   ],
   "source": [
    "# Number of stopwords for each dictionary\n",
    "from sklearn.feature_extraction import stop_words\n",
    "len(stop_words.ENGLISH_STOP_WORDS) #sklearn\n",
    "len(nltk_stopwords.words('english')) #nltk\n",
    "len(wordcloud_STOPWORDS) #wordcloud\n",
    "len(substances_all_alternative_spellings) #all substance spellings\n",
    "\n",
    "# overlap between sklearn, nltk, and wordcloud stopwords\n",
    "overlapped_stopwords = []\n",
    "overlap_stopwords = [word for word in stop_words.ENGLISH_STOP_WORDS if word in nltk_stopwords.words('english') and word in wordcloud_STOPWORDS]\n",
    "print(overlap[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve all alternative spellings for substances\n",
    "with open('substances_unique') as f:\n",
    "    substances_all_alternative_spellings = f.readlines()\n",
    "substances_all_alternative_spellings = [spelling.strip().replace(\"'\", \"\") for spelling in substances_all_alternative_spellings]\n",
    "substances_all_alternative_spellings[:30]\n",
    "\n",
    "## prepare stopwords\n",
    "stopword_list = []\n",
    "stopwords_nltk = nltk_stopwords.words('english')\n",
    "stopwords_wordcloud = list(wordcloud_STOPWORDS)\n",
    "stopword_list.extend(stopwords_nltk + stopwords_wordcloud + substances_all_alternative_spellings)\n",
    "# deduplicate list\n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "\n",
    "# How many unique stopwords in total do we have?\n",
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7e1a2ea6f571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# TODO: Adapt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# TODO: Vectorize for pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordPunctTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# Code from: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "# TODO: Adapt\n",
    "# TODO: Vectorize for pandas dataframe\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Stemming is super easy!\n",
    "# From: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Stemming is super easy!\n",
    "# From: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.2 | Cleaning and Tokenizing the Text\n",
    "\n",
    "To improve performance of wordclouds and subsequent processing of the reports, we use `nltk` to tokenize the text; Note that other tokenization methods may be tested for performance comparison\n",
    "\n",
    "Overview:\n",
    "- Use `nltk` for tokenization\n",
    "- Use `regex` to remove numbers\n",
    "- Use `unicodedata.normalize` to normalize accented characters\n",
    "- Make all text lowercase\n",
    "- NOTE: only unigrams considered at this stage; ngram analysis down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Remove accented characters and normalise using the unicodedata library\n",
    "# from https://towardsdatascience.com/creating-word-clouds-with-python-f2077c8de5cc\\\n",
    "# TODO\n",
    "unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Long Run Time\n",
    "\n",
    "# Tokenize the text\n",
    "reports_text_tokenized = {}\n",
    "\n",
    "# reports_text is {substance: text} mapping\n",
    "# NOTE: This code takes an extremely long time to run\n",
    "for substance in reports_text:\n",
    "    substance_text = reports_text[substance]\n",
    "    tokens = nltk.word_tokenize(substance_text)    \n",
    "    tokens = [token.strip().lower() for token in tokens]\n",
    "    substance_tokenized_text = ' '.join([token for token in tokens if token not in stopword_list])\n",
    "    \n",
    "    reports_text_tokenized[substance] = substance_tokenized_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the tokenization and removal of stopwords takes a substantial amount of time to run, we store a serialized version of the object so we may retrive it later at a substantially faster rate; see [pickle tutorial](https://www.journaldev.com/15638/python-pickle-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "## Use this cell as a template to save and load large data files; current code saves and loads `reports_text_tokenized`\n",
    "\n",
    "import pickle\n",
    "\n",
    "### Stores the tokenized reports:\n",
    "# with open('reports_tokenized_map', 'wb') as out_file:\n",
    "#     pickle.dump(reports_text_tokenized, out_file)\n",
    "    \n",
    "### Loads the tokenized reports:\n",
    "with open('reports_tokenized_map', 'rb') as in_file:\n",
    "    reports_text_tokenized = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4.2.3 Insights from Text Pre-processing\n",
    "\n",
    "TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Data Wrangling and Exploratory Data Analysis (EDA)\n",
    "\n",
    "TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Stemming is super easy!\n",
    "# From: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Treatment of `Substances`\n",
    "\n",
    "TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Facts about the Data\n",
    "\n",
    "TODO\n",
    "\n",
    "- average length of trip reports, character level + word level\n",
    "- mistribution of the length of trip reports\n",
    "- most frequently co-consumed substances\n",
    "- distribution of the number of labels for different reports\n",
    "- distribution of the number of labels for each substance: e.g.: are reports with `cannabis` frequently not just cannabis? normalize using proportion of trip reports;\n",
    "- think more critically about treatment of longtail; do we simply label substances not within the top 20 substances as `<UNK>`? If so, how do we treat them in bayesian methods? My intuition is telling me that I should include all of them in trip report generation, but not in classification tasks;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Addressing Substance co-occurence\n",
    "\n",
    "TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Text Parsing: Language Syntax and Structure\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "[Referece](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72)\n",
    "- POS tagging: What are the top nouns, verbs, adjectives etc. associated with each substance?\n",
    "- Shallow parsing or chunking\n",
    "- Constituency Parsing\n",
    "- Dependency Parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Text Parsing: Named Entity Recognition\n",
    "\n",
    "TODO\n",
    "\n",
    "- TODO: This can absolutely be part of EDA!!\n",
    "\n",
    "![Named Entity Recognition Example](./images/infographic_NER.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.6 Text Parsing: Sentiment Analysis\n",
    "- TODO: Show distribution of sentiment across all trip reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.7 Treatment of the Longtail\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.4 The Data: Clean and Filtered\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the ability to use R in a jupyter notebook\n",
    "# https://stackoverflow.com/questions/39008069/r-and-python-in-one-jupyter-notebook\n",
    "# To use R, add %%R to the beginning of a cell, before any code and any comments\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: pacman\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# Installs the proper R packages\n",
    "if(!require('pacman')) {\n",
    "  install.packages('pacman', repos = \"http://cran.us.r-project.org\")\n",
    "}\n",
    "pacman::p_load(dplyr, leaps, car, tidyverse, GGally, reshape2, data.table, ggcorrplot, bestglm, glmnet, mapproj, pROC, data.tale, tm, SnowballC, wordcloud, RColorBrewer, reshape2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 19924\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## Read in trip reports\n",
    "tripReports <- fread(\"trips.csv\")\n",
    "tripReports.df <- as.data.frame(tripReports)\n",
    "# glimpse(tripReports)\n",
    "\n",
    "nrow(tripReports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Examine the substances present in the dataset\n",
    "tripReports %>% select(substance) %>% write.table(file = \"substances-all\", append=FALSE, col.names = FALSE, row.names = FALSE, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 19,924\n",
      "Variables: 24\n",
      "$ report                    \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"After having had some success with other f‚Ä¶\n",
      "$ title                     \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"Sideways World\", \"Physical Wellbeing = Cru‚Ä¶\n",
      "$ substance                 \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"salvia divinorum (5x extract)\", \"mushrooms‚Ä¶\n",
      "$ substance.mushrooms       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0‚Ä¶\n",
      "$ substance.lsd             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.mescaline       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.cannabis        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0‚Ä¶\n",
      "$ substance.mdma            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ayahuasca       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.nitrous_oxide   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.salvia          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.methamphetamine \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0‚Ä¶\n",
      "$ substance.dmt             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.5_meo_dmt       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.alcohol         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ketamine        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ibogaine        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.pcp             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.kava            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.kratom          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0‚Ä¶\n",
      "$ substance.morning_glory   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.syrian_rue      \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.unknown         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.UNK             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## Manipulate the dataframe to have 1 hot encoding for substances\n",
    "tripReports$substance <- tolower(tripReports$substance)\n",
    "tripReports <- as.data.frame(tripReports)\n",
    "substances.of.interest <- c(\"substance.mushrooms\", \"substance.lsd\", \"substance.mescaline\", \"substance.cannabis\", \"substance.mdma\", \"substance.ayahuasca\", \"substance.nitrous_oxide\", \"substance.salvia\", \"substance.methamphetamine\", \"substance.dmt\", \"substance.5_meo_dmt\", \"substance.alcohol\", \"substance.ketamine\", \"substance.ibogaine\", \"substance.pcp\", \"substance.kava\", \"substance.kratom\", \"substance.morning_glory\", \"substance.syrian_rue\", \"substance.unknown\", \"substance.UNK\")\n",
    "\n",
    "for (sub in substances.of.interest) {\n",
    "  tripReports[sub] = 0\n",
    "}\n",
    "\n",
    "### Adding one hot encodings to the substances\n",
    "tripReports$substance.mushrooms[grepl(\"mushroom|mushhrooms|mushooms|psilocin|psilocybin|psilocybe\", tripReports$substance)] <- 1\n",
    "tripReports$substance.lsd[grepl(\"lysergic acid|lsd\", tripReports$substance)] <- 1\n",
    "tripReports$substance.mescaline[grepl(\"mescaline|peyote\", tripReports$substance)] <- 1\n",
    "tripReports$substance.cannabis[grepl(\"cannabis|canabis|cannabbis|cannabinoid|cannabus|cannibis|cannibus| thc \", tripReports$substance)] <- 1\n",
    "tripReports$substance.mdma[grepl(\"mdma|ecstacy|ecstasy|ectasy|molly\", tripReports$substance)] <- 1\n",
    "tripReports$substance.ayahuasca[grepl(\"ayahuasca|ayahausca|ayahusca|p. viridis|p.viridis|b.caapi|b. caapi|cappi|viridis\", tripReports$substance)] <- 1\n",
    "tripReports$substance.nitrous_oxide[grepl(\"nitric|nitrites|nitrogen|nitrous|whippets\", tripReports$substance)] <- 1\n",
    "tripReports$substance.salvia[grepl(\"salia|saliva|salivia|sally|salva|salvia|salvinorin\", tripReports$substance)] <- 1\n",
    "tripReports$substance.methamphetamine[grepl(\"met|meth|methampetamine|methamphetamine|speed\", tripReports$substance)] <- 1\n",
    "# dmt\n",
    "tripReports$substance.dmt[grepl(\"nn-dmt\", tripReports$substance)] <- 1\n",
    "tripReports$substance.dmt[tripReports$substance == \"dmt\"] <- 1 # cannot just grep dmt otherwise this will include 5-meo-dmt\n",
    "#5-meo-dmt\n",
    "tripReports$substance.5_meo_dmt[grepl(\"5 meo-dmt|5-meo dmt|5-meo-dmt|5meo-dmt\", tripReports$substance)] <- 1\n",
    "tripReports$substance.alcohol[grepl(\"alchohol|alcohol\", tripReports$substance)] <- 1\n",
    "tripReports$substance.ketamine[grepl(\"ketamin|ketamine\", tripReports$substance)] <- 1\n",
    "tripReports$substance.ibogaine[grepl(\"iboga|ibogaine\", tripReports$substance)] <- 1\n",
    "tripReports$substance.pcp[grepl(\"pcp\", tripReports$substance)] <- 1\n",
    "tripReports$substance.kava[grepl(\"kava\", tripReports$substance)] <- 1\n",
    "tripReports$substance.kratom[grepl(\"kratom\", tripReports$substance)] <- 1\n",
    "tripReports$substance.morning_glory[grepl(\"glory|glories\", tripReports$substance)] <- 1\n",
    "tripReports$substance.syrian_rue[grepl(\"syrian rue|rue\", tripReports$substance)] <- 1\n",
    "tripReports$substance.unknown[grepl(\"unknown|unidentified|unkown\", tripReports$substance)] <- 1\n",
    "\n",
    "glimpse(tripReports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   substance count\n",
      "1         substance.cannabis  3110\n",
      "2        substance.mushrooms  1686\n",
      "3           substance.salvia  1556\n",
      "4             substance.mdma  1188\n",
      "5              substance.lsd  1131\n",
      "6  substance.methamphetamine   929\n",
      "7          substance.alcohol   928\n",
      "8    substance.morning_glory   427\n",
      "9    substance.nitrous_oxide   300\n",
      "10       substance.5_meo_dmt   297\n",
      "11      substance.syrian_rue   293\n",
      "12        substance.ketamine   289\n",
      "13          substance.kratom   207\n",
      "14       substance.ayahuasca   172\n",
      "15             substance.dmt   167\n",
      "16            substance.kava   167\n",
      "17             substance.pcp    81\n",
      "18       substance.mescaline    73\n",
      "19         substance.unknown    47\n",
      "20        substance.ibogaine    43\n",
      "21             substance.UNK     0\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## Number of reports containing each substance: NOTE: a report might have more than one substance\n",
    "tripSubstances <- tripReports %>% select(-report, -title, -substance)\n",
    "tripReportsCount <- data.frame(substance = names(tripSubstances), count = colSums(tripSubstances))\n",
    "tripReportsCountSorted <- tripReportsCount %>% arrange(desc(count))\n",
    "tripReportsCountSorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 19,924\n",
      "Variables: 25\n",
      "$ report                    \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"After having had some success with other f‚Ä¶\n",
      "$ title                     \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"Sideways World\", \"Physical Wellbeing = Cru‚Ä¶\n",
      "$ substance                 \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"salvia divinorum (5x extract)\", \"mushrooms‚Ä¶\n",
      "$ substance.mushrooms       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0‚Ä¶\n",
      "$ substance.lsd             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.mescaline       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.cannabis        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0‚Ä¶\n",
      "$ substance.mdma            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ayahuasca       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.nitrous_oxide   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.salvia          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.methamphetamine \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0‚Ä¶\n",
      "$ substance.dmt             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.5_meo_dmt       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.alcohol         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ketamine        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.ibogaine        \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.pcp             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.kava            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.kratom          \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0‚Ä¶\n",
      "$ substance.morning_glory   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.syrian_rue      \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.unknown         \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.UNK             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n",
      "$ substance.unique_label    \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"substance.salvia\", \"substance.mushrooms\", ‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## Find reports with only one unique substance\n",
    "\n",
    "tripReports$substance.unique_label <- \"NA\"\n",
    "# glimpse(tripReports)\n",
    "uniqueSubstanceRows <- tripReports %>% select(-report, -title, -substance, -substance.unique_label) %>% rowSums() == 1\n",
    "\n",
    "for (row in 1:nrow(tripReports)) {\n",
    "  # this row contains a unique substance\n",
    "  if (uniqueSubstanceRows[row]) {\n",
    "    for (substance in substances.of.interest) {\n",
    "      if (tripReports[row, substance] == 1) {\n",
    "        tripReports[row, ]$substance.unique_label<- substance\n",
    "      } \n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "glimpse(tripReports)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From cffi callback <function _processevents at 0x10cb41dd0>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alextzhao/opt/anaconda3/lib/python3.7/site-packages/rpy2/rinterface_lib/callbacks.py\", line 262, in _processevents\n",
      "    @ffi_proxy.callback(ffi_proxy._processevents_def,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "# TEMP, SAVE: Use this code to save the encoded table for future use\n",
    "save(tripReports, file=\"tripReportsEncoded.Rda\")\n",
    "write.table(tripReports, file=\"tripReportsEncodedTable\", sep = \";;\", row.names = TRUE, col.names = TRUE )\n",
    "write.csv(tripReports, file=\"tripReportsEncoded.csv\", sep = \",\", row.names = TRUE, col.names = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 21\n",
      "Variables: 2\n",
      "$ substance \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \"NA\", \"substance.cannabis\", \"substance.salvia\", \"substance.‚Ä¶\n",
      "$ count     \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m 11569, 1609, 1270, 1094, 747, 746, 696, 409, 328, 250, 171,‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "#### Create a data frame that combines the count for reports containing a substance and reports that are uniquely a single substance\n",
    "tripReportsUniqueSubstanceCountSorted <- tripReports %>% group_by(substance.unique_label) %>% summarise(count = n()) %>% arrange(desc(count))\n",
    "# glimpse(tripReportsCountSorted)\n",
    "colnames(tripReportsUniqueSubstanceCountSorted) = c(\"substance\", \"count\")\n",
    "tripReportsUniqueSubstanceCountSorted %>% glimpse()\n",
    "\n",
    "tripReportsCount_merged <- merge(tripReportsCountSorted, tripReportsUniqueSubstanceCountSorted, by=\"substance\")\n",
    "colnames(tripReportsCount_merged) = c(\"substance\", \"n_includes_substance\", \"n_single_substance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   substance n_includes_substance n_single_substance\n",
      "1         substance.cannabis                 3110               1609\n",
      "2        substance.mushrooms                 1686               1094\n",
      "3           substance.salvia                 1556               1270\n",
      "4             substance.mdma                 1188                747\n",
      "5              substance.lsd                 1131                696\n",
      "6  substance.methamphetamine                  929                746\n",
      "7          substance.alcohol                  928                409\n",
      "8    substance.morning_glory                  427                328\n",
      "9    substance.nitrous_oxide                  300                155\n",
      "10       substance.5_meo_dmt                  297                250\n",
      "11      substance.syrian_rue                  293                164\n",
      "12        substance.ketamine                  289                170\n",
      "13          substance.kratom                  207                171\n",
      "14       substance.ayahuasca                  172                114\n",
      "15             substance.dmt                  167                167\n",
      "16            substance.kava                  167                131\n",
      "17             substance.pcp                   81                 38\n",
      "18       substance.mescaline                   73                 44\n",
      "19         substance.unknown                   47                 13\n",
      "20        substance.ibogaine                   43                 39\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "# EXPORT: Table of selected substances and their counts\n",
    "tripReportsCount_merged %>% arrange(desc(n_includes_substance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using substance as id variables\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## This data frame that encodes how many trip reports contains a substance and how many reports are uniquely such a substance'\n",
    "tripReportsCount_merged.long <- reshape2::melt(tripReportsCount_merged)\n",
    "# EXPORT: Graph of distribution of substances\n",
    "tripReportsCount_merged.long %>% ggplot( aes(x = reorder(substance, -value), y = value, fill=variable)) + geom_bar(stat=\"identity\", position = \"dodge\") + theme(axis.text.x = element_text(angle = 67.5, hjust = 1)) + ggtitle(\"Count of Substances Present in Trip Reports\") + xlab(\"substance\") + ylab(\"count\") + labs(fill = \"Number of Reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 5. Text Presentation and Feature Engineering\n",
    "\n",
    "- TODO: Bag of words is SO easy with `sklearn` and `pandas`(mostly for display purposes)  \n",
    "- TODO: Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Word as Vectors\n",
    "\n",
    "`TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Documents as Vectors\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 5.3 Key Vectorization Techniques: An Overview\n",
    "\n",
    "`TODO`\n",
    "\n",
    "\n",
    "## 5.4 Bag of Word (BOW) Features\n",
    "\n",
    "`TODO`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Term-Document Counts: The Very Basics\n",
    "\n",
    "`TODO`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Example BOW's Code\n",
    "# Code from: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "\n",
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)\n",
    "\n",
    "############################################\n",
    "## Ngram model extensions\n",
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "bv = CountVectorizer(ngram_range=(2,2))\n",
    "bv_matrix = bv.fit_transform(norm_corpus)\n",
    "\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab = bv.get_feature_names()\n",
    "pd.DataFrame(bv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 TF-IDF Weighting Scheme\n",
    "\n",
    "`TODO`\n",
    "\n",
    "\n",
    "- TODO: We can use this for clustering and obtain the \"cononical\" trip report for different categories\n",
    "- Cosine similarity\n",
    "- TODO: Other similarity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-05f8d2d3b287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtv_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtv_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtv_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'norm_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Sample TFIDF Code from: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.4.3 Document Similarity matrix: Bag of Words\n",
    "\n",
    "`TODO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sample code from https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Latent Dirichlet Allocation Embeddings\n",
    "\n",
    "`TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6: Modern Techniques: Contextual Embeddings\n",
    "\n",
    "`TODO`\n",
    "\n",
    "### 5.6.1 Word2Vec: Negative Skip Gram Sampling\n",
    "\n",
    "`TODO`\n",
    "\n",
    "### 5.6.2 Doc2Vec:\n",
    "\n",
    "`TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3: Deep Learning Techniques: State of the Art\n",
    "\n",
    "`TODO`\n",
    "- BERT\n",
    "- GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.4: Beyond Feature Engineering\n",
    "`TODO`\n",
    "\n",
    "- Autoencoders\n",
    "- Automatic Feature Selection\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> \n",
    "<br /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Topic Modelilng: Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "`TODO`\n",
    "\n",
    "- Topic modelling with `Latent Dirichlet Allocation (LDA)`; see [`sklearn example`](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py) which uses `LDA` and `Non-negative matrix factorization (NMF)`; Also see [`LDA Documentation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation); Also see `gensim` package\n",
    "- As per conversation with Lyle Ungar, `LDA` can also give me document and word embeddings which can be used in wordclouds.\n",
    "- [Excellent Talk by Christine Doig on Topic Models](http://chdoig.github.io/pygotham-topic-modeling/#/)\n",
    "- [Blei 2012: Probabilistic Topic Models](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf); [local-link](./papers/Blei2012_Probabilistic-Topic-Models.pdf)\n",
    "- TODO: Latent Semantic Indexing (singular value decomposition)\n",
    "- TODO: Non-negative Matrix factorization (NMF)\n",
    "- LDA gives us two things:\n",
    "    - A document-topic matrix, which can be used as an input to wordcloud\n",
    "    - A topic-term matrix, which helps us in looking at potential topics in the corpus.\n",
    "\n",
    "- **TODO**: Explain how `Latent Dirichlet Allocation (LDA)` works; see Blei 2012 and Christine Doig's talk.\n",
    "- TODO: Investigate how to choose the number of topics, which apparently is an art in itself\n",
    "- LDA Visualizations:\n",
    "    - [LDAvis R Package](https://github.com/cpsievert/LDAvis)\n",
    "    - [LDAvis port Python Package](https://github.com/bmabey/pyLDAvis)\n",
    "- [Example LDA visualization Jupyter Notebook](https://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb#topic=0&lambda=1&term=)\n",
    "- [In depth PyLDAvis python tutorial](https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb)\n",
    "\n",
    "![LDA Pipeline](./images/infographic_lda-pipeline-chdoig.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Latent Dirichlet Allocation: An Introduction\n",
    "\n",
    "Latent Dirichlet Allocation sounds like a fancy name, and the inner workings are indeed fairly complex, so let us begin with what LDA is trying to do intuitivly: LDA attempts to (1) automatically discover topics (in an unsupervised manner) from a collection of documents, and (2) model each document in that collection as exhibiting multiple topics in diffrent proporitions (Blei 2012). Intuitively, one can imagine that a document such as a blog post, wikipedia article, or in our case, a psychedelic trip report, might consist of numerous `topics`; for example, a news article published in the New York Times might talk about extreme weather, deforestration, and climate change, when covering say, a hurricane. A psychedelic trip report might also have numerous topics, such as musings on time, experiences with nature, or feelings of bliss. So what *is* a `topic`? It should be noted that since LDA is an unsupervised technique, we don't require our input text documents have topic labels. Instead, LDA processes our collection of text documents, and extracts topics automatically in a unsupervised manner, where each `topic` is precisely a list of words that approximate that particular topic. For example, the following represents 4 topics extracted automatically from the Journal __Science__, where each topic is a list of words (Blei 2012):\n",
    "\n",
    "- `topic1`: human genome dna genetic genes sequence gene molecular sequencing map information genetics mapping project sequences\n",
    "- `topic2`: evolution evolutionary species organisms life origin biology groups phylogenetic living diversity group new two common\n",
    "- `topic3`: disease host bacteria diseases resistance bacterial new strains control infectious malaria parasite parasites united tuberculosis\n",
    "- `topic4`: computer models information data computers system network systems model parallel methods networks software new simulations\n",
    "\n",
    "Notice that topics discovered don't have \"names\" by themselves; by looking at the list of words associated with each topic above, it is reasonable to say `topic1` is approximately \"genetics\", `topic2` is approximately \"evoluation\", `topic 3` is approximately \"disease\", and `topic4` is approximately \"computers\"; in practice the labelling of topics can either by done by humans, or by taking the word that is most \"relavent\" to that topic. Indeed, the above word list are sorted from most relavent to least relavent for each topic. The notion of \"relavence\" has a precise definition, which we defer to Section 6.3.\n",
    "\n",
    "\n",
    "![LDA Topics Example from Blei 2012](./images/infographic_lda-topics-example-blei2012.png)\n",
    "\n",
    "For another beautiful graphic presented by Blei 2012, this graphic depicts the intuition that each document exhibit multiple topics graphically:\n",
    "\n",
    "![LDA Intution: Each Document has Multiple Topics](./images/infographic_lda-intuition-blei2012.png)\n",
    "\n",
    "Now, before we dive deeper into the inner workings of LDA, let's take a look at LDA topic modelling pipeline, lest we lose track of the bic picture:\n",
    "\n",
    "![LDA Pipeline](./images/infographic_lda-pipeline-chdoig.png)\n",
    "\n",
    "As we can see, we have the collection of $M$ documents (i.e.: trip reports from Erowid), each with some $N_j$ words, which was obtained via processes described in *Section 3: Data Aquisision*. In *Section 4: Data Cleaning and Exploratory Data Analysis (EDA)*, we processed our documents, performing tokenization, stopword removal, and other pre-processing techniques. In *Section 5: Text Representation and Feature Engineering*, we created vector representations of documents through a variety of methods, including the naive (but fairly effective) term-document counts, along with tf-idf weighting, both of which fall under \"Bag of Words\" methods. And now, in this section, we use LDA to create a topic model to capture the topics representd in our collection of trip reports. It should be noted that LDA is not the only topic model available to modern data scientists. Other methods such as Non-negative Matrix Fatorization (NMF) and Probabilistic Latent Semantic Analysis (pLSI), which we note here but do not dive deeply into to maintain thesis scope. For those familiar with singular value decomposition (SVD): both NMF, and pLSI relate deeply to SVD of the document-term matrix. Indeed, \"LDA can also be seen as a type of principal component analysis (PCA) for discrete data\" (Blei 2012).\n",
    "\n",
    "From blei2012: This is the distinguishing characteristic of latent Dirichlet allocation‚Äîall the documents in the collection share the same set of topics, but each docu- ment exhibits those topics in differ- ent proportion.\n",
    "\n",
    "Now, we are ready dive deeper into what LDA does. Let us begin with an infographic for intuition:\n",
    "\n",
    "\n",
    "![LDA Overview](./images/infographic_lda-overview-chdoig.png)\n",
    "\n",
    "## 6.2 The Input to LDA\n",
    "\n",
    "LDA takes as input a collection of text documents, which has been preprocessed and vectorized. Note that the format of these text documents isn't set in stone, but most commonly for LDA implementation we have a $D x m$ matrix, where each row represents a document $d$, represented by a $m$-dimensional document vector embedding. For example, below is the vectorized representation of the first 6 documents in our corpus:\n",
    "\n",
    "todo: insert picture of input\n",
    "\n",
    "\n",
    "## 6.3 The Output of LDA\n",
    "\n",
    "\n",
    "LDA gives us three things:\n",
    "- K topics, where each topic is a collection (cluster) of words; note that most of the time K is prespecified by the user. This is where expert knowledge of a particular field can come in very handy\n",
    "- A topic-term matrix (ttm), which specifies a word distribution for each topic k; in other words, since each topic is a collection of words, it makes sense that some words are more \"representative\" of some topic $k$ than other words. In this way, each topic has a distribution over words\n",
    "- A document-topic matrix (dtm), which specifies a topic distribution for each document $d$; in other words, since we can understand each document as having multiple topics, it makes sense that some topics are more saliently present in a document, i.e.: the main topics / main themes of a document. Since this document topic matrix is a vectorized representation of the doucments, it could also be used as an input to wordcloud, or as a feature for other machine learning algorithms, as briefly mentioned in Section 5.5.\n",
    "\n",
    "\n",
    "## 6.4 The Details of LDA\n",
    "\n",
    "For the reader interested in how LDA works, we present a more technical treatment of LDA here:\n",
    "\n",
    "Firstly, we note that the topic structure of our documents is *latent* (i.e.: hidden, unobserved). We *only* observe the documents and words themselves. \n",
    "\n",
    "As such, LDA attempts to infer the hidden topic structure from the observed documents. Formally, LDA tries to compute the posterior distribution, or conditional distribution of the hidden structure given the observed documents. In Blei's words: what is the hidden structure that likely generated the observed collection?\n",
    "\n",
    "![LDA Overview with Details](./images/infographic_lda-bringing-it-together-chdoig.png)\n",
    "\n",
    "To describe LDA formally, we introduce the following notation and (adpated) explanations from Blei 2012 to maintain consistency with standard notation:\n",
    "- $D$, an fixed integer, representing the number of documents in our corpus\n",
    "- $V$, a fixed integer, representing the vocaulary size, i.e.: the number of unique tokens in our corpus\n",
    "- Topics $\\beta_{1:K}$ (`K x 1` vector), where each `topic` $\\beta_k$ is a (discrete) distribution over the vocabulary. Recall that a `topic` $k$ is a list of words, some of which are more \"relavent\" than others for the given topic $k$; in this way, each topic $\\beta_k$ is a discrete distribution over the vocabulary.\n",
    "- Documents $\\theta_{1:D}$, where document $\\theta_d$ is a (discrete) topic distribution of the $d$-th document. $\\theta_{d, k}$ (`real number` $\\in [0, 1]$) represents the proportion of topic $k$ in the $d$-th document. Recall that if we have $K$ topics, each document is modelled as a mix of these K topics, each topic with some proportion between 0 and 1.\n",
    "- The topic assignments for the $d$-th document are $z_d$(`n x 1 vector`), where $z_{d,n}$ (`integer` $\\in [0, K]$) is the topic assignment for the $n$th word in document $d$; Recall that each document is thought of as a mixture of $K$ topics, each with some proportion between 0 and 1. One way to get this proportion is by conceptualizing each word $n$ in a document $d$ as having a topic $k$, and the proportion of words \"with\" topic $k$ is the proporition of topic $k$ in document $d$\n",
    "- The observed words for document $d$ are $w_d$, where $w_{d,n}$ is the $n$th word in document $d$\n",
    "\n",
    "The full joint probability distribution modelled by LDA is given by:\n",
    "\n",
    "$$ p( \\beta_{1:K}, \\theta_{1, D}, z_{1:D}, w_{1:D}) = \\prod_{i=1}^{K}p(\\beta_i)\\prod_{d=1}^D p(\\theta_d) \\left( \\prod_{n=1}^N p(z_{d, n} | \\theta_d)p(w_{d, n} | \\beta_{1:K}, z_{d, n}) \\right)$$\n",
    "\n",
    "\n",
    "Recall that LDA infers the hidden topic structure by modelling the conditional distribution of the hidden structure given the observed documents. Formally, LDA is modelling:\n",
    "\n",
    "$$p( \\beta_{1:K}, \\theta_{d, k}, z_{1:D} | w_{1:D}) = \\cfrac{p( \\beta_{1:K}, \\theta_{d, k}, z_{1:D}, w_{1:D})}{w_{1:D}}$$\n",
    "\n",
    "The numerator, or joint probability distribution over all the random variables, is straightforward to compute, but the denominator, obtained by margininalizing over all latent topic structures, is exponentially large and intractible to compute (Blei 2012). As such, LDA is approximately in two main ways, through sampling-based algorithms and variational algorithms (Blei 2012). A sampling-based algorithm constructs an approximation of the posterior, and repeatedly draws samples from this approximation so in the limit converges to the true posterior distribution, and is non-deterministic. The most popular sampling-based algorithm is Gibbs Sampling. Variational methods are deterministic algorithms. They pre-suppose that the posterior distribution is from a parametrized family of distributions (e.g.: a bell curve that's centered or off center, narrowerer or flatter, depending on the parameters of mean and variance). This makes variational methods an optimization problem as it tries to find the parameters that define a distribution closest to the posterior distribution. Here, closeness is measured by Kullback Liebler Divergence (KL-Divergence), which takes as input two distributions (order matters) and returns a real valued measure of closeness.\n",
    "\n",
    "Assumptions of LDA:\n",
    "- Bag of words assumption, that ordering of words in the document doesn't matter. We have seen two examples: term-document counts and tf-idf weighting.\n",
    "- Ordering of documents don't matter; this is fairly realistic, unless we are modelling changes in topic over time. This can be an interesting area of further research in a followup study, where we can compare trip reports of the 1960's with those of the 21st century, to examine if and how the topics present in trip reports have evolved. Indeed, one can conceive that the 1960's may be marked by hippie culture, and extending the analysis through the dimension of time may reveal cultural around psychedelic substances. See below for an example from Blei 2012 for a dynamic LDA model examining how the topics in the journal Science has evolved through time.\n",
    "- The number of topics K is assumed to be fixed. To account for chaning number of topics, one can turn to Bayesian non-parametric topic models, which is outside the scope of this thesis.\n",
    "\n",
    "Topic Model Extensions, and Future Research Areas:\n",
    "See Wallach, Griffiths et al. for models that relax the bag of words assumptions use words conditioned on previous words and LDA in conjunction with a hidden markov model (HMM), which lead to improved performance. (todo sources). Other types of topic models include (Blei 2012): correlated topic model and pachinko allocation machine, which account for correlations between topics (e.g.: machine learning and statistics may be more correlated than machine learning and penguins; the spherical topic model allows words to be unlikely in a topic (for example, \"coconuts\" is unlikely in documents about watches); sparse topic models and ‚Äúbursty‚Äù topic models respectively impose more constraints over topic distributions and more realistically model word counts (Blei 2012).\n",
    "\n",
    "For those familiar with plate models, the following graphic (Blei 2012) reminds us that LDA is a type of a broader class of algorithms: probabilistic graphical models. $D$ and $N$ represent multiplicity, i.e.: the referenced cells are repeated $D$ and $N$ times. $\\alpha$ and $\\eta$ are hyperparameters that specify the shape of two Dirichlet Distributions, used to describe the $\\theta_d$ (the topic distribution of document d) distributions and $\\beta_k$ (word distribution of topic k) distributions respectively.\n",
    "\n",
    "![Plate Model of LDA](./images/infographic_lda-plate-model-blei2012.png)\n",
    "\n",
    "\n",
    "## 6.5 Implementation of LDA on psychedelic trip reports\n",
    "\n",
    "In practice, LDA is implemented either from a Gibbs Sampling approach (e.g.: `mallet` package) or via Variational Methods (e.g.: `gensim`), and optimized using Expectation Maximization (EM) algorithms. We note without theoretical justification that Lyle Ungar has found empirically that `mallet` works better and provides more interpretable models (source: conversations). Luckily for us, packages such as `mallet` and `sklearn` are easy to use, and provide state of the art LDA implementations. We first use `sklearn` implementation of LDA, which uses an online variational Bayes algorithm, due to its ease of use and compatibility with LDA visualization packages suc as `pyLDAvis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "(11314, 9144)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alextzhao/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1616: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. <class 'numpy.int64'> 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 9144)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alextzhao/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el9851120818824486109621610\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el9851120818824486109621610_data = {\"mdsDat\": {\"x\": [0.19557575517173603, 0.12052608038322218, 0.299475296871869, 0.01713748581580441, 0.10404075598957423, 0.13945030831630997, -0.12075368805453596, -0.052732311340574545, -0.02977954856087365, -0.08740590703185008, -0.07485933816355882, -0.025622134645726168, -0.06400148612916663, -0.07299355844990724, -0.06839302990398284, -0.06432088208937828, -0.05834579550690911, -0.054621244703666895, -0.05363339144282387, -0.048743366525561854], \"y\": [0.043593329317624614, 0.13856802384131345, -0.13284966190001218, 0.18880672082044103, 0.18037164994340457, -0.14923302102700564, -0.01555392140208369, -0.030380650297066165, -0.07312977241643563, 0.005969741182290279, -0.028866314317100662, -0.05317365845587737, -0.011279041070266201, -0.01514193477494176, -0.004103371097681621, -0.01897370363363898, -0.009242867444480957, -0.008681369352735084, -0.00806763753329649, 0.0013674596175481165], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [34.043426794482436, 20.06689725904099, 12.540160654593526, 5.209774193801941, 4.859943295544359, 3.561402465246245, 2.472662320569808, 2.0448485446562104, 1.9826302065651336, 1.5049501288886955, 1.4085468630514746, 1.30486033908219, 1.2879101431391797, 1.2732339122246352, 1.262125336622128, 1.20176156243687, 1.0428095514767826, 0.9926426331088528, 0.9708067212148839, 0.9686070742536502]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\"], \"Freq\": [52.0, 82.0, 90.0, 112.0, 42.0, 79.0, 43.0, 50.0, 41.0, 51.0, 65.0, 28.0, 26.0, 25.0, 34.0, 62.0, 39.0, 59.0, 129.0, 38.0, 24.0, 71.0, 38.0, 30.0, 25.0, 69.0, 130.0, 40.0, 18.0, 114.0, 33.485263335757374, 23.311624530821923, 19.9739239819652, 15.146155753626172, 14.844229695509611, 13.968468987791864, 12.518202591636594, 14.471216099010894, 53.7663176084826, 11.441119983189829, 10.402918605589658, 10.265609282843322, 9.077880610492434, 9.142387330987424, 8.791931806336956, 10.69005516125258, 10.900660525233226, 14.543407848689077, 8.05553074203653, 8.207964893259065, 11.20820504997563, 11.707007719728885, 8.058039643940077, 7.604915050251376, 8.662288229739419, 11.383042564471792, 11.268847761380169, 10.89014500530056, 7.373761549792573, 6.94651793460403, 10.618218574937819, 12.023901938169036, 19.85870095840321, 12.105405381439322, 17.123803407196526, 11.245945871708727, 16.893318666961832, 19.442630805476135, 19.025926198536805, 35.617197102757835, 25.032639623799746, 14.816757204576374, 82.0623224047647, 36.73522552094957, 82.84248148049113, 61.15600177625821, 35.13333316584253, 48.04490697715599, 79.2140077581433, 60.32532466531027, 18.73922802884618, 26.056118265283217, 23.548220913070836, 32.529741135989184, 48.280129032440065, 41.80784847944463, 32.27189520661475, 45.49643565163788, 45.746701177831575, 62.07442989957443, 42.05114196491904, 35.36532458377288, 35.06319551171817, 64.73482287380074, 53.363996271490855, 27.177927710142885, 31.829229917355203, 33.11920107688091, 46.74302303128713, 39.96149590987, 40.03195845925067, 35.09121499549034, 48.25531248334767, 46.462361840738616, 36.14258307890607, 34.49910529560059, 33.333169419956775, 16.432625363642174, 15.152206565364567, 10.164526339120599, 13.204697752177465, 10.45643096507133, 8.82651547205732, 12.323426053537098, 8.474472654379113, 8.14700536037096, 10.560181519674382, 7.6315609479388655, 7.508882456905512, 10.373987176207292, 13.43167522411703, 11.65701549569732, 9.717559624954339, 8.062388743925332, 6.666023973523856, 6.712687210785873, 9.309098892099364, 6.37276304614852, 9.090848475803195, 13.856032760195964, 6.677528701609625, 16.99755360462076, 5.67779110233339, 5.618448439587049, 7.486802111171998, 5.568252199569546, 11.553836902024408, 5.714856965578022, 7.3937375847157965, 16.041555215188758, 10.116314404759098, 13.662165292618646, 13.811440129593647, 17.999027072316355, 9.161380748845605, 17.90624177431278, 33.046001064939404, 17.873058925688323, 63.23347455483357, 10.643065025747603, 19.65961631617042, 32.3630286283757, 23.844940190945337, 14.244200563335504, 12.977856056470406, 23.455277699845713, 29.371468727806054, 13.722652268707263, 13.663121620866875, 23.535095006850597, 34.39796663122822, 19.81231294499802, 35.84712134222822, 18.97013035886836, 33.81150840872412, 15.83057655231258, 18.47854446902994, 16.939703372190493, 18.906021023574176, 29.024688869332977, 28.88021578977059, 24.475211684514267, 18.36821890717369, 20.41963426174368, 18.398833190752836, 19.65781943719464, 19.260719206485376, 18.615618213745062, 18.343414304034333, 17.86445475995274, 17.838842574488115, 17.956267004112902, 17.924507953507167, 40.56408947545278, 16.94626134224035, 22.844452083400835, 9.263962472514402, 31.33896128120758, 9.181332956423253, 12.536030521067302, 9.605778843107293, 19.052944713139073, 8.112490765527644, 15.639820023326612, 7.166411377143074, 10.782073381732415, 12.985412024739052, 6.914902635545731, 16.889963038284844, 25.706190110832324, 8.656901319013217, 19.21164370199194, 6.794667583138051, 7.360440448482147, 17.25726676706917, 9.503371833589199, 9.079386019969506, 7.137300664359227, 36.764770041036904, 9.29868915225884, 10.410970322840997, 8.765572300370916, 5.994081369502656, 34.24193453077309, 68.2701687528996, 31.25978961447691, 50.80952610306259, 16.138693882014554, 20.619103956416858, 27.693019088568267, 22.880311843069805, 21.249930132630766, 29.69117050702023, 41.877026319675664, 32.58563825600983, 19.211034864143457, 26.816832097675746, 22.445044436262673, 70.07859190256217, 47.47796426866058, 34.49312287417979, 30.461852232990157, 27.782404311291405, 24.757881581031967, 23.05874502567958, 36.36306534397806, 31.229318829620436, 33.5889461705646, 32.944587980350015, 26.37206684969183, 42.69269992963727, 39.99473536618324, 43.19535084269817, 32.57694824183695, 30.57291836773276, 27.959537743776533, 28.22177478684321, 26.01353461118814, 10.751371266655354, 6.615333225661187, 8.701262107049487, 8.297322694845604, 10.506017675206994, 27.41329546437758, 13.725865886234912, 4.90611157482599, 9.443978609287063, 5.554452140004608, 4.7385987597255665, 5.4854203964979655, 8.573679621596813, 4.379873504032216, 4.042433841978168, 4.472722331397499, 6.258151858089436, 5.097901764848105, 9.498430932730342, 3.659470365066471, 7.7572463755082115, 4.344348508978444, 6.580044721282021, 4.42664770280165, 16.469088675098128, 3.138140038306002, 3.6183090127142283, 3.8615701531146955, 3.029894807009038, 2.995942907818473, 9.383537676945888, 18.839105652268323, 10.525078525711857, 16.021749457511454, 18.950187189370187, 52.28360599023825, 9.238085514877005, 8.915958075993304, 19.600011392540722, 19.583971134005946, 11.337360701942568, 9.566035710728313, 7.263864769068392, 21.361948940194363, 14.480461670052987, 10.249912712216254, 12.630874085620803, 10.809482812846763, 17.271023255148194, 9.686346711140137, 10.065239711029296, 13.827424088204701, 11.625462330686782, 14.416425838577785, 10.362063800667748, 9.713654785775375, 9.493032472451015, 21.953706713632172, 13.700380310328274, 24.025351273333158, 10.022683186900338, 9.75176660659766, 16.23702469843512, 9.093444442988174, 8.719141630557822, 10.459654865139644, 18.039510727301394, 8.574743650420492, 21.775810157974117, 7.43784426427013, 8.428425151994253, 37.455884187645026, 7.059231077912766, 6.659803543338897, 7.853731948959852, 6.273444079708485, 6.254237252534372, 18.350484760936496, 6.5272199087778455, 5.968246734786344, 5.884161567957281, 5.612798870484436, 5.608620485239281, 7.000742443775247, 5.476508016223911, 10.162800784341659, 4.465817710040752, 5.752719376039175, 39.59372305700842, 11.101308077535172, 29.028600996404162, 9.722288653323739, 11.27671315490631, 14.372069343499623, 19.90369119512323, 9.528122915032958, 15.879491382065956, 26.189400946802714, 8.997175407477366, 11.283121384353128, 10.610356701343724, 8.857707078989407, 11.733327489767168, 11.079204745072314, 8.859253630978296, 9.787787889393938, 7.456218761101948, 11.453670266546304, 5.7910968913580145, 8.832606875812855, 8.581471945231376, 5.836556296465721, 3.655662691072888, 3.536433412277131, 3.3010344043503683, 3.06992907347815, 3.4661061331002023, 2.913214785638258, 4.3761751992637565, 4.232394840173601, 4.681982073270016, 2.9570422760637514, 2.640549161151329, 2.6229978396613958, 2.3685168115287203, 2.303244978054725, 2.455974685502543, 4.504379228990781, 2.1600115386673138, 4.061162019752181, 3.386714078127409, 2.1104795916230183, 3.6999751976762485, 2.0358210677312965, 2.68351109631633, 3.162213266471772, 23.44853481469594, 6.5558589148310285, 5.415919548715087, 13.64876756904481, 13.463342626502003, 11.461337810670585, 10.87205246553313, 13.574705364704972, 6.336478848468496, 5.902424493302143, 6.440727252872419, 5.026607366024945, 6.906691992969172, 6.919245986626104, 6.688211900976324, 9.429602286949093, 9.916629103009011, 5.468558278356089, 7.141510550993731, 6.190289307026811, 7.171813241966349, 7.712953986275008, 7.836637604795128, 6.339729262655741, 7.000282646341756, 6.339381372760944, 5.901169787564376, 6.547768720455806, 8.108196443926467, 5.8720862379808905, 4.0296627339978555, 3.8052510364551306, 3.6357505137640116, 4.253820479841653, 6.190591705736565, 3.9109830967964907, 7.705367036874115, 3.161078284794215, 2.993181679291327, 7.361390922967492, 3.530940981372815, 2.8756328548056653, 2.700462854375759, 2.661986057231883, 2.6170602428332423, 2.579595271472977, 2.5234149792823977, 2.577603288732275, 3.9941977943818965, 2.2596353828503144, 2.2556219728031737, 3.1124822205330167, 2.701903842177811, 2.421837253456303, 2.0845255757598866, 2.1015877331365744, 2.032294855670443, 2.5775372054715584, 5.916777550313154, 5.361263620302496, 5.119536941293989, 2.5682342024598874, 6.127507996988217, 5.76725385916333, 4.946976624918438, 3.4294401351404726, 5.784506537510701, 3.752452153951469, 3.4942505852804535, 3.135620319894837, 3.095880726353627, 3.5534156376806276, 3.029211890166509, 3.076233176539129, 9.091300031939282, 4.408131515640131, 3.64631480822625, 3.003671120255317, 4.502233264234965, 3.0025826364443464, 2.383126634935253, 3.238867128513489, 2.0666097671930257, 2.051787548881888, 2.037139454604952, 2.1760732038954775, 1.9933216309806174, 2.1250898055586447, 2.0040356855132773, 1.7782140643312527, 2.499782973317105, 1.980889428509263, 2.0258539759041416, 1.5188404656836585, 2.005856629168278, 1.5960462934122606, 1.4566175537473287, 1.4555228957067345, 1.6240721385737014, 1.4459628313825479, 2.337327324169424, 2.4565295375733496, 2.0808148178214845, 1.374165790027, 2.068446321856156, 2.360443706261177, 2.3732693888427496, 4.665881611207063, 1.9731384289487823, 2.203187003948904, 3.907324266194235, 2.7064242360990662, 5.67261344734328, 2.592073747945741, 2.7516212477108133, 8.057234623506208, 3.567682797518314, 2.3566956757457755, 2.511527048442474, 3.8220254421733797, 4.061550684334806, 2.884335231427631, 3.2431059487921816, 2.719780028631626, 2.628529743041984, 2.66662618137615, 2.575651439016865, 24.569505579468448, 14.603026706557301, 10.041897227384661, 11.745647310756688, 7.690037200098597, 10.846607230069722, 22.865785559579393, 6.5642274882542555, 9.055168304067413, 5.220191069195214, 7.974379478255415, 4.6679385383290395, 4.5647479282506, 4.518355342212102, 3.453561135825555, 3.3637772104746393, 3.0737684208296154, 2.9732096699765678, 2.9137419617665854, 4.969494771169164, 2.502393929996267, 2.490467479995362, 9.75273852740495, 3.0119585300234606, 2.3546497485932725, 17.840877387139702, 2.320507131386845, 2.293008381260613, 2.2378098696451256, 2.8686063948983302, 8.411076288878386, 34.702978852723945, 10.869292402813606, 20.399878790496285, 5.7386308185566035, 9.289985335625484, 6.8740396720040495, 3.972481330582307, 5.127207871309909, 5.2452262391935385, 4.931959334121499, 7.807597498390454, 6.156564978219138, 7.002571166499397, 7.929767015031557, 5.194103848667283, 3.2458690677002147, 5.112693292009268, 2.5119046911648213, 2.80967136153572, 2.3627669688632857, 2.2721284322472965, 2.3117223852238205, 2.7480845172478787, 4.2126235290322, 2.3974623944597266, 1.980751273638089, 1.8029374279012964, 1.9596725941709157, 1.7098566046284038, 2.770456824739974, 1.6850405575903353, 2.6553021253872546, 1.6057798964579433, 1.8069040192470165, 1.5492143419160402, 1.5281837377020113, 1.5312473899096863, 1.7284285430728772, 2.9185149429779496, 1.6867554099835311, 1.9853746009481457, 1.733369158430683, 1.3503555612179334, 1.4561325006704853, 2.6323069804775256, 2.4777655835164287, 8.091662737474055, 4.33666826120956, 3.1653582820998145, 2.164181081252463, 4.3528770346183805, 2.2313104354756894, 2.2441130399943003, 3.5082345947644544, 2.8513579974996777, 2.24166214922155, 2.293589299011158, 3.0385034462863127, 5.575690385577401, 4.367470528367394, 2.4918487272963845, 2.845842621620441, 2.588478656741095, 4.916697848727143, 4.125505299898188, 5.205680191500058, 3.16169568296108, 2.690107592523073, 2.638155135404854, 2.2548896266718557, 2.2105123881660234, 2.7141343481855897, 2.121466692240459, 8.69021379953311, 1.8334430525853171, 1.7631185084993586, 3.229650916666364, 2.6937330151964565, 1.986333015177629, 2.2147838044082575, 2.2540621302952637, 2.4757744332705958, 2.139965111006676, 1.5956477086786343, 1.5465467549961314, 1.5264795954055017, 2.033646258351851, 1.5725698351869606, 1.8491163528450647, 1.3988223435859448, 2.248504478858504, 1.871975552195754, 1.4591759559509447, 3.0778186575843898, 4.088681796596844, 3.553895600504158, 4.285489792122933, 1.947602202400827, 2.3217349133585037, 3.4469211540059983, 2.1144599765542664, 2.069364008081427, 2.418635991506235, 2.060315479916705, 2.2342852309006553, 2.092660459279516, 4.432867924950199, 4.075021239743968, 3.8998576915600798, 3.8344707371804625, 2.7782792282543376, 2.624475275418173, 2.5673990705189444, 2.5125342621836526, 2.7013082472523573, 2.3588930819062486, 2.422253296011428, 2.682386538675163, 2.335022073117454, 9.96962732208929, 2.241270118425575, 2.3526881422513757, 3.614837792582625, 1.5747159475365644, 4.770819259546824, 1.827502066378274, 1.6075903557180344, 3.2490422465282154, 4.276886792302109, 1.8201402844418062, 1.367341671844346, 1.4735001672834416, 4.653839289550006, 3.062810830251746, 1.278001171559006, 2.5776571611594616, 5.451981754974744, 4.380307566760355, 4.124748746579527, 3.819795204628398, 2.54892048557413, 9.223120950431888, 3.1330739510970305, 5.237505787282351, 6.852643458460073, 4.096082019670317, 4.243449333708685, 3.7391919785600045, 2.9499967607492326, 3.9261227231127243, 2.902187980971893, 3.0235084618187558, 3.1515859061966007, 2.991127923941851, 12.969486596866098, 12.821079832727676, 12.793854787151348, 12.44278337551373, 12.41959843380722, 12.665987393205153, 14.528769769306415, 12.50509466231716, 13.355537289848701, 12.827672363481161, 3.8797280567939953, 12.714306265524767, 2.6373988128817207, 2.462436629139171, 2.289869455630903, 2.631289620544525, 1.2656053677133992, 4.329373228281152, 1.201466668397044, 1.3170517256233445, 2.0326038517805416, 1.8792585903998273, 2.000197144662607, 1.256813436602627, 1.1506299102078394, 1.7067411542670206, 1.7756608549591941, 1.5435032989251152, 1.0237636185848018, 0.7696704472072282, 11.02176259445751, 2.122719309401459, 9.330906124779036, 3.531255757157731, 1.7164864886004945, 2.5902730648757526, 2.1466883380729587, 1.7744874917831324, 3.696965951280317, 3.696965951280317, 3.678390934742354, 3.6557858275103374, 3.2490954388274798, 2.963599722662595, 2.6585316398862058, 4.781930258849022, 3.134286914762457, 2.4886208508587444, 2.396521629087176, 2.656970484399756, 2.2921564681277125, 1.8706813299667324, 1.7713480109051931, 1.6131941669453973, 1.6729872539850004, 1.7348057815896814, 2.860400090800131, 1.3524949077550767, 1.5261629897600792, 1.3493004508969662, 1.7195538763812723, 2.297041104248563, 1.568074416464407, 1.4482570361736937, 1.2764932500269566, 2.2862842740260096, 1.5968385067559738, 1.781518557994042, 1.7756752922020014, 1.9109510233439693, 3.7449546733655166, 2.9676180570880937, 2.3023673573506125, 3.1050108654854034, 2.245038909204165, 4.815445858970475, 2.6837806312650483, 2.5505157353968935, 2.2889273809934387, 4.766631149024625, 2.1018926014093777, 1.8638707039298548, 3.1328841115232304, 2.6097211235073585, 2.557542249031602, 2.335969487069798, 2.532600150480348, 2.5650435893827948, 2.3691515348439354, 2.290593850101683, 1.9420380493198917, 1.8445870126104165, 2.8852455051770076, 2.2523262557575414, 1.7303431881400588, 1.570642148057062, 3.151646335136847, 1.9567003272040833, 1.8227630089503872, 1.85144846014596, 1.983852624821445, 1.4013478702336821, 3.318801114316042, 1.3458184600098086, 1.3458184600098086, 1.4003804581090435, 2.9518988853030788, 1.4118219732003978, 1.6059161280482301, 1.8350152464652296, 1.416123296413525, 1.5372212331521573, 2.2736250373716325, 1.934651847610502, 4.331420987433707, 3.933731042032344, 2.0368238215268244, 2.5010061899959632, 1.8111333672587675, 2.7018762459006127, 2.636783213404139, 2.4548762962769883, 2.0401473038351567, 1.8249544390011943, 6.956443550069919, 2.959443903645739, 2.8736687054812835, 2.5851591186512843, 4.540698816220627, 2.029482473186575, 2.365245255478489, 2.446300213035464, 1.8169770530303397, 1.6462054978286247, 1.60414505760474, 1.9014303669844992, 1.681234208042087, 1.5750616788498373, 4.4350445012186155, 1.6176298862735154, 2.0504745387679564, 1.4528833012909994, 1.4317937949907154, 2.6103742365404345, 2.1518166272532206, 1.6294958011973038, 2.3849124515744675, 2.9869748110443064, 1.8103502540388403, 1.1244976912938087, 2.120216715010326, 1.834631055849416, 2.0982645878510358, 1.0379981409495647, 3.831761603173868, 4.204325562258748, 2.8686162310025396, 2.1848870666505187, 2.563642316266534, 2.0802882646694, 2.3587088632495887, 1.9676127466121798, 1.942275155585709, 1.8110152654639387, 4.254168722678659, 4.106987171185541, 3.8830163346443167, 3.9406901262431173, 3.8734201968456246, 3.184540589364571, 2.893749591202527, 2.494783334106302, 2.439080341595402, 3.489317424284552, 1.9874686187105044, 4.0509964645471825, 1.7380433727778621, 1.6848772022061187, 1.6403023718190313, 1.858305540237429, 1.742855585327115, 2.7808606581432196, 2.3098363651420883, 1.5550031779512832, 1.7620659686317945, 1.250160524750145, 1.5001306507921475, 2.103218848139836, 3.910193127674448, 3.7295106137731504, 1.2802078612548349, 3.6759063593166657, 1.339313137890079, 1.6396009724291605, 2.1174455480384746, 3.989092147724929, 1.9148913483109442, 2.4045484704031366, 1.7683604962298904, 4.484982381837333, 2.3451931009965867, 4.048945118079838, 1.6952056531703692, 1.7002482771065834, 1.4642621105208293, 2.0218275411289826, 1.9503209503426229, 1.6247785266599166, 11.366359888587287, 6.5012663026766955, 1.9374882446974055, 1.7728193157838912, 1.10870325061796, 1.252162714421165, 0.9731464478534934, 1.777430978105911, 2.7891126441712375, 1.2478410223170546, 1.3796180038759136, 1.3524639991103473, 0.9139851725909299, 0.780416645898022, 1.9319415238968034, 1.7344916329033595, 1.1414493494726952, 0.8449025783924087, 1.8307997481502583, 1.9314612291337951, 0.8264942767285357, 2.284845818229291, 1.3364368141007055, 4.736241074326277, 2.6127738420935325, 1.3969381821566274, 1.1791069318756002, 1.924938767380062, 1.5692905765653156, 3.5775111624665374, 1.4700569140888642, 2.464648784440739, 2.224997861507169, 2.4393619008423744, 1.5765705020832432, 1.2877487703603467, 1.626776749174085, 1.6196340618629053, 1.9209997062882416, 1.4379045453868733, 1.3501255061106583, 1.0305769796836757, 1.4785382509273137, 0.9163389900573115, 1.3450686014362445, 1.365162825249938, 1.254228989679338, 1.2567389019587611, 0.8031993137224293, 0.7735207514135287, 0.8947894600912333, 0.6282186563141472, 3.1261244019786147, 2.6387011383306263, 0.6702070088745611, 0.5282076127973258, 2.65296353674878, 3.384157674977488, 1.4106181526070456, 0.9323135578713967, 4.834320410215798, 1.7726434260989148, 1.30959765321489, 2.1082464100846168, 1.4466254577951034, 1.9873638717832143, 1.5926580354693252, 1.50351931052401, 1.3199049636194502, 2.154614890770369, 1.9288133138855323, 1.8338493648802396, 1.574277315429351, 1.7791295004209162, 1.2637240112810677, 1.823949923065472, 1.2371205093330215, 2.5741415584348974, 1.112696860959182, 1.9705584260553746, 1.4561111415111412, 1.1636427290713647, 2.272944557378274, 4.183090871618082, 0.9912810471218486, 0.7100482322992021, 0.8430561228827187, 1.7636993047160479, 0.9952686113861062, 1.0821065903953284, 0.757451250512445, 0.9193200592632532, 1.7731888029988807, 0.6969606617245696, 0.6001351197518702, 1.4923777222962178, 0.6720006869857766, 1.625945547498931, 1.2688685192005769, 1.0563935347006759, 1.3766245458891018, 1.1418881083565113, 1.719095713958243, 1.228214215656426, 1.138413866228385, 1.20645857152867, 1.07908069077574], \"Term\": [\"key\", \"god\", \"edu\", \"thanks\", \"chip\", \"windows\", \"window\", \"game\", \"team\", \"government\", \"year\", \"soon\", \"encryption\", \"clipper\", \"sale\", \"com\", \"address\", \"card\", \"does\", \"jesus\", \"keys\", \"drive\", \"games\", \"deleted\", \"test\", \"mail\", \"people\", \"stuff\", \"max\", \"use\", \"bike\", \"cars\", \"engine\", \"msg\", \"insurance\", \"ride\", \"battery\", \"driving\", \"car\", \"vehicle\", \"scientific\", \"ford\", \"amp\", \"moving\", \"jobs\", \"effects\", \"army\", \"dod\", \"duo\", \"forward\", \"range\", \"areas\", \"phones\", \"shift\", \"edge\", \"notice\", \"fun\", \"circuit\", \"chances\", \"reverse\", \"riding\", \"thread\", \"turn\", \"break\", \"parts\", \"signal\", \"business\", \"level\", \"science\", \"power\", \"left\", \"design\", \"don\", \"thing\", \"just\", \"time\", \"long\", \"right\", \"like\", \"good\", \"company\", \"pretty\", \"money\", \"course\", \"make\", \"work\", \"little\", \"want\", \"way\", \"think\", \"really\", \"things\", \"doesn\", \"know\", \"use\", \"stuff\", \"case\", \"probably\", \"new\", \"used\", \"problem\", \"going\", \"people\", \"does\", \"say\", \"need\", \"did\", \"armenian\", \"armenians\", \"batf\", \"population\", \"amendment\", \"waco\", \"political\", \"constitution\", \"soviet\", \"civil\", \"soldiers\", \"fight\", \"brought\", \"koresh\", \"greek\", \"treatment\", \"creation\", \"crimes\", \"militia\", \"died\", \"cult\", \"race\", \"turkish\", \"believed\", \"killed\", \"forum\", \"nazis\", \"genocide\", \"bomb\", \"judge\", \"serve\", \"accident\", \"fbi\", \"killing\", \"federal\", \"women\", \"war\", \"pass\", \"rights\", \"government\", \"children\", \"people\", \"members\", \"evidence\", \"said\", \"law\", \"death\", \"school\", \"space\", \"did\", \"national\", \"guns\", \"years\", \"think\", \"group\", \"just\", \"state\", \"don\", \"today\", \"fact\", \"news\", \"world\", \"know\", \"like\", \"time\", \"day\", \"believe\", \"didn\", \"say\", \"right\", \"god\", \"new\", \"way\", \"make\", \"does\", \"good\", \"dos\", \"unix\", \"modem\", \"vram\", \"scsi\", \"ati\", \"gif\", \"centris\", \"format\", \"svga\", \"simms\", \"irq\", \"quadra\", \"microsoft\", \"cursor\", \"floppy\", \"screen\", \"norton\", \"port\", \"bitmap\", \"ethernet\", \"vga\", \"gateway\", \"sys\", \"animation\", \"files\", \"ports\", \"xterm\", \"monitors\", \"modems\", \"mac\", \"windows\", \"monitor\", \"card\", \"ide\", \"ram\", \"ftp\", \"server\", \"mouse\", \"video\", \"file\", \"advance\", \"controller\", \"disk\", \"apple\", \"thanks\", \"drive\", \"software\", \"version\", \"graphics\", \"color\", \"drivers\", \"mail\", \"program\", \"help\", \"using\", \"info\", \"does\", \"use\", \"know\", \"need\", \"problem\", \"looking\", \"like\", \"edu\", \"islam\", \"palestinian\", \"muslims\", \"islamic\", \"atheism\", \"israel\", \"christianity\", \"palestine\", \"atheists\", \"jerusalem\", \"teachings\", \"lebanese\", \"arabs\", \"jew\", \"borders\", \"villages\", \"israelis\", \"harvard\", \"adam\", \"judaism\", \"eternal\", \"subjective\", \"marriage\", \"romans\", \"israeli\", \"kurds\", \"allah\", \"reject\", \"destruction\", \"catcher\", \"beliefs\", \"christians\", \"belief\", \"faith\", \"jews\", \"god\", \"existence\", \"morality\", \"bible\", \"christian\", \"moral\", \"arab\", \"atheist\", \"jesus\", \"religion\", \"peace\", \"truth\", \"jewish\", \"believe\", \"religious\", \"christ\", \"say\", \"life\", \"people\", \"true\", \"word\", \"hell\", \"hockey\", \"nhl\", \"players\", \"pitching\", \"braves\", \"teams\", \"leafs\", \"playoffs\", \"rangers\", \"league\", \"pens\", \"season\", \"coach\", \"espn\", \"team\", \"sox\", \"alomar\", \"montreal\", \"phillies\", \"puck\", \"baseball\", \"scoring\", \"bruins\", \"hawks\", \"pitcher\", \"mets\", \"minnesota\", \"pts\", \"stats\", \"sharks\", \"talent\", \"game\", \"fans\", \"games\", \"detroit\", \"toronto\", \"player\", \"play\", \"pittsburgh\", \"win\", \"year\", \"played\", \"hit\", \"runs\", \"chicago\", \"think\", \"good\", \"fan\", \"vesa\", \"eisa\", \"font\", \"widgets\", \"vlb\", \"postscript\", \"converter\", \"orchid\", \"adaptor\", \"truetype\", \"cartridges\", \"dot\", \"tgv\", \"archie\", \"adobe\", \"printers\", \"cheapest\", \"dpi\", \"pairs\", \"partition\", \"laserjet\", \"matrix\", \"string\", \"capacitors\", \"edit\", \"refresh\", \"xputimage\", \"joystick\", \"atm\", \"resolutions\", \"postage\", \"sale\", \"manuals\", \"obo\", \"shipping\", \"condition\", \"printer\", \"asking\", \"offer\", \"fonts\", \"offers\", \"widget\", \"laser\", \"includes\", \"manual\", \"excellent\", \"price\", \"email\", \"isa\", \"sell\", \"contact\", \"interested\", \"mail\", \"new\", \"original\", \"com\", \"windows\", \"thanks\", \"detector\", \"lunar\", \"homosexuality\", \"reagan\", \"keenan\", \"libertarians\", \"handgun\", \"captain\", \"orientation\", \"gay\", \"deficit\", \"neil\", \"radar\", \"shock\", \"legislation\", \"stacker\", \"garrett\", \"heterosexual\", \"communist\", \"venus\", \"vat\", \"survey\", \"democrats\", \"irrational\", \"gravity\", \"galileo\", \"fate\", \"hospitals\", \"rll\", \"ufl\", \"knife\", \"homosexual\", \"spacecraft\", \"cellular\", \"republicans\", \"orbit\", \"moon\", \"administration\", \"profit\", \"president\", \"traded\", \"homosexuals\", \"surprise\", \"johnson\", \"yeah\", \"sexual\", \"earth\", \"fpu\", \"coprocessor\", \"baerga\", \"mormon\", \"pds\", \"weiss\", \"redesign\", \"qur\", \"saint\", \"approval\", \"listserv\", \"miracles\", \"quran\", \"messenger\", \"soccer\", \"baptism\", \"sticker\", \"mlb\", \"sciences\", \"leftover\", \"launches\", \"lutheran\", \"rental\", \"gregory\", \"protestant\", \"dis\", \"excerpts\", \"mormons\", \"priest\", \"collision\", \"spelled\", \"commitment\", \"davidian\", \"colorado\", \"versa\", \"freeware\", \"math\", \"iisi\", \"church\", \"orthodox\", \"ted\", \"edu\", \"apr\", \"corn\", \"revelation\", \"university\", \"address\", \"services\", \"lost\", \"robert\", \"catholic\", \"andrew\", \"send\", \"encryption\", \"nsa\", \"crypto\", \"escrow\", \"rsa\", \"des\", \"clipper\", \"cryptography\", \"encrypted\", \"nist\", \"pgp\", \"eff\", \"crypt\", \"denning\", \"vesselin\", \"escrowed\", \"plaintext\", \"ncsl\", \"skipjack\", \"wiretap\", \"hellman\", \"sunrise\", \"secure\", \"cipher\", \"pkp\", \"keys\", \"dorothy\", \"cripple\", \"ncsa\", \"govt\", \"privacy\", \"key\", \"algorithm\", \"chip\", \"scheme\", \"security\", \"secret\", \"classified\", \"agencies\", \"enforcement\", \"proposal\", \"phone\", \"chips\", \"public\", \"government\", \"cooling\", \"apostles\", \"bodies\", \"towers\", \"hood\", \"clause\", \"reactor\", \"licensing\", \"volunteer\", \"temperature\", \"lds\", \"ankara\", \"virgin\", \"abraham\", \"israelites\", \"migraine\", \"trillion\", \"blessed\", \"preached\", \"extras\", \"pharisees\", \"treaty\", \"temperatures\", \"armored\", \"steam\", \"functional\", \"leaks\", \"sandy\", \"coercion\", \"unity\", \"thou\", \"verses\", \"water\", \"spirit\", \"sabbath\", \"naive\", \"father\", \"liquid\", \"plants\", \"sin\", \"holy\", \"plant\", \"kingdom\", \"heaven\", \"god\", \"jesus\", \"bmp\", \"son\", \"mother\", \"adb\", \"convertible\", \"tire\", \"ssto\", \"dealership\", \"cga\", \"std\", \"sdio\", \"cycles\", \"micron\", \"honda\", \"fairing\", \"automatics\", \"beast\", \"hawk\", \"ucs\", \"arc\", \"fork\", \"cobra\", \"fade\", \"bio\", \"connects\", \"hobby\", \"ireland\", \"infrared\", \"housing\", \"socialists\", \"relay\", \"propulsion\", \"intensity\", \"terminals\", \"dial\", \"rubber\", \"tires\", \"vinyl\", \"orbital\", \"rear\", \"comm\", \"bone\", \"constant\", \"astronomy\", \"chris\", \"hook\", \"tempest\", \"xdm\", \"wuarchive\", \"olwm\", \"wustl\", \"tvtwm\", \"metzger\", \"pmetzger\", \"newest\", \"phigs\", \"alias\", \"shearson\", \"autocad\", \"zip\", \"pex\", \"mwm\", \"icons\", \"oem\", \"indiana\", \"mapped\", \"charger\", \"pixmap\", \"app\", \"rectangle\", \"organizers\", \"informatik\", \"icon\", \"expose\", \"xcreatewindow\", \"perry\", \"cica\", \"shareware\", \"mask\", \"map\", \"msdos\", \"window\", \"formats\", \"manager\", \"address\", \"title\", \"directory\", \"pub\", \"colormap\", \"edu\", \"event\", \"ftp\", \"thanks\", \"does\", \"geb\", \"intellect\", \"cadre\", \"chastity\", \"dsl\", \"shameful\", \"pitt\", \"skepticism\", \"gordon\", \"surrender\", \"lyme\", \"banks\", \"creed\", \"kingman\", \"octopus\", \"centaur\", \"northeast\", \"cleveland\", \"yount\", \"grabbed\", \"gosh\", \"yanks\", \"proton\", \"lerc\", \"demos\", \"combo\", \"stomach\", \"larc\", \"idiotic\", \"mississippi\", \"soon\", \"stage\", \"edu\", \"gov\", \"chipset\", \"nasa\", \"brain\", \"meant\", \"dtmedin\", \"catbyte\", \"ingr\", \"paradox\", \"intergraph\", \"gainey\", \"sandberg\", \"maine\", \"hillary\", \"fujitsu\", \"wallet\", \"fluke\", \"brooks\", \"medin\", \"simmons\", \"defensively\", \"affects\", \"resistant\", \"cheat\", \"tartar\", \"grounded\", \"assaults\", \"obsolete\", \"bonds\", \"winfield\", \"difficulties\", \"celebrate\", \"lineup\", \"accidentally\", \"ssd\", \"huntsville\", \"harris\", \"netcom\", \"easter\", \"sue\", \"gary\", \"gifs\", \"dave\", \"hall\", \"uucp\", \"uunet\", \"com\", \"era\", \"networking\", \"moa\", \"deeds\", \"hst\", \"motorcycling\", \"cruiser\", \"ieee\", \"carbs\", \"offended\", \"ghetto\", \"clh\", \"hudson\", \"ntsc\", \"nth\", \"demonstrating\", \"physician\", \"tasks\", \"fooled\", \"gsfc\", \"immune\", \"basing\", \"ditto\", \"carnegie\", \"mellon\", \"societies\", \"wagon\", \"fri\", \"credible\", \"horses\", \"instruct\", \"radiator\", \"mercedes\", \"designers\", \"bmw\", \"kent\", \"morals\", \"bell\", \"assert\", \"objective\", \"values\", \"cheers\", \"did\", \"sig\", \"flyers\", \"scorer\", \"champaign\", \"sanderson\", \"yankees\", \"savard\", \"roberts\", \"defenseman\", \"urbana\", \"brind\", \"denis\", \"cardinals\", \"recchi\", \"guerillas\", \"subscribe\", \"amour\", \"translate\", \"padres\", \"stephenson\", \"winnipeg\", \"innings\", \"picks\", \"barrel\", \"pirates\", \"popup\", \"cassels\", \"cobb\", \"champions\", \"trades\", \"galley\", \"islanders\", \"doug\", \"jersey\", \"mvp\", \"mailing\", \"uiuc\", \"division\", \"randy\", \"tonight\", \"illinois\", \"bos\", \"deskjet\", \"tor\", \"det\", \"chi\", \"ulf\", \"stl\", \"nyi\", \"canadiens\", \"pit\", \"nyr\", \"ink\", \"openlook\", \"mcrae\", \"helicopters\", \"phi\", \"springfield\", \"que\", \"buf\", \"edm\", \"providence\", \"ott\", \"sunview\", \"hank\", \"cal\", \"min\", \"comeback\", \"shaft\", \"fidonet\", \"cape\", \"rochester\", \"van\", \"todd\", \"mon\", \"david\", \"henrik\", \"bishop\", \"loser\", \"grin\", \"dell\", \"giz\", \"films\", \"har\", \"swapping\", \"max\", \"dog\", \"chocolate\", \"callback\", \"aggression\", \"lisa\", \"bxn\", \"bhj\", \"jupiter\", \"expo\", \"leagues\", \"expects\", \"xerox\", \"refusal\", \"royals\", \"foreground\", \"watson\", \"driveway\", \"hewlett\", \"packard\", \"coaches\", \"yup\", \"birthday\", \"deleted\", \"stuff\", \"joke\", \"orbital\", \"palestineans\", \"palestinean\", \"risc\", \"fallacy\", \"train\", \"pentium\", \"powerpc\", \"dawn\", \"occupying\", \"hype\", \"xwindows\", \"acs\", \"token\", \"idaho\", \"flows\", \"dayton\", \"proportional\", \"ohm\", \"motherboards\", \"ported\", \"awesome\", \"montana\", \"absurdity\", \"calculator\", \"harmless\", \"satan\", \"angels\", \"fascism\", \"petri\", \"virginia\", \"intel\", \"duke\", \"semiconductor\", \"test\", \"sam\", \"rape\", \"tells\", \"instruction\", \"air\", \"energy\", \"diagrams\", \"subdirectory\", \"stefan\", \"thereof\", \"quack\", \"paperback\", \"distributor\", \"jointly\", \"bel\", \"udel\", \"paradise\", \"pal\", \"jagr\", \"toner\", \"survivor\", \"suck\", \"cubs\", \"moore\", \"src\", \"swelling\", \"esdi\", \"harvey\", \"deciding\", \"females\", \"davidsson\", \"chemistry\", \"ahh\", \"deed\", \"sarcasm\", \"yearly\", \"salvation\", \"jerk\", \"fundamentalist\", \"circuits\", \"suit\", \"theory\", \"wave\", \"bring\", \"book\", \"books\"], \"Total\": [52.0, 82.0, 90.0, 112.0, 42.0, 79.0, 43.0, 50.0, 41.0, 51.0, 65.0, 28.0, 26.0, 25.0, 34.0, 62.0, 39.0, 59.0, 129.0, 38.0, 24.0, 71.0, 38.0, 30.0, 25.0, 69.0, 130.0, 40.0, 18.0, 114.0, 34.62244130040138, 24.256389201150455, 20.904688766254342, 15.972894500338658, 15.698841480982287, 14.86342096654089, 13.345274627606383, 15.475339877539179, 57.56052651554546, 12.278867503480855, 11.232549706839334, 11.162297594438414, 9.90461984853869, 9.97647669727159, 9.618670555210494, 11.742695289804827, 11.977433695570467, 16.002903284571552, 8.882269489605342, 9.06756967242858, 12.4088612128058, 12.965603539814719, 8.928714111976277, 8.432355071340613, 9.608613552617697, 12.626847744527861, 12.513385089067148, 12.094484907975971, 8.200500703358413, 7.773256684415852, 11.907279004220726, 13.602944438419781, 23.131511003471303, 13.784804066218696, 20.05203511058143, 12.735878429460325, 20.131047431332185, 23.865048598774727, 23.536270263605093, 49.63437470454425, 33.182572775867605, 17.77944128716532, 149.13858934529463, 55.270453496049846, 153.53813412812414, 104.69792160609583, 52.82611488316533, 80.5873703276518, 155.24262578445402, 109.72509223168046, 24.077484488897852, 37.06333180111983, 32.495900974118044, 49.723490012621646, 83.61120568005782, 71.24343078683273, 50.36868507084467, 81.73480751252369, 82.45867201793236, 127.63491341957797, 73.80614380659789, 58.73308333540901, 58.23170537951101, 151.02128808090703, 114.48221541136684, 40.269609584109034, 51.9659945288733, 55.54536085239886, 99.12933172838743, 77.90483764334682, 80.23253522568952, 63.37027533419902, 130.32589972118362, 129.01502398563824, 77.46033647751827, 84.02773716077796, 80.0423562581471, 17.259865466321294, 16.313941516585146, 10.991766441591396, 14.341402362082421, 11.400224026928104, 9.701455553197965, 13.552328512562482, 9.322539877977325, 8.974245476957108, 11.659024236570689, 8.458801052799943, 8.33612271573324, 11.520525292986552, 14.981351956053333, 13.019128321508305, 10.865723645248424, 9.016254909501967, 7.493264079649597, 7.553378640942351, 10.504603937222832, 7.200433715305931, 10.272221960237388, 15.781976337072807, 7.611852960022511, 19.410766818710904, 6.50503124521408, 6.445688543036474, 8.597481777076219, 6.395492303264738, 13.270682185094472, 6.564772862131747, 8.516688782117855, 18.899390772916917, 12.030076977232484, 16.731676017987578, 17.072180649363283, 23.531465277790172, 10.875425392211397, 23.92634014936997, 51.415637370160084, 24.935590694947713, 130.32589972118362, 13.220408613390168, 30.44954769508275, 63.8234582576241, 42.20155399797053, 20.165051794731728, 18.030115869997477, 48.72951921502604, 80.0423562581471, 20.745000271122144, 20.892947368043714, 60.23846731490918, 127.63491341957797, 43.18537574385687, 153.53813412812414, 40.47527423518375, 149.13858934529463, 28.66706171346169, 41.60208060297397, 33.712526605561095, 46.057084861266226, 151.02128808090703, 155.24262578445402, 104.69792160609583, 44.319254629981415, 65.33230132335963, 53.46583563546707, 77.46033647751827, 80.5873703276518, 82.3226819870989, 99.12933172838743, 82.45867201793236, 83.61120568005782, 129.01502398563824, 109.72509223168046, 43.065400094994196, 18.111710515777443, 24.63304266833446, 10.091417854402923, 34.181614546001086, 10.059809179907147, 13.762149282555258, 10.55816000464517, 20.990874733031827, 8.939946145395135, 17.32324197505161, 7.993866756131136, 12.066265481010333, 14.539186531724802, 7.742359031028635, 18.913983371640843, 28.79378892381797, 9.734748239874358, 21.64901079713561, 7.665560655574839, 8.312459003345744, 19.524641431010718, 10.759249933856498, 10.285375827770617, 8.0869665524222, 41.678193325620065, 10.544022927463315, 11.824748303981874, 9.964022272806956, 6.822890733204297, 39.45616358016852, 79.52549653895572, 36.41440028789808, 59.83231154721986, 18.61247592544029, 23.994878285453215, 33.25128044186518, 27.54148718736525, 25.664192810501966, 37.7345959502234, 55.53579639067203, 42.08316538522372, 23.826893450250925, 35.353332547512814, 28.947272258381833, 112.77425729207732, 71.02140079731413, 48.554946589519744, 42.29119831819935, 37.73443392564358, 32.732490329781534, 30.208143353953375, 69.66486364491904, 55.64042897917527, 64.93369994500253, 66.66575111932896, 42.767796477619775, 129.01502398563824, 114.48221541136684, 151.02128808090703, 84.02773716077796, 80.23253522568952, 58.69315373347937, 155.24262578445402, 90.17889677584078, 11.57933920419111, 7.443301162794432, 9.920844741439211, 9.475172578622495, 12.158045642767876, 31.854789097929725, 16.035040089511075, 5.734152665841594, 11.039150733378648, 6.49488368623276, 5.567076360761606, 6.490975216526335, 10.166079063520575, 5.207841567343498, 4.87040242539811, 5.40575242819349, 7.571074127917402, 6.186579130970703, 11.639262654354546, 4.4874383018080115, 9.563651817245244, 5.364817503732813, 8.170613522187793, 5.5288518988597275, 20.633601914907516, 3.966107980101758, 4.5749000881590804, 4.914089153678701, 3.8579601844303677, 3.8239116063342617, 11.980779327606847, 24.700831321295787, 13.749444073921364, 21.605967366867652, 26.357766428969665, 82.3226819870989, 12.305357089611425, 11.920116820599127, 29.05690790585969, 29.335420962273734, 15.693046456592969, 13.357309824099223, 9.583580420841797, 38.65427518583961, 24.545441634526583, 15.44049867739741, 21.38646877494, 22.319405073042727, 65.33230132335963, 18.082000262533594, 21.210178031590456, 77.46033647751827, 39.458402542923146, 130.32589972118362, 48.50067215750664, 29.84053025382926, 25.086848195878893, 22.960336228494512, 14.528583644662106, 25.871863742406546, 10.850886521246442, 10.5799699414001, 17.62926125053663, 9.921647777608912, 9.547344965168856, 11.581636655640466, 20.007495280984404, 9.514111155956032, 24.234694802126658, 8.283444084913272, 9.388423699664404, 41.871054773042225, 7.892389685879846, 7.4880068776751445, 8.884762829837936, 7.101649961305894, 7.0824405873523215, 20.78938226042939, 7.42641853050456, 6.796450069768427, 6.712364902463672, 6.441002205805994, 6.4368366327030415, 8.048548276828617, 6.400789969352945, 11.962278200523343, 5.294021044371681, 6.825509983686269, 50.188668101759404, 13.682376543828516, 38.993118447711026, 11.887126618196524, 14.247820794220692, 19.573826760336893, 31.12955094889769, 12.91561027650148, 29.755027294846364, 65.17881312808076, 12.943392349816625, 21.06139350349722, 23.49426275982652, 15.996873381201544, 127.63491341957797, 109.72509223168046, 16.758798854315987, 10.784180739929923, 8.28734138100192, 13.170125424677044, 6.715452375190829, 10.255193226562394, 10.311125407853117, 7.152301875034602, 4.486785300828319, 4.36755606028489, 4.132157015697798, 3.9013729623760263, 4.439859754433662, 3.7443374494002004, 5.642675883231026, 5.490334787716207, 6.171093566755292, 3.9189906677126958, 3.501035815125301, 3.501487967001784, 3.1996434632969777, 3.1343675880457247, 3.383614315164536, 6.222113251164453, 2.9911341579071458, 5.629892091002296, 4.708144008872087, 2.9416022070163756, 5.186879349989427, 2.8697770458284366, 3.7946717334822706, 4.478508348054866, 34.27710952824829, 9.596352031723727, 8.037839641772075, 24.556014105107167, 24.82671033130088, 21.313231949767935, 23.049280381823323, 31.386843828200835, 11.85431482689508, 12.094507209387963, 14.700024482386585, 9.58748312072337, 18.285769117917457, 20.12774836913521, 18.917854846285177, 40.92890468103085, 49.940357441812914, 13.338683273332466, 29.30036752415974, 24.302370043767993, 44.728575822423316, 69.66486364491904, 99.12933172838743, 33.87820332272399, 62.94346271315901, 79.52549653895572, 112.77425729207732, 7.375265738227631, 9.36536001015681, 6.966512531540181, 4.857159636199633, 4.632748105062852, 4.463256009102646, 5.253240989369901, 7.6884638030306, 4.883775352111403, 9.812866933375096, 4.030408685019347, 3.8230349123443466, 9.46005963281629, 4.552656604286691, 3.73331331469635, 3.527959822733354, 3.4894829588365783, 3.444557155746258, 3.4070921753181955, 3.350911884329307, 3.431365416419039, 5.352777788702812, 3.0871322845160107, 3.083137843881412, 4.302384596257977, 3.746472593248505, 3.3614239777488413, 2.912023060048892, 2.935928883612869, 2.8599973715043863, 3.641735154454687, 8.567450652126093, 7.794880744976422, 7.879764276645903, 3.66821083010673, 12.276731694572977, 13.359541884418483, 11.118558307659914, 6.231400270863282, 18.798185648800885, 7.808313807138333, 7.503937530829933, 6.77797114966594, 6.7644432018038545, 21.456093579009302, 8.486639177719466, 20.705475845727328, 9.976302193282118, 5.237284969876249, 4.61153703118874, 3.832824966403408, 5.7454660628063055, 3.96854104341378, 3.212299021040354, 4.492464477171072, 2.8957632133446327, 2.8809485888108632, 2.8662929229107728, 3.0718543056317107, 2.8224753161162783, 3.0227067522800857, 2.885722545795792, 2.6073690125218327, 3.736686467503606, 3.0074728496049232, 3.1016948664626205, 2.347994011091995, 3.115385980147958, 2.502229893442272, 2.2857711018349325, 2.284687753222432, 2.5513964157525333, 2.2751165545423397, 3.68480178921532, 3.893803432828498, 3.3035413621499785, 2.2033195333438638, 3.371927843253087, 3.9018450362981687, 4.075206797858282, 11.112472948173263, 3.450602877623244, 4.11879893663374, 10.178990966519551, 6.035631683714929, 24.47886102042698, 5.8649258305430845, 6.760674970451804, 90.17889677584078, 13.281687778851419, 4.8095942224118655, 6.216658430855645, 28.679072757519787, 39.584633553178435, 11.260483694342145, 22.091660858588583, 11.385961362325798, 10.062061631385873, 10.97678352795772, 43.4805980111113, 26.321537731890917, 15.647624236530923, 10.868312196419, 12.875926491268855, 8.516452165868474, 12.013145583875668, 25.531408525062695, 7.408786626958443, 10.317292087912659, 6.072269901145901, 9.366947265389607, 5.494353713971706, 5.391163274858864, 5.344770308671579, 4.279976101867293, 4.190192176300754, 3.9001833866538838, 3.810319060885435, 3.7401569273975537, 6.449584322590549, 3.3288088959257345, 3.3169080205562063, 13.049562503404315, 4.034681871516715, 3.1810647148716975, 24.149818184717834, 3.1469221402742567, 3.119460622883295, 3.0642248597030357, 4.017935860040852, 11.784790051316763, 52.525764574646864, 17.55401506089398, 42.436794134599765, 9.403112586801084, 17.83791000962914, 12.257146247650342, 6.071406951959716, 9.061224730738054, 10.376711930101528, 10.157236267956531, 35.91150303025351, 21.03993770150239, 34.068959636964266, 51.415637370160084, 6.021826027945657, 4.177225228851415, 6.632420303590994, 3.339626828982152, 3.7570288022667646, 3.190490122238689, 3.0998505671508374, 3.163923633174556, 3.8367840453598494, 5.901792413629721, 3.3648879187624128, 2.8084763633029404, 2.63066048405545, 2.8730273514925795, 2.537579734606941, 4.12656957298249, 2.512764942430745, 3.9618867558586612, 2.4335020433731716, 2.7455250855526128, 2.3769364869913594, 2.3692957377635504, 2.3856006200654365, 2.695081321023742, 4.596689348122472, 2.65810230254573, 3.1484872157648702, 2.753861206520673, 2.1780793552476116, 2.352399102820753, 4.295839553612306, 4.222917071753743, 15.899220347072422, 9.855898283618986, 7.107812713843984, 4.042478855160398, 13.345000747888694, 4.497784427212797, 4.639316987065963, 12.86129936404478, 8.66770443315349, 4.908680859991566, 5.3238331193145, 12.456312623598288, 82.3226819870989, 38.65427518583961, 7.073529686740474, 12.898543312286495, 11.953669180469841, 5.743516456674268, 4.9523239055752315, 6.637780266846143, 4.064723160006101, 3.5206952742310977, 3.464974095447758, 3.0820796414242486, 3.0373309932567327, 3.7511403619165264, 2.9482870285258604, 12.481312938442107, 2.660261668502349, 2.5899377639033254, 4.76802703589545, 3.993753906100548, 2.974634313414887, 3.3226851972820493, 3.3968339999281194, 3.743271737693131, 3.2472403052542886, 2.4224814073485197, 2.3807682179605774, 2.3532996705311513, 3.1794015611423014, 2.469462045614478, 2.9080460305228573, 2.226131129416873, 3.6025073922908635, 3.0100886954419286, 2.353854149899626, 5.07943032408893, 6.936502489147093, 6.399028962851631, 9.165083355819387, 3.3296278691476098, 4.765938323131142, 12.535483499824933, 4.030952440310503, 3.9112990911061845, 6.020101339401457, 3.987706696081473, 11.392304320526206, 7.764208194233282, 5.259823834893821, 4.9048179384164285, 4.7823367859309105, 4.754154646263331, 3.605195047919127, 3.4512975033558564, 3.394222283084252, 3.3393574654532316, 3.6202607994358793, 3.18572380207773, 3.287963029951587, 3.782447349536061, 3.300390611763552, 14.263279154232222, 3.289194563149428, 3.467630785936783, 5.677353424409792, 2.5108994449993993, 7.704437195295901, 2.980865862956102, 2.631467977518154, 5.530148463896991, 7.306292654012005, 3.114536062100832, 2.3829776369117455, 2.615201648630958, 8.282282613789322, 5.466212389970077, 2.2890544804976134, 4.669002969081517, 10.110456793417937, 9.214182429528346, 8.75031522151852, 8.361297054448238, 5.241916977275587, 43.91678187917103, 7.750578516178028, 19.76985435792103, 39.584633553178435, 13.815968648709175, 17.172756723359473, 14.782216768587656, 7.152036014947477, 90.17889677584078, 13.621500654483647, 33.25128044186518, 112.77425729207732, 129.01502398563824, 13.795384335806824, 13.646978004998621, 13.619752526313214, 13.268681114457094, 13.245496172738212, 13.595079039844176, 15.764558655580615, 13.67111944338502, 16.116115651997546, 15.530869021172363, 4.705642591671056, 15.967416901104826, 3.4639905079788873, 3.288334381538764, 3.115996981689177, 3.9390517940733596, 2.0970667054652203, 7.245178943895993, 2.027382541780249, 2.242772832148588, 3.4798624680450105, 3.238086792379373, 3.4819779556514607, 2.267261475461546, 2.170068639454922, 3.4194497169617715, 3.690650549938923, 3.335060228335899, 2.244914986566045, 1.7392466009591465, 28.577417130511176, 6.583889332018257, 90.17889677584078, 17.557017502529792, 4.957124806112406, 21.858904405903502, 12.297462924588851, 13.887869733378981, 4.522941947658183, 4.522941947658183, 4.504367628968819, 4.519994775998305, 4.075073034087266, 3.7910806670775314, 3.5211672221770396, 6.368312598099, 4.178683433665667, 3.32370921275554, 3.363814361949062, 3.7297890363407826, 3.2547692743704775, 2.696657326357189, 2.6154902567269387, 2.439170620778027, 2.5480850937220696, 2.6432977088813536, 4.556029317245907, 2.1785577760621404, 2.458452557092995, 2.1755877894079947, 2.7762288751492603, 3.7153387047291515, 2.5512033202795834, 2.3760667872829697, 2.1035484441551278, 3.768517173652841, 2.6802357194418023, 3.027362225879223, 3.0719918726885904, 3.4111052949084106, 8.375003962710647, 6.1596022920890965, 4.557537907321799, 7.33646410278547, 4.614527133312581, 17.348993421567236, 7.284121374441989, 6.718155285883587, 6.197487276915472, 62.94346271315901, 7.921390553216173, 4.534466394431923, 3.9601532212256445, 3.4371353959852993, 3.45683561921175, 3.163239062428768, 3.4808164671367248, 3.542262332962679, 3.29623234701478, 3.195080703826012, 2.7693337492724144, 2.673222553215281, 4.185453233999004, 3.3035756423552214, 2.557676623061374, 2.4158997510278613, 4.860173966558337, 3.0193098945863515, 2.8191031980702124, 2.885792798482163, 3.1206939218969976, 2.2286258320702483, 5.353856612967142, 2.173130380630366, 2.173130380630366, 2.2825078378086427, 4.8167443427973256, 2.324154773065886, 2.6521054734291103, 3.0653599694678118, 2.3877984487887427, 2.594969972394212, 4.0101110680311285, 3.5086958730973516, 11.869299639299033, 10.513763792046396, 4.344415739203373, 6.417784044090946, 4.255058453067798, 15.519739938128929, 14.564645408073833, 17.899991483077173, 80.0423562581471, 7.528254058348915, 8.40062984231931, 3.7852451682222994, 3.6997660763018665, 3.492669030069601, 6.182712187004781, 2.855283623753309, 3.3568473828971537, 3.5156443541076072, 2.6815585493267236, 2.4720066032296257, 2.4299461865771117, 2.890063801795406, 2.5560520853271376, 2.400863213672633, 6.79373469312638, 2.4877838011378803, 3.1889953037037078, 2.2786844122107888, 2.2575949924651937, 4.315077570758595, 3.572677624617962, 2.7349663605894294, 4.071102535753563, 5.112089970024896, 3.1365783436333654, 1.9502987975265085, 3.681487740241419, 3.250072764852323, 3.723345584913091, 1.86379924745242, 7.41100475584238, 9.954641685296785, 6.506214311412645, 4.80838668946196, 13.033017949920001, 7.734657665675422, 14.498872186623181, 6.316125129626406, 6.880869744572679, 5.647833885312512, 5.078685921042354, 4.931662909888702, 4.7076575501555995, 4.800351187910782, 4.81759770870915, 4.009057806706198, 3.7182667840199493, 3.3193005411141496, 3.27762786728465, 4.747766488478549, 2.8119858247389997, 5.870551711821592, 2.5672453435890485, 2.509397927493401, 2.4720023597914373, 2.8662361091597037, 2.718064800935481, 4.345701163371546, 3.6616986066217647, 2.515491874750474, 2.9402050298431215, 2.0936491707686535, 2.5803511315147896, 3.6291547668037603, 6.774628547149449, 6.640843763357386, 2.3271265984346985, 6.718099984126342, 2.584247800316639, 3.2009413071717265, 4.169548182281434, 8.701032201378808, 4.260541008174727, 7.162622400148961, 28.932326984534395, 5.3944687239356, 3.1703424571726817, 5.795515973883686, 2.600897066182137, 2.6336911157979683, 2.289399602795363, 3.2195736981377783, 3.12165381973053, 2.61314433887509, 18.751874368137244, 10.893516871540909, 3.3166428055963433, 3.0602065784520724, 1.933844492465122, 2.3043000696470513, 1.7982838269345198, 3.2975047693516104, 5.1913554782209035, 2.3809020807824366, 2.7076139781178727, 2.678814927256383, 1.8614681834980633, 1.6595460943545122, 4.233754439931039, 3.8563387894248176, 2.6445792558776566, 1.9911063444254553, 4.428154863446588, 4.69581865532516, 2.1806322811804275, 6.9213983660946985, 3.880329504871768, 30.14548846524105, 40.269609584109034, 10.18571389109486, 4.765938323131142, 2.7500875354718852, 2.3944394234736035, 5.7344123701190615, 2.3820913165594977, 4.516239130306519, 4.151794150369223, 4.6303218237237775, 3.0258695310703074, 2.560794974203529, 3.258865076793852, 3.3017545244034263, 4.012321734986891, 3.195849919509141, 3.0651471505605796, 2.396944083167409, 3.5284584819792384, 2.3130650886329733, 3.418206070424929, 3.4710709537887525, 3.200056237033762, 3.267023592113306, 2.155067039228749, 2.2473302763339813, 2.6102334713888653, 1.8534556537818982, 9.465285634840006, 8.059397443655763, 2.060028884450444, 1.6800481657163708, 8.59182886528574, 11.322764069583341, 5.030761530748851, 3.0609745301092013, 25.54381740599238, 7.10685601944605, 4.7752253311835595, 11.15629657054709, 7.116434456124727, 21.710481399033327, 10.740796158048518, 2.354810022585988, 2.194416916181273, 3.7646617419912887, 3.438687765841908, 3.2764279489661545, 2.865272302464418, 3.3886446158720513, 2.433058882148644, 3.517537721268357, 2.402241119908241, 5.277548239743658, 2.3848561277216174, 4.474806422282491, 3.311163226031916, 2.7230847577943833, 5.380088073001811, 10.143975129265122, 2.4790064157013276, 1.8765650230411233, 2.273406968089156, 4.76829683977527, 2.750829750140828, 3.002778582343658, 2.118371899401817, 2.574730907790355, 5.279278706641779, 2.2184879456048585, 1.9122886539332207, 4.814404375353425, 2.3135683979716957, 5.659157237999028, 4.704142385008325, 3.8392958044192644, 5.927594172132925, 5.015785673393754, 16.841984372833686, 8.026149729987551, 14.067291414435662, 37.538218655517404, 20.675157863901788], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0441, 1.0378, 1.032, 1.0244, 1.0216, 1.0154, 1.0136, 1.0104, 1.0093, 1.0069, 1.0008, 0.9938, 0.9904, 0.9902, 0.9877, 0.9836, 0.9833, 0.9819, 0.9798, 0.9779, 0.9758, 0.9754, 0.9749, 0.9743, 0.9739, 0.9738, 0.9728, 0.9726, 0.9713, 0.9651, 0.963, 0.9541, 0.925, 0.9476, 0.9197, 0.9531, 0.9022, 0.8726, 0.8648, 0.7457, 0.7957, 0.8952, 0.4801, 0.669, 0.4605, 0.5399, 0.6697, 0.5603, 0.4047, 0.4793, 0.8269, 0.7252, 0.7555, 0.6532, 0.5284, 0.5445, 0.6324, 0.4917, 0.4884, 0.3567, 0.515, 0.5703, 0.5703, 0.2304, 0.3143, 0.6843, 0.5873, 0.5604, 0.3258, 0.41, 0.3823, 0.4865, 0.084, 0.0562, 0.3152, 0.1873, 0.2015, 1.557, 1.5322, 1.5279, 1.5235, 1.5197, 1.5116, 1.511, 1.5107, 1.5094, 1.5071, 1.5032, 1.5016, 1.5013, 1.4969, 1.4956, 1.4944, 1.4943, 1.4891, 1.4881, 1.4853, 1.484, 1.4839, 1.476, 1.4751, 1.4733, 1.4701, 1.4687, 1.4678, 1.4676, 1.4676, 1.4674, 1.4647, 1.4422, 1.4328, 1.4034, 1.3941, 1.3381, 1.4346, 1.3163, 1.1641, 1.2731, 0.8829, 1.3892, 1.1686, 0.927, 1.0352, 1.2585, 1.2773, 0.8749, 0.6036, 1.1928, 1.1814, 0.6663, 0.2949, 0.8269, 0.1514, 0.8483, 0.122, 1.0123, 0.7946, 0.9179, 0.7157, -0.0432, -0.0757, 0.1527, 0.7253, 0.4431, 0.5393, 0.2348, 0.1748, 0.1195, -0.0811, 0.0766, 0.0613, -0.3659, -0.2057, 2.0164, 2.0097, 2.0009, 1.9907, 1.9894, 1.9849, 1.9829, 1.9817, 1.9794, 1.9791, 1.974, 1.967, 1.9637, 1.9632, 1.9632, 1.9631, 1.9628, 1.9589, 1.9568, 1.9556, 1.9546, 1.9528, 1.9521, 1.9515, 1.9513, 1.9508, 1.9505, 1.9489, 1.9481, 1.9467, 1.9345, 1.9236, 1.9236, 1.9128, 1.9336, 1.9246, 1.8933, 1.8908, 1.8875, 1.8365, 1.7939, 1.8205, 1.8609, 1.7999, 1.8218, 1.6005, 1.6735, 1.7343, 1.7481, 1.7701, 1.797, 1.8062, 1.4261, 1.4987, 1.4171, 1.3714, 1.5928, 0.9703, 1.0246, 0.8245, 1.1287, 1.1114, 1.3347, 0.3713, 0.8331, 2.8804, 2.8367, 2.8235, 2.8219, 2.8086, 2.8045, 2.7991, 2.7987, 2.7986, 2.7982, 2.7935, 2.7863, 2.7843, 2.7815, 2.7683, 2.7652, 2.7642, 2.7611, 2.7514, 2.7507, 2.7453, 2.7436, 2.7381, 2.7323, 2.7292, 2.7205, 2.7201, 2.7136, 2.713, 2.7106, 2.7103, 2.6837, 2.6874, 2.6556, 2.6247, 2.5007, 2.6679, 2.6642, 2.5609, 2.5505, 2.6295, 2.6208, 2.6775, 2.3616, 2.4269, 2.5449, 2.428, 2.2296, 1.6242, 2.3304, 2.2092, 1.2315, 1.7326, 0.753, 1.4112, 1.8323, 1.9828, 2.9793, 2.9654, 2.9501, 2.9447, 2.9426, 2.9419, 2.937, 2.9334, 2.9222, 2.9206, 2.9202, 2.9172, 2.9165, 2.9163, 2.9127, 2.9126, 2.9069, 2.9008, 2.9001, 2.8998, 2.8994, 2.8951, 2.8942, 2.8925, 2.8865, 2.8864, 2.8847, 2.8682, 2.8611, 2.854, 2.8531, 2.787, 2.8151, 2.729, 2.8231, 2.7903, 2.7152, 2.5769, 2.72, 2.3962, 2.1124, 2.6605, 2.4, 2.2292, 2.433, 0.6374, 0.7312, 2.3867, 3.2381, 3.2293, 3.1954, 3.1869, 3.1857, 3.1514, 3.1317, 3.1302, 3.1239, 3.1105, 3.0953, 3.0874, 3.084, 3.0808, 3.0748, 3.0589, 3.0534, 3.0529, 3.0461, 3.0342, 3.0269, 3.0146, 3.012, 3.0095, 3.0084, 3.0056, 3.003, 2.9972, 2.9917, 2.9885, 2.987, 2.9553, 2.954, 2.9402, 2.7477, 2.7231, 2.7147, 2.5836, 2.4968, 2.7086, 2.6176, 2.5098, 2.6893, 2.3614, 2.2672, 2.2953, 1.867, 1.7184, 2.4434, 1.9233, 1.9674, 1.5046, 1.1342, 0.7974, 1.6591, 1.1387, 0.8057, 0.3848, 3.5809, 3.5557, 3.529, 3.5131, 3.5031, 3.4948, 3.4888, 3.4832, 3.4777, 3.4581, 3.4569, 3.4552, 3.449, 3.4457, 3.4389, 3.4326, 3.4292, 3.4251, 3.4216, 3.4163, 3.4138, 3.4071, 3.3878, 3.3874, 3.3761, 3.373, 3.372, 3.3656, 3.3655, 3.3582, 3.3542, 3.3297, 3.3256, 3.2686, 3.3434, 3.005, 2.8598, 2.89, 3.1027, 2.5213, 2.9671, 2.9356, 2.929, 2.9183, 1.9018, 2.6697, 1.7932, 3.797, 3.7175, 3.655, 3.6461, 3.646, 3.6109, 3.5913, 3.5627, 3.5525, 3.5504, 3.5484, 3.5451, 3.542, 3.5375, 3.5252, 3.5071, 3.4879, 3.4723, 3.4639, 3.4542, 3.4496, 3.4402, 3.4393, 3.439, 3.4381, 3.4366, 3.4346, 3.4292, 3.4276, 3.4177, 3.4012, 3.3872, 3.3492, 3.0221, 3.3309, 3.2642, 2.9324, 3.0878, 2.4277, 3.0733, 2.9909, 1.4746, 2.5754, 3.1765, 2.9835, 1.8745, 1.613, 2.5278, 1.9712, 2.458, 2.5475, 2.4749, 1.0636, 3.8519, 3.8517, 3.8417, 3.8289, 3.8187, 3.8186, 3.8105, 3.7997, 3.7903, 3.7695, 3.7598, 3.7577, 3.7543, 3.7528, 3.7062, 3.7011, 3.6826, 3.6727, 3.6711, 3.66, 3.6354, 3.6342, 3.6295, 3.6284, 3.6199, 3.618, 3.6161, 3.613, 3.6064, 3.5838, 3.5835, 3.5063, 3.4414, 3.1883, 3.4269, 3.2684, 3.3424, 3.4965, 3.3513, 3.2385, 3.1983, 2.3948, 2.6918, 2.3386, 2.0514, 4.0485, 3.9441, 3.9362, 3.9116, 3.9058, 3.8961, 3.8858, 3.8826, 3.8627, 3.8592, 3.8574, 3.8472, 3.8186, 3.8138, 3.8016, 3.798, 3.7968, 3.7962, 3.7807, 3.7781, 3.7683, 3.7579, 3.753, 3.7522, 3.7421, 3.7416, 3.7353, 3.7335, 3.7183, 3.7168, 3.7066, 3.6632, 3.521, 3.3754, 3.3875, 3.5716, 3.0761, 3.4954, 3.4702, 2.8973, 3.0846, 3.4126, 3.3543, 2.7855, 1.5042, 2.0159, 3.1531, 2.6852, 2.6664, 4.1072, 4.0799, 4.0196, 4.0114, 3.9935, 3.99, 3.9501, 3.9449, 3.939, 3.9335, 3.9006, 3.8904, 3.8781, 3.8731, 3.8688, 3.8588, 3.857, 3.8525, 3.8492, 3.8456, 3.8451, 3.8312, 3.8298, 3.8157, 3.8113, 3.8098, 3.798, 3.7912, 3.7876, 3.7844, 3.7616, 3.734, 3.6745, 3.5024, 3.7264, 3.5434, 2.9715, 3.6174, 3.626, 3.3507, 3.6023, 2.6336, 2.9515, 4.168, 4.1537, 4.1351, 4.1241, 4.0785, 4.0652, 4.0599, 4.0546, 4.0463, 4.0386, 4.0335, 3.9954, 3.9931, 3.9809, 3.9555, 3.9512, 3.8876, 3.8725, 3.8598, 3.8498, 3.8463, 3.8072, 3.8036, 3.8019, 3.7836, 3.7654, 3.7626, 3.7598, 3.7562, 3.745, 3.7215, 3.5954, 3.587, 3.5557, 3.6181, 2.7785, 3.4333, 3.0108, 2.5853, 3.1233, 2.9411, 2.9645, 3.4535, 1.2049, 2.7929, 1.9414, 0.7616, 0.5748, 4.2904, 4.2897, 4.2896, 4.2879, 4.2878, 4.2814, 4.2705, 4.263, 4.1643, 4.1609, 4.1592, 4.1243, 4.0795, 4.0629, 4.0441, 3.9487, 3.8472, 3.8372, 3.8289, 3.8198, 3.8145, 3.808, 3.7978, 3.7622, 3.7177, 3.6573, 3.6205, 3.5817, 3.567, 3.5369, 3.3994, 3.2202, 2.0837, 2.7483, 3.2916, 2.2193, 2.6067, 2.2946, 4.162, 4.162, 4.161, 4.1514, 4.1371, 4.1174, 4.0826, 4.0771, 4.076, 4.0743, 4.0246, 4.0244, 4.013, 3.9979, 3.9739, 3.9502, 3.9429, 3.9425, 3.8981, 3.8869, 3.8868, 3.8859, 3.8846, 3.8828, 3.8769, 3.8685, 3.8641, 3.8639, 3.8457, 3.8334, 3.8155, 3.7842, 3.5588, 3.6334, 3.6808, 3.5038, 3.6431, 3.0819, 3.3651, 3.3951, 3.3675, 1.783, 3.0369, 3.4746, 4.138, 4.097, 4.0711, 4.0692, 4.0544, 4.0496, 4.0421, 4.0396, 4.0175, 4.0013, 4.0004, 3.9893, 3.9816, 3.9418, 3.9392, 3.9386, 3.9363, 3.9285, 3.9194, 3.9084, 3.8942, 3.8932, 3.8932, 3.8838, 3.8827, 3.8739, 3.8707, 3.8593, 3.8499, 3.8488, 3.8049, 3.7771, 3.3643, 3.3893, 3.6149, 3.43, 3.5182, 2.6242, 2.6633, 2.3856, 0.7028, 2.9553, 4.2327, 4.1753, 4.1687, 4.1205, 4.1127, 4.08, 4.0713, 4.0587, 4.0322, 4.0148, 4.0061, 4.0027, 4.0024, 3.9998, 3.9949, 3.991, 3.9797, 3.9713, 3.966, 3.9188, 3.9144, 3.9035, 3.8866, 3.884, 3.8718, 3.8707, 3.8696, 3.8495, 3.8479, 3.8361, 3.7617, 3.5595, 3.6025, 3.6326, 2.7953, 3.1082, 2.6054, 3.2551, 3.1565, 3.284, 4.3861, 4.3803, 4.3707, 4.3659, 4.3451, 4.333, 4.3125, 4.2777, 4.2678, 4.2553, 4.2162, 4.1923, 4.1732, 4.1649, 4.1531, 4.1299, 4.1189, 4.1168, 4.1025, 4.0823, 4.0513, 4.0476, 4.0209, 4.0177, 4.0137, 3.9863, 3.9656, 3.9602, 3.906, 3.8943, 3.8857, 3.7834, 3.7635, 3.4717, 1.7683, 4.4279, 4.3111, 4.2539, 4.1845, 4.1749, 4.1656, 4.1473, 4.1422, 4.1374, 4.1119, 4.0964, 4.075, 4.0666, 4.0562, 4.0027, 3.9985, 3.9946, 3.9913, 3.9665, 3.9383, 3.9291, 3.9012, 3.8581, 3.828, 3.8136, 3.7723, 3.7553, 3.7293, 3.7242, 3.6424, 3.5042, 3.5466, 2.7618, 1.8774, 2.6259, 3.2158, 4.2781, 4.2123, 4.163, 4.1521, 4.0292, 4.011, 3.9939, 3.9829, 3.9474, 3.94, 3.9225, 3.8983, 3.8361, 3.8149, 3.7907, 3.765, 3.7089, 3.7021, 3.7016, 3.6982, 3.6794, 3.6478, 3.5683, 3.5642, 3.5529, 3.527, 3.5182, 3.5119, 3.4777, 3.4597, 3.4271, 3.3633, 3.446, 2.9701, 3.2462, 3.3411, 2.9687, 3.0416, 2.2438, 2.7262, 4.1884, 4.1287, 4.079, 4.0589, 4.0567, 4.0382, 3.9928, 3.982, 3.9803, 3.9735, 3.9191, 3.8747, 3.8169, 3.8155, 3.7869, 3.7754, 3.7512, 3.7205, 3.6652, 3.6451, 3.6425, 3.6204, 3.6164, 3.6086, 3.6072, 3.5461, 3.4792, 3.4782, 3.4658, 3.4008, 3.3899, 3.3267, 3.3466, 3.1771, 3.1572, 2.355, 2.7599, 2.1229, 1.1994, 1.6842], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.3728, -6.735, -6.8895, -7.1662, -7.1863, -7.2471, -7.3567, -7.2117, -5.8993, -7.4467, -7.5418, -7.5551, -7.6781, -7.671, -7.7101, -7.5146, -7.4951, -7.2068, -7.7975, -7.7788, -7.4673, -7.4237, -7.7972, -7.8551, -7.7249, -7.4518, -7.4619, -7.496, -7.886, -7.9457, -7.5213, -7.397, -6.8953, -7.3903, -7.0434, -7.4639, -7.057, -6.9164, -6.9381, -6.3111, -6.6637, -7.1881, -5.4764, -6.2802, -5.467, -5.7705, -6.3248, -6.0118, -5.5118, -5.7842, -6.9533, -6.6237, -6.7249, -6.4018, -6.0069, -6.1508, -6.4097, -6.0663, -6.0608, -5.7556, -6.145, -6.3182, -6.3268, -5.7136, -5.9068, -6.5815, -6.4235, -6.3838, -6.0392, -6.196, -6.1942, -6.326, -6.0074, -6.0453, -6.2964, -6.343, -6.3774, -6.5561, -6.6372, -7.0364, -6.7748, -7.0081, -7.1776, -6.8438, -7.2183, -7.2577, -6.9983, -7.323, -7.3393, -7.016, -6.7577, -6.8994, -7.0814, -7.2681, -7.4583, -7.4513, -7.1243, -7.5033, -7.1481, -6.7266, -7.4566, -6.5223, -7.6188, -7.6293, -7.3422, -7.6383, -6.9083, -7.6123, -7.3547, -6.5802, -7.0412, -6.7407, -6.7298, -6.465, -7.1403, -6.4702, -5.8574, -6.472, -5.2085, -6.9904, -6.3768, -5.8783, -6.1838, -6.699, -6.7921, -6.2002, -5.9753, -6.7363, -6.7406, -6.1968, -5.8173, -6.369, -5.7761, -6.4125, -5.8345, -6.5934, -6.4387, -6.5257, -6.4159, -5.9872, -5.9922, -6.1577, -6.4447, -6.3388, -6.4431, -6.3769, -6.3973, -6.4313, -6.4461, -6.4725, -6.474, -6.4674, -6.4692, -5.1823, -6.0552, -5.7565, -6.6591, -5.4403, -6.668, -6.3566, -6.6228, -5.938, -6.7918, -6.1354, -6.9158, -6.5073, -6.3214, -6.9515, -6.0585, -5.6385, -6.7268, -5.9297, -6.9691, -6.8891, -6.037, -6.6336, -6.6792, -6.9199, -5.2807, -6.6553, -6.5423, -6.7144, -7.0944, -5.3518, -4.6617, -5.4429, -4.9571, -6.104, -5.859, -5.564, -5.7549, -5.8289, -5.4944, -5.1505, -5.4013, -5.9297, -5.5962, -5.7741, -4.6356, -5.0249, -5.3444, -5.4687, -5.5608, -5.6761, -5.7472, -5.2917, -5.4438, -5.371, -5.3904, -5.6129, -5.1312, -5.1965, -5.1195, -5.4016, -5.4651, -5.5544, -5.5451, -5.6266, -5.6318, -6.1174, -5.8433, -5.8909, -5.6549, -4.6958, -5.3875, -6.4163, -5.7614, -6.2922, -6.4511, -6.3047, -5.8581, -6.5298, -6.61, -6.5088, -6.1729, -6.378, -5.7557, -6.7095, -5.9582, -6.5379, -6.1228, -6.5192, -5.2053, -6.8632, -6.7208, -6.6557, -6.8983, -6.9095, -5.7679, -5.0709, -5.653, -5.2329, -5.065, -4.0501, -5.7835, -5.819, -5.0313, -5.0321, -5.5787, -5.7486, -6.0239, -4.9452, -5.334, -5.6795, -5.4707, -5.6264, -5.1578, -5.7361, -5.6977, -5.3802, -5.5536, -5.3384, -5.6687, -5.7333, -5.7562, -4.8484, -5.3199, -4.7582, -5.6324, -5.6598, -5.15, -5.7297, -5.7718, -5.5898, -5.0447, -5.7885, -4.8565, -5.9307, -5.8057, -4.3141, -5.983, -6.0412, -5.8763, -6.101, -6.104, -5.0276, -6.0613, -6.1508, -6.165, -6.2122, -6.213, -5.9913, -6.2368, -5.6186, -6.4408, -6.1876, -4.2586, -5.5302, -4.569, -5.6629, -5.5146, -5.272, -4.9464, -5.683, -5.1723, -4.6719, -5.7404, -5.514, -5.5755, -5.756, -5.4749, -5.5322, -5.7558, -5.3453, -5.6174, -5.1881, -5.8701, -5.448, -5.4768, -5.8623, -6.3301, -6.3633, -6.4322, -6.5048, -6.3834, -6.5572, -6.1502, -6.1837, -6.0827, -6.5422, -6.6554, -6.6621, -6.7642, -6.7921, -6.7279, -6.1214, -6.8563, -6.225, -6.4066, -6.8795, -6.3181, -6.9155, -6.6393, -6.4752, -4.4716, -5.7461, -5.9371, -5.0128, -5.0265, -5.1874, -5.2402, -5.0182, -5.7801, -5.8511, -5.7638, -6.0117, -5.6939, -5.6921, -5.7261, -5.3826, -5.3322, -5.9274, -5.6605, -5.8034, -5.6563, -5.5835, -5.5676, -5.7796, -5.6805, -5.7796, -5.8513, -5.3824, -5.1687, -5.4914, -5.8679, -5.9252, -5.9708, -5.8137, -5.4385, -5.8978, -5.2196, -6.1107, -6.1652, -5.2653, -6.0, -6.2053, -6.2681, -6.2825, -6.2995, -6.3139, -6.336, -6.3147, -5.8767, -6.4464, -6.4481, -6.1261, -6.2676, -6.377, -6.527, -6.5189, -6.5524, -6.3147, -5.4838, -5.5824, -5.6285, -6.3183, -5.4488, -5.5094, -5.6628, -6.0292, -5.5064, -5.9392, -6.0104, -6.1187, -6.1315, -5.9937, -6.1533, -6.1379, -4.8643, -5.5881, -5.7779, -5.9718, -5.567, -5.9721, -6.2032, -5.8964, -6.3457, -6.3529, -6.36, -6.2941, -6.3818, -6.3178, -6.3764, -6.496, -6.1554, -6.388, -6.3656, -6.6536, -6.3755, -6.6041, -6.6955, -6.6962, -6.5867, -6.7028, -6.2226, -6.1728, -6.3388, -6.7537, -6.3448, -6.2127, -6.2073, -5.5313, -6.392, -6.2817, -5.7087, -6.076, -5.3359, -6.1191, -6.0594, -4.985, -5.7997, -6.2143, -6.1507, -5.7308, -5.67, -6.0123, -5.8951, -6.071, -6.1052, -6.0908, -6.1255, -3.8392, -4.3595, -4.7339, -4.5772, -5.0008, -4.6568, -3.9111, -5.1591, -4.8374, -5.3882, -4.9645, -5.5, -5.5223, -5.5325, -5.8013, -5.8276, -5.9178, -5.9511, -5.9713, -5.4374, -6.1234, -6.1282, -4.7631, -5.9381, -6.1843, -4.1592, -6.1989, -6.2108, -6.2352, -5.9869, -4.9111, -3.4939, -4.6548, -4.0252, -5.2935, -4.8118, -5.1129, -5.6613, -5.4061, -5.3834, -5.445, -4.9856, -5.2232, -5.0944, -4.9701, -5.1175, -5.5876, -5.1333, -5.844, -5.732, -5.9052, -5.9443, -5.927, -5.7541, -5.3269, -5.8906, -6.0816, -6.1756, -6.0923, -6.2286, -5.746, -6.2432, -5.7885, -6.2914, -6.1734, -6.3273, -6.341, -6.3389, -6.2178, -5.694, -6.2422, -6.0792, -6.215, -6.4647, -6.3892, -5.7972, -5.8577, -4.6742, -5.2979, -5.6128, -5.993, -5.2942, -5.9624, -5.9567, -5.5099, -5.7172, -5.9578, -5.9349, -5.6537, -5.0466, -5.2908, -5.852, -5.7192, -5.814, -5.1062, -5.2816, -5.0491, -5.5477, -5.7092, -5.7287, -5.8857, -5.9056, -5.7004, -5.9467, -4.5366, -6.0926, -6.1317, -5.5265, -5.7079, -6.0125, -5.9037, -5.8861, -5.7923, -5.938, -6.2315, -6.2628, -6.2759, -5.989, -6.2461, -6.0841, -6.3632, -5.8886, -6.0718, -6.321, -5.5746, -5.2906, -5.4308, -5.2436, -6.0322, -5.8565, -5.4613, -5.95, -5.9716, -5.8156, -5.976, -5.8949, -5.9604, -5.1333, -5.2175, -5.2614, -5.2783, -5.6005, -5.6575, -5.6795, -5.7011, -5.6286, -5.7642, -5.7377, -5.6357, -5.7743, -4.3228, -5.8153, -5.7668, -5.3373, -6.1683, -5.0598, -6.0194, -6.1476, -5.444, -5.1691, -6.0235, -6.3095, -6.2347, -5.0847, -5.503, -6.3771, -5.6755, -4.9264, -5.1452, -5.2054, -5.2822, -5.6867, -4.4007, -5.4804, -4.9665, -4.6977, -5.2123, -5.177, -5.3035, -5.5406, -5.2547, -5.5569, -5.5159, -5.4745, -5.5267, -4.0467, -4.0582, -4.0603, -4.0882, -4.09, -4.0704, -3.9332, -4.0832, -4.0174, -4.0577, -5.2535, -4.0666, -5.6395, -5.7081, -5.7808, -5.6418, -6.3737, -5.1439, -6.4257, -6.3339, -5.9, -5.9784, -5.916, -6.3807, -6.469, -6.0747, -6.0351, -6.1752, -6.5858, -6.8711, -4.2094, -5.8566, -4.376, -5.3476, -6.069, -5.6575, -5.8454, -6.0358, -5.2903, -5.2903, -5.2954, -5.3015, -5.4195, -5.5114, -5.6201, -5.033, -5.4554, -5.6861, -5.7238, -5.6206, -5.7683, -5.9715, -6.0261, -6.1196, -6.0832, -6.0469, -5.5469, -6.2959, -6.1751, -6.2982, -6.0558, -5.7662, -6.148, -6.2275, -6.3537, -5.7709, -6.1298, -6.0204, -6.0236, -5.9502, -5.2774, -5.5101, -5.7639, -5.4648, -5.7891, -5.026, -5.6106, -5.6615, -5.7697, -5.0362, -5.855, -5.9752, -5.4471, -5.6298, -5.65, -5.7406, -5.6598, -5.6471, -5.7265, -5.7603, -5.9253, -5.9768, -5.5295, -5.7771, -6.0407, -6.1376, -5.4411, -5.9178, -5.9887, -5.9731, -5.904, -6.2516, -5.3895, -6.2921, -6.2921, -6.2523, -5.5066, -6.2442, -6.1154, -5.982, -6.2411, -6.1591, -5.7677, -5.9291, -5.1232, -5.2195, -5.8777, -5.6724, -5.9951, -5.5951, -5.6195, -5.691, -5.876, -5.9875, -4.6004, -5.4551, -5.4845, -5.5903, -5.027, -5.8323, -5.6792, -5.6455, -5.9429, -6.0416, -6.0675, -5.8975, -6.0205, -6.0858, -5.0505, -6.0591, -5.822, -6.1665, -6.1811, -5.5806, -5.7737, -6.0518, -5.6709, -5.4458, -5.9465, -6.4227, -5.7885, -5.9332, -5.7989, -6.5028, -5.1967, -5.1039, -5.4862, -5.7585, -5.5986, -5.8076, -5.6819, -5.8632, -5.8762, -5.9462, -4.9503, -4.9855, -5.0416, -5.0268, -5.0441, -5.2399, -5.3356, -5.484, -5.5066, -5.1485, -5.7113, -4.9992, -5.8454, -5.8765, -5.9033, -5.7785, -5.8427, -5.3754, -5.561, -5.9567, -5.8317, -6.1749, -5.9926, -5.6547, -5.0346, -5.0819, -6.1512, -5.0964, -6.106, -5.9037, -5.648, -5.0146, -5.7485, -5.5208, -5.8281, -4.8482, -5.4965, -4.9504, -5.8211, -5.8181, -5.9675, -5.6449, -5.6809, -5.8635, -3.9182, -4.4769, -5.6875, -5.7763, -6.2457, -6.124, -6.3761, -5.7737, -5.3232, -6.1275, -6.0271, -6.047, -6.4388, -6.5968, -5.6904, -5.7982, -6.2166, -6.5174, -5.7441, -5.6906, -6.5394, -5.5226, -6.0589, -4.7936, -5.3885, -6.0146, -6.1841, -5.6717, -5.876, -5.052, -5.9413, -5.4246, -5.5269, -5.4349, -5.8714, -6.0737, -5.84, -5.8444, -5.6738, -5.9635, -6.0264, -6.2965, -5.9356, -6.414, -6.0302, -6.0154, -6.1001, -6.0981, -6.5458, -6.5834, -6.4378, -6.7915, -5.1868, -5.3564, -6.7268, -6.9649, -5.351, -5.1075, -5.9826, -6.3967, -4.7509, -5.7542, -6.0569, -5.5808, -5.9574, -5.6398, -5.8612, -5.9166, -6.0468, -5.5568, -5.6675, -5.718, -5.8706, -5.7482, -6.0903, -5.7234, -6.1116, -5.3789, -6.2176, -5.6461, -5.9486, -6.1728, -5.5033, -4.8933, -6.3331, -6.6668, -6.4951, -5.757, -6.3291, -6.2455, -6.6022, -6.4085, -5.7516, -6.6854, -6.835, -5.924, -6.7219, -5.8383, -6.0862, -6.2695, -6.0047, -6.1917, -5.7826, -6.1188, -6.1947, -6.1367, -6.2483]}, \"token.table\": {\"Topic\": [10, 19, 2, 14, 2, 15, 19, 4, 8, 16, 6, 11, 1, 2, 3, 6, 8, 9, 12, 16, 4, 7, 9, 6, 1, 3, 6, 12, 14, 1, 9, 18, 7, 20, 1, 2, 19, 20, 1, 3, 9, 12, 4, 5, 2, 16, 1, 1, 5, 6, 8, 15, 16, 2, 5, 19, 3, 10, 10, 4, 12, 19, 1, 3, 8, 3, 4, 5, 7, 8, 14, 2, 4, 2, 4, 11, 6, 1, 2, 2, 10, 1, 1, 2, 3, 4, 6, 14, 7, 8, 15, 7, 11, 2, 4, 1, 4, 7, 1, 4, 3, 6, 12, 11, 2, 19, 8, 1, 13, 8, 16, 2, 5, 15, 2, 1, 11, 15, 20, 1, 4, 2, 4, 1, 2, 3, 4, 5, 2, 5, 7, 12, 15, 18, 18, 19, 2, 4, 8, 10, 1, 11, 5, 7, 18, 18, 3, 4, 10, 6, 8, 10, 12, 1, 15, 8, 10, 2, 2, 14, 7, 11, 1, 2, 3, 4, 8, 9, 20, 1, 2, 3, 4, 8, 20, 4, 17, 1, 2, 4, 13, 5, 1, 16, 1, 2, 8, 14, 20, 14, 2, 5, 3, 17, 1, 3, 14, 15, 18, 13, 2, 5, 17, 2, 19, 8, 18, 17, 6, 17, 7, 1, 2, 15, 1, 2, 3, 6, 16, 15, 1, 6, 1, 2, 3, 4, 5, 8, 16, 14, 4, 2, 8, 14, 6, 7, 11, 13, 3, 11, 16, 7, 16, 1, 12, 13, 6, 6, 14, 1, 2, 3, 8, 15, 2, 20, 17, 2, 3, 5, 1, 2, 4, 1, 3, 6, 9, 1, 3, 6, 9, 6, 13, 16, 18, 1, 2, 6, 11, 15, 2, 4, 10, 1, 2, 4, 7, 10, 2, 4, 8, 2, 4, 8, 10, 2, 4, 8, 10, 3, 12, 9, 1, 2, 20, 2, 1, 9, 10, 7, 13, 16, 15, 1, 2, 9, 5, 8, 18, 4, 16, 11, 10, 8, 1, 2, 3, 6, 1, 8, 1, 12, 15, 1, 2, 3, 5, 6, 7, 8, 9, 11, 14, 1, 13, 17, 6, 11, 7, 8, 7, 1, 2, 3, 1, 2, 3, 6, 11, 7, 8, 9, 11, 2, 1, 2, 3, 6, 3, 6, 6, 11, 10, 8, 4, 8, 1, 2, 3, 4, 5, 2, 15, 13, 2, 9, 15, 9, 9, 9, 2, 8, 14, 20, 2, 3, 11, 1, 2, 3, 5, 13, 14, 1, 2, 4, 5, 6, 8, 11, 12, 17, 7, 8, 5, 20, 10, 19, 1, 2, 4, 5, 8, 7, 19, 11, 1, 2, 4, 20, 1, 20, 15, 16, 14, 7, 1, 2, 4, 6, 8, 18, 18, 7, 15, 13, 16, 9, 9, 1, 2, 6, 3, 15, 17, 4, 17, 7, 1, 5, 19, 20, 6, 11, 1, 2, 3, 4, 5, 15, 1, 2, 3, 4, 5, 2, 14, 3, 12, 8, 1, 3, 6, 8, 20, 1, 15, 1, 2, 6, 16, 17, 1, 5, 1, 2, 3, 4, 5, 6, 8, 12, 20, 1, 2, 3, 4, 5, 11, 2, 18, 1, 2, 3, 4, 5, 8, 9, 1, 3, 6, 6, 4, 5, 7, 12, 16, 6, 1, 2, 3, 6, 17, 1, 3, 6, 18, 1, 13, 14, 2, 9, 19, 1, 1, 2, 4, 7, 19, 1, 7, 14, 1, 6, 17, 1, 2, 3, 4, 6, 8, 9, 12, 13, 14, 16, 18, 19, 9, 1, 6, 1, 2, 3, 6, 8, 12, 13, 9, 1, 9, 2, 4, 10, 11, 19, 2, 9, 1, 1, 4, 14, 9, 9, 1, 20, 5, 1, 4, 3, 1, 2, 4, 12, 1, 2, 4, 1, 5, 6, 8, 1, 4, 18, 18, 1, 12, 10, 1, 2, 4, 11, 11, 1, 2, 4, 19, 1, 5, 1, 5, 4, 19, 7, 1, 2, 4, 10, 1, 2, 1, 2, 20, 17, 2, 1, 3, 6, 9, 12, 1, 3, 6, 18, 1, 3, 6, 19, 14, 7, 16, 6, 12, 3, 6, 15, 1, 11, 12, 18, 11, 1, 3, 6, 1, 12, 2, 1, 8, 1, 8, 15, 3, 6, 9, 12, 14, 1, 10, 4, 20, 14, 7, 16, 1, 2, 3, 5, 6, 1, 3, 5, 6, 7, 2, 14, 3, 4, 7, 13, 2, 15, 3, 1, 14, 18, 1, 2, 4, 10, 19, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 8, 1, 13, 13, 1, 2, 8, 9, 13, 15, 1, 2, 4, 9, 9, 13, 1, 2, 3, 6, 9, 12, 7, 2, 4, 8, 18, 14, 1, 2, 3, 4, 8, 9, 15, 16, 1, 2, 4, 6, 7, 14, 7, 17, 18, 19, 5, 14, 4, 5, 20, 1, 11, 5, 2, 4, 10, 17, 1, 2, 4, 5, 9, 1, 2, 3, 6, 18, 7, 6, 18, 14, 1, 2, 5, 11, 5, 2, 10, 2, 7, 10, 7, 2, 4, 7, 2, 6, 11, 10, 1, 3, 11, 15, 7, 11, 15, 15, 2, 14, 14, 19, 1, 12, 1, 3, 12, 11, 19, 3, 6, 13, 15, 1, 8, 2, 16, 15, 1, 2, 3, 6, 4, 12, 1, 2, 3, 6, 12, 11, 14, 6, 17, 4, 16, 15, 1, 3, 19, 1, 1, 19, 13, 11, 1, 2, 3, 6, 14, 11, 3, 7, 3, 6, 4, 4, 2, 16, 2, 4, 18, 2, 4, 2, 4, 10, 2, 3, 20, 1, 4, 20, 7, 13, 16, 4, 2, 4, 10, 4, 2, 4, 5, 2, 4, 1, 1, 7, 14, 20, 1, 2, 18, 2, 6, 4, 2, 5, 1, 18, 1, 2, 3, 4, 5, 6, 7, 15, 7, 2, 5, 8, 15, 1, 2, 9, 1, 9, 1, 2, 2, 4, 2, 10, 13, 7, 1, 2, 3, 4, 5, 6, 12, 2, 8, 4, 8, 13, 2, 6, 17, 6, 8, 1, 2, 4, 9, 10, 5, 1, 5, 18, 10, 4, 1, 2, 5, 8, 7, 13, 1, 2, 3, 5, 7, 10, 1, 2, 4, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 14, 16, 2, 10, 18, 8, 1, 2, 3, 4, 5, 14, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 14, 18, 1, 2, 3, 5, 8, 7, 8, 13, 1, 3, 1, 2, 3, 6, 8, 12, 1, 2, 3, 6, 8, 16, 1, 14, 1, 2, 3, 4, 5, 6, 1, 5, 6, 12, 1, 3, 6, 11, 3, 6, 1, 2, 12, 12, 4, 8, 2, 3, 12, 1, 8, 6, 1, 3, 18, 17, 1, 2, 13, 14, 15, 1, 2, 2, 15, 8, 5, 12, 11, 3, 8, 10, 2, 1, 3, 17, 5, 8, 13, 8, 15, 3, 6, 3, 2, 3, 5, 7, 8, 17, 1, 2, 3, 1, 3, 6, 3, 19, 5, 2, 7, 20, 1, 2, 4, 4, 15, 7, 15, 8, 4, 8, 1, 2, 3, 10, 1, 19, 15, 1, 3, 1, 3, 8, 12, 1, 4, 14, 16, 12, 10, 1, 2, 7, 13, 1, 2, 5, 2, 9, 9, 1, 2, 3, 5, 6, 12, 7, 1, 3, 14, 2, 14, 1, 2, 3, 5, 6, 8, 12, 1, 2, 3, 5, 8, 5, 9, 13, 3, 1, 9, 15, 15, 17, 17, 1, 2, 4, 15, 1, 6, 14, 19, 13, 12, 15, 1, 2, 3, 6, 2, 3, 6, 2, 19, 12, 17, 2, 7, 11, 11, 18, 6, 12, 7, 1, 2, 3, 4, 6, 8, 2, 4, 8, 17, 6, 18, 16, 6, 20, 4, 19, 19, 4, 20, 1, 20, 14, 6, 1, 3, 2, 8, 1, 2, 4, 5, 6, 19, 1, 2, 3, 4, 5, 15, 2, 12, 19, 12, 1, 9, 10, 17, 12, 5, 1, 2, 3, 5, 6, 8, 9, 14, 1, 8, 15, 16, 14, 16, 17, 5, 5, 13, 2, 5, 1, 12, 9, 9, 1, 10, 2, 10, 1, 2, 3, 5, 2, 5, 1, 3, 5, 2, 5, 5, 12, 2, 2, 15, 16, 1, 3, 6, 1, 19, 3, 2, 6, 6, 12, 1, 2, 3, 6, 10, 16, 6, 19, 10, 2, 7, 1, 2, 3, 5, 8, 1, 2, 3, 6, 8, 3, 6, 6, 1, 9, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 6, 15, 4, 6, 7, 1, 2, 3, 6, 12, 6, 19, 1, 2, 4, 9, 11, 8, 13, 17, 17, 5, 3, 12, 1, 2, 3, 9, 12, 5, 8, 20, 1, 3, 6, 17, 8, 8, 2, 2, 7, 15, 1, 3, 6, 1, 16, 1, 5, 4, 19, 10, 7, 1, 2, 3, 4, 5, 6, 8, 1, 11, 16, 1, 12, 8, 2, 6, 18, 4, 11, 1, 2, 4, 1, 2, 4, 8, 7, 14, 6, 1, 8, 1, 1, 1, 13, 1, 2, 3, 4, 5, 7, 1, 2, 4, 5, 6, 9, 19, 7, 3, 4, 5, 8, 9, 10, 16, 7, 17, 4, 16, 18, 9, 2, 4, 11, 1, 3, 5, 2, 10, 15, 1, 2, 3, 4, 5, 6, 10, 8, 1, 3, 6, 4, 20, 1, 2, 3, 5, 19, 14, 16, 10, 2, 20, 1, 4, 19, 16, 1, 2, 3, 4, 5, 1, 9, 2, 3, 8, 19, 20, 1, 2, 4, 8, 11, 8, 1, 16, 5, 1, 3, 6, 1, 3, 6, 11, 2, 5, 16, 1, 9, 1, 9, 1, 2, 9, 1, 3, 6, 2, 19, 1, 2, 3, 5, 6, 8, 12, 2, 1, 3, 12, 2, 7, 8, 2, 7, 1, 11, 17, 13, 1, 8, 12, 5, 12, 1, 1, 3, 6, 16, 7, 1, 6, 14, 15, 1, 14, 1, 3, 2, 4, 10, 13, 9, 8, 11, 15, 1, 3, 6, 2, 2, 4, 10, 1, 2, 6, 13, 2, 5, 1, 2, 6, 7, 8, 18, 2, 7, 8, 1, 10, 17, 20, 14, 11, 7, 4, 11, 13, 1, 2, 4, 14, 3, 5, 11, 6, 10, 20, 16, 8, 17, 7, 13, 4, 6, 1, 2, 3, 6, 18, 20, 4, 4, 16, 4, 5, 20, 7, 14, 1, 7, 20, 9, 17, 3, 7, 16, 2, 13, 7, 2, 20, 3, 18, 8, 20, 3, 5, 14, 15, 4, 2, 5, 7, 2, 5, 8, 11, 14, 1, 2, 4, 19, 7, 10, 10, 12, 2, 11, 1, 2, 3, 19, 6, 1, 2, 3, 5, 6, 8, 12, 16, 19, 1, 2, 20, 7, 20, 1, 2, 3, 5, 6, 1, 2, 3, 4, 18, 1, 2, 3, 4, 5, 6, 8, 10, 15, 1, 1, 2, 3, 4, 5, 6, 11, 1, 11, 1, 2, 7, 12, 1, 2, 4, 5, 12, 2, 17, 7, 19, 17, 20, 4, 16, 17, 1, 3, 5, 10, 1, 7, 16, 6, 16, 4, 19, 16, 2, 10, 10, 1, 2, 3, 4, 5, 6, 1, 2, 4, 2, 4, 1, 2, 3, 12, 11, 20, 7, 4, 6, 15, 16, 17, 10, 1, 2, 3, 5, 8, 16, 3, 16, 1, 2, 3, 6, 7, 9, 1, 2, 3, 4, 6, 9, 1, 2, 3, 6, 2, 6, 14, 2, 6, 8, 14, 16, 1, 3, 15, 1, 2, 17, 7, 1, 7, 8, 8, 10, 1, 2, 3, 4, 6, 6, 9, 3, 6, 1, 3, 6, 4, 1, 11, 10, 1, 7, 19, 6, 19, 10, 3, 2, 6, 15, 14, 1, 2, 3, 4, 6, 1, 2, 4, 18, 2, 4, 10, 18, 2, 20, 1, 2, 3, 4, 5, 15, 8, 3, 6, 6, 1, 2, 3, 5, 1, 3, 6, 12, 1, 3, 6, 14, 2, 16, 9, 1, 2, 1, 2, 3, 4, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 12, 12, 12, 12, 18, 6, 3, 12, 7, 19, 2, 16, 13, 1, 2, 4, 5, 7, 1, 2, 3, 5, 6, 20, 1, 2, 4, 5, 6, 7, 13, 1, 18, 1, 6, 8, 12], \"Freq\": [0.6961298154578901, 0.44497242373794615, 0.8219156739292406, 0.7462030244177661, 0.24923225654616335, 0.24923225654616335, 0.4984645130923267, 0.7732448581382317, 0.08591609534869242, 0.08591609534869242, 0.9158439971435844, 0.8705468222676959, 0.1515739685183528, 0.10104931234556853, 0.3031479370367056, 0.10104931234556853, 0.10104931234556853, 0.025262328086392134, 0.17683629660474492, 0.025262328086392134, 0.08993971811175101, 0.44969859055875505, 0.35975887244700405, 0.7285530217482173, 0.11881235534995199, 0.7841615453096831, 0.04752494213998079, 0.023762471069990396, 0.7849031435125802, 0.33108107227748274, 0.551801787129138, 0.5171046606365303, 0.4507574638758542, 0.4507574638758542, 0.7830319230395755, 0.04606070135526915, 0.0921214027105383, 0.04606070135526915, 0.11393404831100476, 0.22786809662200952, 0.6266372657105261, 0.6082793455343227, 0.8743360342126253, 0.9348282011959557, 0.877175744650221, 0.8039283795823519, 0.9086668784494383, 0.36440547358996866, 0.09110136839749217, 0.09110136839749217, 0.2733041051924765, 0.09110136839749217, 0.09110136839749217, 0.24815750978683523, 0.24815750978683523, 0.37223626468025284, 0.865590324211662, 0.7121299029370778, 0.7181800922007959, 0.13686831986546333, 0.5474732794618533, 0.13686831986546333, 0.1727278465262724, 0.7600025247155986, 0.6942157898157834, 0.3764581793559066, 0.15058327174236263, 0.07529163587118132, 0.07529163587118132, 0.30116654348472527, 0.07529163587118132, 0.22459612298483994, 0.7486537432827998, 0.09836634102014291, 0.8852970691812861, 0.6019228067816946, 0.708883530221406, 0.9255257545975745, 0.927006066832929, 0.9194589783683261, 0.7420926353496038, 0.9183937293736016, 0.30369711696162993, 0.08677060484617999, 0.08677060484617999, 0.043385302423089994, 0.4772383266539899, 0.4596458965565866, 0.23501439781139158, 0.23501439781139158, 0.47002879562278316, 0.250770700107571, 0.501541400215142, 0.0822500613488684, 0.9047506748375524, 0.10434513575169255, 0.7304159502618478, 0.10434513575169255, 0.09058667864515511, 0.815280107806396, 0.8946491766440317, 0.6969182511607439, 0.6059888768533696, 0.7722193281531895, 0.30608900480976947, 0.30608900480976947, 0.8673897602788846, 0.12525507490579862, 0.814157986887691, 0.7670567496948241, 0.49126741034779636, 0.04810147735382233, 0.8658265923688019, 0.4487069949606862, 0.9097718781724944, 0.9741275741982754, 0.6291910631829274, 0.2842897729151911, 0.5685795458303822, 0.145460426563239, 0.8000323460978144, 0.16693404872181206, 0.7512032192481543, 0.2908209203585261, 0.3061272845879222, 0.061225456917584443, 0.2602081918997339, 0.045919092688188334, 0.9196183947278059, 0.1558170223756175, 0.1558170223756175, 0.1558170223756175, 0.4674510671268525, 0.1558170223756175, 0.6065192137366524, 0.3032596068683262, 0.2409065693665348, 0.6883044839043851, 0.03441522419521925, 0.03441522419521925, 0.9531390266121249, 0.8255997317185032, 0.25771007300913396, 0.25771007300913396, 0.25771007300913396, 0.6308466757195702, 0.913175215032601, 0.25240499328287075, 0.7572149798486123, 0.14137213587645323, 0.14137213587645323, 0.28274427175290645, 0.14137213587645323, 0.5897567853812645, 0.33700387736072257, 0.15077452185268922, 0.7538726092634461, 0.938160772539301, 0.269154464632559, 0.538308929265118, 0.2556695299200917, 0.5113390598401834, 0.3196741995170895, 0.2663951662642412, 0.18647661638496885, 0.10655806650569648, 0.05327903325284824, 0.02663951662642412, 0.02663951662642412, 0.38693779523530325, 0.38693779523530325, 0.048367224404412906, 0.048367224404412906, 0.048367224404412906, 0.048367224404412906, 0.821287370247036, 0.7876053101505904, 0.5692231025965085, 0.16263517217043102, 0.08131758608521551, 0.16263517217043102, 0.9451822694570577, 0.8705237986956542, 0.809059327506262, 0.28434756074614725, 0.42652134111922085, 0.07108689018653681, 0.07108689018653681, 0.07108689018653681, 0.6144828807832564, 0.8680159754597114, 0.8828138128592823, 0.2730972992128882, 0.5461945984257764, 0.8444667401429402, 0.04967451412605531, 0.04967451412605531, 0.04967451412605531, 0.5560857440978435, 0.9544960508558538, 0.1476095690029778, 0.1476095690029778, 0.5904382760119112, 0.38310749247572684, 0.38310749247572684, 0.3267753252480833, 0.6535506504961666, 0.6101973991504104, 0.6686426935124072, 0.6248162050078797, 0.780389965240514, 0.9381429126680441, 0.05211905070378022, 0.606753344257207, 0.05014013201934192, 0.016713377339780643, 0.8523822443288127, 0.06685350935912257, 0.6920262448038454, 0.46016567110433904, 0.9482037828989458, 0.7689600632729383, 0.6157873103384981, 0.2501635948250149, 0.057730060344234196, 0.03848670689615613, 0.019243353448078065, 0.019243353448078065, 0.5127419456281586, 0.8843801327299936, 0.784536963414776, 0.6956824810300702, 0.29814963472717293, 0.4753871976557409, 0.1269073496226031, 0.6345367481130156, 0.1269073496226031, 0.7616046086303706, 0.9471347276040899, 0.8658073386295628, 0.8108620756365971, 0.3076854188664413, 0.6153708377328826, 0.8536064141953231, 0.7600320494442354, 0.9043852886723773, 0.7655032263067212, 0.21948936900267665, 0.65846810700803, 0.3351956902142919, 0.16759784510714595, 0.2793297418452433, 0.05586594836904865, 0.1117318967380973, 0.5682594473039937, 0.3788396315359958, 0.8302893354438635, 0.1875366472254134, 0.1875366472254134, 0.5626099416762402, 0.12030996324493891, 0.7218597794696334, 0.12030996324493891, 0.2120801107513934, 0.23564456750154822, 0.04712891350030964, 0.47128913500309644, 0.28517194704295923, 0.3327006048834524, 0.0475286578404932, 0.28517194704295923, 0.40345968242193353, 0.40345968242193353, 0.30150970683748285, 0.6030194136749657, 0.35111421600570814, 0.2633356620042811, 0.08777855400142703, 0.17555710800285407, 0.08777855400142703, 0.37717740926477816, 0.4714717615809727, 0.09429435231619454, 0.10226544912575461, 0.13635393216767283, 0.6817696608383641, 0.03408848304191821, 0.03408848304191821, 0.06236342375309216, 0.8730879325432903, 0.06236342375309216, 0.12145340215386007, 0.7692048803077804, 0.04048446738462002, 0.04048446738462002, 0.32681259121182993, 0.2859610173103512, 0.24510944340887245, 0.08170314780295748, 0.39562999790514525, 0.4945374973814316, 0.743553047187892, 0.9095054550645485, 0.674810029810236, 0.168702507452559, 0.9434751808385871, 0.16470646884857917, 0.6588258753943167, 0.6268629343370757, 0.13802281596405458, 0.5520912638562183, 0.13802281596405458, 0.748160678801117, 0.03916744346550087, 0.03916744346550087, 0.90085119970652, 0.8450591237465075, 0.45858259030205517, 0.45858259030205517, 0.27162931688438097, 0.5432586337687619, 0.5342919617245158, 0.4591200947709807, 0.45386063386019726, 0.1222027398373846, 0.0611013699186923, 0.7637671239836538, 0.0611013699186923, 0.44994485235817205, 0.44994485235817205, 0.419460974990914, 0.419460974990914, 0.13982032499697133, 0.2541963741796974, 0.1429854604760798, 0.2700836475659285, 0.03177454677246218, 0.11121091370361762, 0.01588727338623109, 0.03177454677246218, 0.03177454677246218, 0.01588727338623109, 0.07943636693115544, 0.2924447156042739, 0.5848894312085477, 0.4297144816584679, 0.24808032712064704, 0.4961606542412941, 0.25628900960883333, 0.5125780192176667, 0.8805162424817061, 0.7891189799649093, 0.08306515578577993, 0.1245977336786699, 0.36251278884311267, 0.04027919876034585, 0.04027919876034585, 0.5236295838844961, 0.8400649777294353, 0.1661101605474675, 0.1661101605474675, 0.1661101605474675, 0.332220321094935, 0.8581352404722272, 0.37033425068383097, 0.12344475022794366, 0.2057412503799061, 0.2468895004558873, 0.7974182635126489, 0.16787752916055768, 0.8388907661942013, 0.8077016116609167, 0.8303129278056788, 0.7637545069644196, 0.4158354962005632, 0.4158354962005632, 0.6636702289325104, 0.24133462870273104, 0.02011121905856092, 0.02011121905856092, 0.04022243811712184, 0.8872863600572156, 0.7541178207418904, 0.8660531814650932, 0.934172334725368, 0.64113647895687, 0.8618667569300951, 0.927443623033453, 0.9201060679223864, 0.9448240788213571, 0.1971613666746923, 0.09858068333734615, 0.1971613666746923, 0.3943227333493846, 0.8332831378262431, 0.9041172040648693, 0.7997567967484013, 0.2882011583325807, 0.05764023166651614, 0.11528046333303228, 0.17292069499954843, 0.05764023166651614, 0.2882011583325807, 0.17281707076906466, 0.4147609698457552, 0.034563414153812935, 0.06912682830762587, 0.1036902424614388, 0.034563414153812935, 0.034563414153812935, 0.034563414153812935, 0.06912682830762587, 0.24538632015571535, 0.4907726403114307, 0.3883901020391308, 0.3883901020391308, 0.3304835154760559, 0.6609670309521118, 0.4512711273458614, 0.40614401461127525, 0.02256355636729307, 0.09025422546917228, 0.02256355636729307, 0.2834098814276155, 0.2834098814276155, 0.8521044186805361, 0.049590747902827484, 0.6942704706395848, 0.14877224370848244, 0.3330248876423994, 0.5229336052081817, 0.5229336052081817, 0.8728198497807537, 0.5688857570770037, 0.819950840241777, 0.7443413892865803, 0.43124197556093735, 0.2653796772682691, 0.06634491931706728, 0.03317245965853364, 0.03317245965853364, 0.16586229829266821, 0.7593904949609211, 0.6478504371294063, 0.8278489201172715, 0.4608149170116481, 0.8230634945941969, 0.9354938961339069, 0.9156635889574554, 0.8436710556719377, 0.05624473704479585, 0.1124894740895917, 0.28500617784157956, 0.5700123556831591, 0.8110854438123534, 0.7776130018415297, 0.8332723676704344, 0.9491183434540473, 0.08412461918839363, 0.8412461918839363, 0.08412461918839363, 0.8493254151363152, 0.28832974588118665, 0.5766594917623733, 0.4122817161150363, 0.3623081747677592, 0.06246692668409641, 0.037480156010457844, 0.08745369735773498, 0.024986770673638563, 0.4675883150962296, 0.3366635868692853, 0.056110597811547556, 0.018703532603849184, 0.09351766301924593, 0.8567671902515713, 0.4208635907677911, 0.6987812261776724, 0.23292707539255747, 0.4395379208170542, 0.14142938274009365, 0.7637186667965057, 0.08485762964405619, 0.2951032384204901, 0.5902064768409802, 0.1867812443048962, 0.5603437329146886, 0.41382529087577347, 0.2758835272505157, 0.06897088181262892, 0.13794176362525784, 0.06897088181262892, 0.9373299165321799, 0.06248866110214533, 0.3565476219662654, 0.13951863468245165, 0.33329451618585676, 0.06975931734122583, 0.02325310578040861, 0.02325310578040861, 0.007751035260136203, 0.02325310578040861, 0.007751035260136203, 0.6010471404176813, 0.13738220352404143, 0.12020942808353625, 0.05151832632151553, 0.05151832632151553, 0.01717277544050518, 0.3671908757446292, 0.6425840325531011, 0.5498241626125929, 0.22797587230278243, 0.10057759072181578, 0.06034655443308947, 0.04023103628872631, 0.006705172714787719, 0.6355416215749458, 0.023220497146065926, 0.952040382988703, 0.023220497146065926, 0.6756970188088007, 0.10045564989817976, 0.20091129979635952, 0.10045564989817976, 0.10045564989817976, 0.40182259959271904, 0.8568892631829963, 0.2393644705560764, 0.014080262973886848, 0.6617723597726819, 0.04224078892166054, 0.014080262973886848, 0.1986219387830988, 0.7613840986685454, 0.0331036564638498, 0.5022333451951083, 0.9046651066009556, 0.90596832640353, 0.8843801327299936, 0.39755412530998885, 0.19877706265499442, 0.19877706265499442, 0.9006707136461198, 0.19318561088879388, 0.5795568326663817, 0.04829640272219847, 0.14488920816659542, 0.04829640272219847, 0.1623481440164279, 0.1623481440164279, 0.48704443204928366, 0.9366595868087658, 0.7104931915822698, 0.795073130656958, 0.2439595158797049, 0.07762348232536065, 0.2883157914941967, 0.03326720671086885, 0.044356275614491805, 0.08871255122898361, 0.011089068903622951, 0.044356275614491805, 0.09980162013260656, 0.011089068903622951, 0.022178137807245903, 0.011089068903622951, 0.011089068903622951, 0.9100251385864357, 0.9367525707279788, 0.8446617169707707, 0.220262740666533, 0.04004777103027873, 0.4605493668482054, 0.20023885515139364, 0.04004777103027873, 0.020023885515139363, 0.020023885515139363, 0.8723219158003729, 0.0379917013278601, 0.9497925331965026, 0.5586178074428826, 0.0931029679071471, 0.0931029679071471, 0.0931029679071471, 0.1862059358142942, 0.3854785626645861, 0.4818482033307326, 0.9567231650100073, 0.5049618464242942, 0.12624046160607355, 0.2524809232121471, 0.9319717698091226, 0.715957615731244, 0.4194369745014155, 0.4194369745014155, 0.852113225384786, 0.10456256868289507, 0.8365005494631605, 0.8421094163811836, 0.3670667518085977, 0.2936534014468782, 0.07341335036171955, 0.22024005108515862, 0.1642060515994936, 0.6568242063979745, 0.1642060515994936, 0.52860115913003, 0.052860115913003, 0.370020811391021, 0.5427700360582762, 0.16253083802732257, 0.7313887711229515, 0.3732993981126537, 0.42000887313743274, 0.3658840632811467, 0.5488260949217201, 0.7284581046169697, 0.38459614923336843, 0.4326706678875395, 0.14422355596251316, 0.6159076052252258, 0.7518057429012022, 0.046283509690636736, 0.1388505290719102, 0.7405361550501878, 0.41979918781800524, 0.41769103268384006, 0.5370313277363657, 0.14617343658051182, 0.803953901192815, 0.48543008670811477, 0.48543008670811477, 0.5949859384710547, 0.0749344281721535, 0.44960656903292096, 0.0749344281721535, 0.299737712688614, 0.10582351696045288, 0.846588135683623, 0.11953375130201406, 0.8367362591140984, 0.4720606425540193, 0.3869597953715868, 0.9596787706713031, 0.10803842548313179, 0.7562689783819225, 0.054019212741565896, 0.03601280849437726, 0.01800640424718863, 0.04798672496128994, 0.8877544117838638, 0.02399336248064497, 0.621200254293546, 0.05287093577016543, 0.8988059080928124, 0.05287093577016543, 0.41719788418199716, 0.8043350363170236, 0.11903869338015165, 0.8332708536610616, 0.8352236326762044, 0.07592942115238221, 0.4217873468870588, 0.5061448162644706, 0.7094454723647857, 0.8958729074722461, 0.25931331623204, 0.25931331623204, 0.51862663246408, 0.5887835555232673, 0.04763974882982709, 0.9051552277667146, 0.04763974882982709, 0.38706788064116826, 0.38706788064116826, 0.9223629793345506, 0.8822650709070702, 0.9021378688849718, 0.24278922457363059, 0.48557844914726117, 0.43026394437615695, 0.8420728353289659, 0.030074029833177356, 0.030074029833177356, 0.09022208949953206, 0.601737357866481, 0.8790586976829011, 0.7524164882911205, 0.5209288634905072, 0.2604644317452536, 0.7913310909083452, 0.8007532219523723, 0.5365384718160363, 0.05977444936210277, 0.019924816454034256, 0.05977444936210277, 0.7969926581613702, 0.03984963290806851, 0.025645550799969476, 0.12822775399984737, 0.7437209731991148, 0.07693665239990842, 0.859726221732352, 0.40891633325936616, 0.40891633325936616, 0.9294328193392607, 0.10190701726514231, 0.8152561381211385, 0.942344169872647, 0.814191897290711, 0.7221953657718067, 0.9446198942543553, 0.4334138563325082, 0.4334138563325082, 0.43679574276984995, 0.04858928187770723, 0.23079908891910933, 0.631660664410194, 0.07288392281656084, 0.012147320469426808, 0.5523094197621634, 0.252484306176989, 0.07890134568030907, 0.03156053827212363, 0.07890134568030907, 0.5468211398110491, 0.16404634194331472, 0.11847791362572731, 0.01822737132703497, 0.10025054229869233, 0.027341056990552455, 0.009113685663517486, 0.12409938245585286, 0.8066459859630436, 0.5747353576084292, 0.17087184651765197, 0.3987009752078546, 0.05695728217255066, 0.17087184651765197, 0.22782912869020264, 0.05695728217255066, 0.17504402279807774, 0.6418280835929517, 0.019449335866453085, 0.15559468693162468, 0.746652038385077, 0.44587663345377465, 0.02650099381298575, 0.07950298143895725, 0.7420278267636009, 0.07950298143895725, 0.02650099381298575, 0.02650099381298575, 0.697287732623733, 0.9217206946317098, 0.07681005788597581, 0.4376965730172767, 0.7689654565744906, 0.8135198681095991, 0.2547158571745147, 0.4631197403172995, 0.1852478961269198, 0.04631197403172995, 0.023155987015864975, 0.023155987015864975, 0.6930504508334547, 0.8330337141284166, 0.2871782470087083, 0.6700825763536528, 0.13728491723225938, 0.13728491723225938, 0.27456983446451877, 0.41185475169677815, 0.7614347044223037, 0.5510925073502511, 0.6406860323072742, 0.539532736032579, 0.2931601089807022, 0.5863202179614044, 0.808201090481401, 0.3635266777047199, 0.3635266777047199, 0.2503909914109825, 0.7511729742329475, 0.8938727389206435, 0.24084173949813584, 0.4816834789962717, 0.24084173949813584, 0.8090607163371558, 0.35875371548182217, 0.15944609576969873, 0.35875371548182217, 0.07972304788484937, 0.9012232584669618, 0.3542074457405094, 0.10780226609493763, 0.5236110067468399, 0.015400323727848234, 0.7415002671628711, 0.8709392425076641, 0.45165538732837585, 0.45165538732837585, 0.7179294741090998, 0.2848814347922281, 0.14244071739611405, 0.5222826304524182, 0.8498705137491437, 0.9581741217141801, 0.5768540031055138, 0.3461124018633083, 0.11672083570762566, 0.700325014245754, 0.11672083570762566, 0.8612630742908459, 0.13326336951653706, 0.2665267390330741, 0.3997901085496111, 0.08011977625527095, 0.08011977625527095, 0.7210779862974386, 0.7985033274671679, 0.2575922682605886, 0.38638840239088296, 0.2575922682605886, 0.6524519207925936, 0.6868077480012883, 0.6877470229177929, 0.8678457209035815, 0.7167682523915435, 0.3255216945365176, 0.6510433890730352, 0.30685529361768593, 0.6137105872353719, 0.3622189847766419, 0.6036983079610698, 0.17613840908696965, 0.17613840908696965, 0.7045536363478786, 0.3262486108757003, 0.3262486108757003, 0.8596384524065682, 0.10745480655082103, 0.44545116674091045, 0.8469163822462744, 0.4970482224908564, 0.4970482224908564, 0.5311770956652351, 0.35411806377682337, 0.6408831016610069, 0.21874934405031768, 0.16406200803773827, 0.21874934405031768, 0.38281135208805595, 0.12979533412389552, 0.6489766706194776, 0.2104387118637189, 0.09352831638387507, 0.607934056495188, 0.046764158191937535, 0.3823796916476761, 0.8098929900752285, 0.8880269839155462, 0.17034174113248837, 0.6813669645299535, 0.27990210846603697, 0.5598042169320739, 0.4187958160820774, 0.4215594225585909, 0.2810396150390606, 0.1405198075195303, 0.9554845189163246, 0.6182236030868378, 0.2649529727515019, 0.9525918481907389, 0.42483515813528305, 0.40242729997623233, 0.11178536110450897, 0.29064193887172335, 0.15649950554631256, 0.7361831247944589, 0.6290491973217237, 0.8756713382332949, 0.6486897768677665, 0.5247894306025573, 0.3748495932875409, 0.9499678527440133, 0.8443117983992482, 0.2698689402976463, 0.5397378805952926, 0.09417736186471795, 0.8475962567824615, 0.03139245395490598, 0.1453939071022078, 0.7754341712117748, 0.13208165487544538, 0.7924899292526724, 0.7881525741731187, 0.22347335406967708, 0.22347335406967708, 0.44694670813935417, 0.4251571989771863, 0.21257859948859315, 0.21257859948859315, 0.15369921003768436, 0.15369921003768436, 0.46109763011305305, 0.9238040725376241, 0.3104443154685258, 0.5432775520699201, 0.1034814384895086, 0.7680725206931336, 0.3584324928831711, 0.49284467771436025, 0.08960812322079277, 0.22763689086361413, 0.7208501544014447, 0.9356802427467114, 0.1478318274197842, 0.44349548225935265, 0.1478318274197842, 0.4110052606359021, 0.29453016568851714, 0.49088360948086196, 0.0981767218961724, 0.19279415088034357, 0.7711766035213743, 0.8913771579630142, 0.9042489174730074, 0.07535407645608394, 0.3852558370141525, 0.5778837555212287, 0.5405823150797076, 0.23446943786589725, 0.11723471893294862, 0.02605215976287747, 0.045591279585035574, 0.013026079881438736, 0.013026079881438736, 0.006513039940719368, 0.8634184093947694, 0.38045366807897835, 0.09511341701974459, 0.09511341701974459, 0.38045366807897835, 0.30461241506083436, 0.019038275941302148, 0.6663396579455751, 0.2070408961987148, 0.7453472263153733, 0.10303559971016245, 0.8758025975363808, 0.8312498763661691, 0.08312498763661691, 0.3756691757944361, 0.3756691757944361, 0.6082106525505193, 0.8237831343474563, 0.4304028976708064, 0.19202590819159054, 0.28472807076684115, 0.026486332164357317, 0.03310791520544665, 0.013243166082178659, 0.006621583041089329, 0.8677454503528467, 0.06674965002714206, 0.756409057708769, 0.29984465992656806, 0.5996893198531361, 0.3129079824417622, 0.5215133040696037, 0.10430266081392074, 0.638087251676501, 0.6419750274105729, 0.18956647900654844, 0.5686994370196453, 0.14217485925491133, 0.07108742962745566, 0.5943734377742926, 0.9071073879795573, 0.04998126881731286, 0.8996628387116314, 0.3693288659615814, 0.6352257014053445, 0.770300275876845, 0.7534075241501927, 0.12054520386403084, 0.09040890289802313, 0.8517909290023482, 0.8035757374529401, 0.4410607293525464, 0.7961433609221936, 0.041902282153799666, 0.08380456430759933, 0.041902282153799666, 0.8962067136283797, 0.6321265086898696, 0.35480402392800103, 0.3041177347954294, 0.3041177347954294, 0.025343144566285787, 0.5088808540876345, 0.18680436415875193, 0.18036283436017428, 0.02576611919431061, 0.04509070859004357, 0.02576611919431061, 0.0064415297985776524, 0.0064415297985776524, 0.0064415297985776524, 0.5307127201072008, 0.2653563600536004, 0.22233168711904755, 0.4446633742380951, 0.4339712579851502, 0.69776539027594, 0.6353153741256357, 0.21838965985568726, 0.03970721088285223, 0.019853605441426114, 0.05956081632427834, 0.019853605441426114, 0.6625510900699197, 0.24609040488311304, 0.01893003114485485, 0.01893003114485485, 0.01893003114485485, 0.01893003114485485, 0.37483076986968755, 0.05111328680041194, 0.4770573434705115, 0.01703776226680398, 0.0851888113340199, 0.17254719070852306, 0.6901887628340923, 0.27159569569742775, 0.27159569569742775, 0.04526594928290462, 0.27159569569742775, 0.13579784784871388, 0.8542116898147999, 0.7992870700016442, 0.8500433090859818, 0.10137833071055297, 0.8617158110397002, 0.2296710158158323, 0.057417753953958074, 0.5167597855856226, 0.11483550790791615, 0.028708876976979037, 0.028708876976979037, 0.07672819939652874, 0.2301845981895862, 0.38364099698264365, 0.07672819939652874, 0.07672819939652874, 0.2301845981895862, 0.15702746757414346, 0.7851373378707173, 0.5740857294137611, 0.21528214853016042, 0.08372083553950682, 0.04784047745114676, 0.03588035808836007, 0.04784047745114676, 0.4552385585174552, 0.10116412411499005, 0.10116412411499005, 0.2529103102874751, 0.19873062434215338, 0.3477785925987684, 0.3477785925987684, 0.049682656085538346, 0.20841252940579688, 0.7294438529202891, 0.23919733828090622, 0.23919733828090622, 0.47839467656181245, 0.670945990845967, 0.8567288100203343, 0.1223898300029049, 0.22856319451002843, 0.11428159725501422, 0.45712638902005687, 0.4912078236876188, 0.392966258950095, 0.5910839161060665, 0.21331200932087666, 0.15998400699065748, 0.5866080256324108, 0.797003925956761, 0.4320317021392577, 0.36002641844938144, 0.14401056737975257, 0.7416589347307704, 0.46016567110433904, 0.15128125449725652, 0.8320468997349107, 0.24936965162188807, 0.49873930324377613, 0.6616586271530844, 0.9321348889789053, 0.8838549010037047, 0.6783600038426372, 0.8941353061007735, 0.24233203446931034, 0.7269961034079311, 0.9267376008475444, 0.1505832746010025, 0.1505832746010025, 0.60233309840401, 0.8697220615738447, 0.6510725447926836, 0.5749615951231571, 0.6650101596969462, 0.7575464464154035, 0.9337051987315509, 0.0405958782057196, 0.8793926554913718, 0.13961366998478142, 0.13961366998478142, 0.13961366998478142, 0.13961366998478142, 0.13961366998478142, 0.27922733996956284, 0.7385546878394059, 0.21541178395316007, 0.030773111993308584, 0.054923326601225876, 0.8513115623190011, 0.054923326601225876, 0.9032496870829072, 0.4640226878315015, 0.9004179574871022, 0.5239700627881745, 0.44911719667557815, 0.40338741911528825, 0.06372249026127617, 0.12744498052255235, 0.700947392874038, 0.7550261574993226, 0.167783590555405, 0.46036109803035014, 0.46036109803035014, 0.7827124969954204, 0.25681830561066354, 0.5136366112213271, 0.08365632216372704, 0.5019379329823622, 0.08365632216372704, 0.2509689664911811, 0.2880955224808866, 0.2880955224808866, 0.632263310021336, 0.11689438363213898, 0.8182606854249729, 0.9021220890999885, 0.19076990428027266, 0.19076990428027266, 0.572309712840818, 0.9390909080180784, 0.9071808131828878, 0.4159399252109219, 0.4159399252109219, 0.5767626726902814, 0.49474593972134556, 0.18299178795620277, 0.594723310857659, 0.04574794698905069, 0.1372438409671521, 0.19281754387673627, 0.6748614035685769, 0.09640877193836814, 0.9308547814464339, 0.6526936147218082, 0.7873356409431145, 0.40462829476110596, 0.11900832198856057, 0.3927274625622499, 0.023801664397712113, 0.03570249659656817, 0.011900832198856057, 0.7847168725331759, 0.3582087857339977, 0.11940292857799925, 0.477611714311997, 0.44106623051741894, 0.44106623051741894, 0.4741280827836018, 0.1815809678745709, 0.19166879942315818, 0.04035132619434909, 0.08070265238869818, 0.010087831548587273, 0.8286695810609749, 0.2076379525599041, 0.5042635990740528, 0.14831282325707437, 0.08898769395424462, 0.059325129302829745, 0.9636176755016095, 0.8234153094967744, 0.47685655272379934, 0.9245231389893817, 0.871159629272247, 0.9586119766974606, 0.7819596824582652, 0.6054046331974219, 0.6025365811945079, 0.7112411387015559, 0.1933022081529597, 0.06443406938431989, 0.5154725550745591, 0.1933022081529597, 0.24882307798306197, 0.622057694957655, 0.7204016995509682, 0.3905037342206691, 0.6418491454750394, 0.7965273177239793, 0.6259622793267978, 0.41418627725542767, 0.03186048286580213, 0.09558144859740639, 0.4460467601212298, 0.16536432327292888, 0.33072864654585776, 0.49609296981878664, 0.2925511158183879, 0.2925511158183879, 0.8413693490479783, 0.7790451368407078, 0.32581961547373645, 0.48872942321060464, 0.16290980773686822, 0.41964454099062565, 0.20982227049531282, 0.8915068878516536, 0.41964304847441397, 0.8190384920695993, 0.3246925433209644, 0.23614003150615595, 0.17710502362961694, 0.029517503938269494, 0.17710502362961694, 0.029517503938269494, 0.17050514002960568, 0.34101028005921136, 0.511515420088817, 0.47763494188133926, 0.42591082552388526, 0.42591082552388526, 0.43884971286120134, 0.8567786119136109, 0.4193125062665119, 0.8719684130115772, 0.8352685728414078, 0.7272495781327272, 0.9404429361248626, 0.6980139368533322, 0.3789638500958817, 0.5684457751438226, 0.8849567750034718, 0.6250696438343663, 0.8477942466313121, 0.09974049960368377, 0.8275538358660883, 0.8702514200489088, 0.06476474762203445, 0.19429424286610333, 0.6476474762203445, 0.9459633015077622, 0.24085972564681926, 0.4817194512936385, 0.3683074515709475, 0.48340353018686866, 0.015346143815456147, 0.10742300670819303, 0.007673071907728073, 0.007673071907728073, 0.2141784887741717, 0.642535466322515, 0.5952210302099291, 0.6080515948819353, 0.1067583676589009, 0.8540669412712072, 0.8414192011211574, 0.6977792211913558, 0.6278008152168117, 0.8448740831625957, 0.38984723051568604, 0.055692461502240866, 0.13923115375560216, 0.027846230751120433, 0.0835386922533613, 0.027846230751120433, 0.22276984600896346, 0.027846230751120433, 0.8959856816637715, 0.20575395178871256, 0.6172618553661378, 0.7312704202946642, 0.19561471059069213, 0.5868441317720764, 0.631876063677548, 0.9315320517343606, 0.9215836863116783, 0.951501423396337, 0.23227706130605125, 0.7742568710201708, 0.18082697174016174, 0.5424809152204852, 0.6287203119917245, 0.7691945999938774, 0.40744144038800606, 0.40744144038800606, 0.43109794083392805, 0.43109794083392805, 0.09637145119519404, 0.09637145119519404, 0.09637145119519404, 0.6424763413012936, 0.15451899671636973, 0.6953354852236637, 0.15326589106627844, 0.05108863035542615, 0.7152408249759661, 0.03865202793105705, 0.9276486703453692, 0.9426704526582301, 0.8983764185284151, 0.8854566939457279, 0.9064664439212035, 0.3188187542102376, 0.6376375084204752, 0.046191486963104586, 0.8776382522989872, 0.046191486963104586, 0.312494508198685, 0.312494508198685, 0.8535641530670707, 0.22328863145567793, 0.6698658943670338, 0.8728436173558181, 0.09698262415064646, 0.7253037882373894, 0.12088396470623157, 0.08058930980415438, 0.04029465490207719, 0.020147327451038595, 0.020147327451038595, 0.21596770982017494, 0.43193541964034987, 0.8218608262303829, 0.6383594791641749, 0.31917973958208745, 0.7015019626275055, 0.05396168943288504, 0.10792337886577008, 0.08094253414932756, 0.02698084471644252, 0.43978699504124247, 0.04886522167124916, 0.2687587191918704, 0.21989349752062123, 0.6054109153633783, 0.422272887622658, 0.5161113070943598, 0.8102291669884624, 0.1697102783580376, 0.6788411134321504, 0.5941090217721543, 0.1980363405907181, 0.0900165184503264, 0.01800330369006528, 0.05400991107019584, 0.01800330369006528, 0.49855086702024676, 0.07478263005303702, 0.38637692194069123, 0.02492754335101234, 0.01246377167550617, 0.01246377167550617, 0.16047757430633847, 0.16047757430633847, 0.4814327229190154, 0.251615601404508, 0.03594508591492971, 0.5571488316814106, 0.07189017182985942, 0.03594508591492971, 0.4323267879119659, 0.4323267879119659, 0.09845197784310117, 0.19690395568620234, 0.09845197784310117, 0.49225988921550584, 0.6644322484678041, 0.7838844593697137, 0.5743861751777833, 0.28719308758889167, 0.6802246713069233, 0.7811535800955908, 0.6764885237815002, 0.2705954095126, 0.23481785429456234, 0.3815790132286638, 0.11740892714728117, 0.20546562250774203, 0.029352231786820292, 0.8471655958137762, 0.3052104351373087, 0.6104208702746174, 0.08287568358029099, 0.9116325193832009, 0.23011246342216624, 0.6903373902664987, 0.6677849129903674, 0.7085978710176983, 0.8761492922210973, 0.10570757889633904, 0.7399530522743732, 0.770721827719158, 0.04167556043017086, 0.875186769033588, 0.08335112086034172, 0.47497475721755494, 0.3166498381450366, 0.8864632951691109, 0.863435824947057, 0.4188284031204642, 0.2094142015602321, 0.6451923912700921, 0.8235265668825543, 0.5690583172866622, 0.23033312842555376, 0.08129404532666604, 0.027098015108888675, 0.05419603021777735, 0.013549007554444338, 0.013549007554444338, 0.6381883873974008, 0.2393206452740253, 0.7824566688139413, 0.3210751071944485, 0.642150214388897, 0.6226070446431441, 0.2123979211586534, 0.6371937634759602, 0.6025744047735863, 0.8139860460214868, 0.5551688816183619, 0.08148152434082594, 0.28518533519289074, 0.5703706703857815, 0.11060723210717209, 0.3318216963215163, 0.5530361605358605, 0.43748912530972023, 0.8178373978337314, 0.7566306259336948, 0.7905822191494226, 0.48257436585382535, 0.48257436585382535, 0.9005234593672805, 0.9419096741938118, 0.9238046741073989, 0.08398224310067262, 0.5956268309145937, 0.23576895390369332, 0.062044461553603505, 0.04963556924288281, 0.024817784621441404, 0.012408892310720702, 0.0835898840990382, 0.7523089568913437, 0.0835898840990382, 0.0417949420495191, 0.17438578453318268, 0.17438578453318268, 0.6975431381327307, 0.6812154106195031, 0.263482362580861, 0.17565490838724068, 0.08782745419362034, 0.263482362580861, 0.08782745419362034, 0.08782745419362034, 0.5957971191034263, 0.23983413940376488, 0.47966827880752977, 0.7234775091054548, 0.47239395396596895, 0.47239395396596895, 0.9393582966463115, 0.15627371055910413, 0.15627371055910413, 0.6250948422364165, 0.2553814972334252, 0.2979450801056627, 0.4681994115946128, 0.42207077208954297, 0.42207077208954297, 0.140690257363181, 0.31336440465619675, 0.5013830474499148, 0.015668220232809837, 0.031336440465619675, 0.07834110116404919, 0.015668220232809837, 0.015668220232809837, 0.6906642058243367, 0.17504393114172326, 0.08752196557086163, 0.6710017360432725, 0.5301142685798114, 0.3534095123865409, 0.14070919648066063, 0.28141839296132126, 0.14070919648066063, 0.14070919648066063, 0.28141839296132126, 0.8519902096967674, 0.858941965062237, 0.7262530135013128, 0.6231300418714351, 0.207710013957145, 0.1056492153093807, 0.4225968612375228, 0.3169476459281421, 0.7004558087896616, 0.46475398425939546, 0.25819665792188634, 0.05163933158437727, 0.18073766054532045, 0.03872949868828295, 0.3190432925594261, 0.6380865851188522, 0.7210158877366005, 0.11092552119024623, 0.05546276059512312, 0.05546276059512312, 0.05546276059512312, 0.8072646934795069, 0.04248761544628984, 0.04248761544628984, 0.04248761544628984, 0.04248761544628984, 0.6448087533126472, 0.8902698194970948, 0.7925510413923647, 0.9425808646855797, 0.06945942422831396, 0.9029725149680814, 0.03472971211415698, 0.0292554934365144, 0.9069202965319465, 0.0292554934365144, 0.6584728514739613, 0.04126315632050986, 0.9077894390512169, 0.04126315632050986, 0.3263402360697775, 0.5710954131221107, 0.22989276454420388, 0.7663092151473463, 0.11212075848125556, 0.2803018962031389, 0.50454341316565, 0.5801975004573775, 0.13651705893114766, 0.2389048531295084, 0.32669334232072966, 0.32669334232072966, 0.3219826920600852, 0.09199505487431005, 0.4139777469343952, 0.022998763718577512, 0.06899629115573254, 0.06899629115573254, 0.022998763718577512, 0.91396916938443, 0.07261771982006501, 0.8351037779307476, 0.07261771982006501, 0.5328367912840825, 0.08880613188068041, 0.26641839564204123, 0.5891613741664462, 0.35349682449986775, 0.14885161018186982, 0.14885161018186982, 0.5954064407274793, 0.9562283501184413, 0.21705669659748375, 0.10852834829874188, 0.4341133931949675, 0.7555693425609982, 0.7931372793247122, 0.9487266525564044, 0.24433932861897642, 0.12216966430948821, 0.570125100110945, 0.040723221436496065, 0.878607887147403, 0.3984987723246359, 0.13283292410821196, 0.13283292410821196, 0.2656658482164239, 0.8637017117370615, 0.7646749953879882, 0.05772591535927101, 0.9236146457483362, 0.5442684912202023, 0.07775264160288604, 0.3110105664115442, 0.9509096935211292, 0.8021053817352621, 0.6930673230916808, 0.4492098361977205, 0.43811459633806366, 0.20595223972830878, 0.7002376150762499, 0.061785671918492636, 0.9457605102737254, 0.5426969410826536, 0.15505626888075819, 0.23258440332113725, 0.34992665552420854, 0.2099559933145251, 0.03499266555242085, 0.3849193210766294, 0.8914398453375665, 0.8869303567870697, 0.41042883907282385, 0.47199316493374743, 0.02052144195364119, 0.04104288390728238, 0.02052144195364119, 0.02052144195364119, 0.2565786527637312, 0.641446631909328, 0.5931325025242795, 0.5073104303755102, 0.40584834430040817, 0.7358176299960385, 0.5328885424814218, 0.6606411293974408, 0.7380576442493804, 0.8503498199352205, 0.3037718131551446, 0.1518859065775723, 0.3037718131551446, 0.4200095075630764, 0.4694223908057913, 0.07411932486407231, 0.024706441621357437, 0.08359611632809631, 0.8359611632809631, 0.6489124982752837, 0.21754787506109202, 0.652643625183276, 0.5312562288643005, 0.4429492461391599, 0.5352335598378833, 0.8068275285929307, 0.2709549404552943, 0.5419098809105886, 0.16071710038592635, 0.8035855019296317, 0.6704807987672815, 0.07449786652969795, 0.12416311088282991, 0.024832622176565983, 0.07449786652969795, 0.4557019190957574, 0.7455985217049452, 0.14719444387661454, 0.5887777755064582, 0.1858705631638576, 0.1858705631638576, 0.3717411263277152, 0.2194167158529773, 0.4388334317059546, 0.39874112058037176, 0.19937056029018588, 0.19937056029018588, 0.6029711971526494, 0.7750883108788006, 0.14753677434128443, 0.4426103230238533, 0.14753677434128443, 0.12877579466245656, 0.8370426653059677, 0.7472755563367701, 0.36723058183835966, 0.36723058183835966, 0.8948599767707448, 0.7653614728610678, 0.4398684503199706, 0.4398684503199706, 0.8750287933767, 0.8790551935812372, 0.4590192699904216, 0.6624030224873628, 0.8981374919233142, 0.047765694244885776, 0.8836653435303868, 0.023882847122442888, 0.056723874346666696, 0.9075819895466671, 0.44374267556298674, 0.14791422518766226, 0.2958284503753245, 0.4481774008410756, 0.17927096033643022, 0.08963548016821511, 0.17927096033643022, 0.16944004971956983, 0.6777601988782793, 0.8383632965123644, 0.7604817434119916, 0.19687247116227832, 0.5906174134868349, 0.5872262458500473, 0.0782968327800063, 0.11744524917000947, 0.19574208195001577, 0.801209837665824, 0.20394725314334486, 0.02660181562739281, 0.6207090313058322, 0.02660181562739281, 0.05320363125478562, 0.017734543751595205, 0.02660181562739281, 0.008867271875797602, 0.008867271875797602, 0.47500340950939796, 0.35625255713204845, 0.11875085237734949, 0.29080860726392993, 0.5816172145278599, 0.6694354335747286, 0.18092849556073745, 0.09046424778036873, 0.018092849556073744, 0.018092849556073744, 0.5959162709051781, 0.27241886669951, 0.05107853750615812, 0.03405235833743875, 0.017026179168719374, 0.4857605050131197, 0.2663847930717108, 0.07051362169545286, 0.06267877484040253, 0.0940181622606038, 0.007834846855050316, 0.007834846855050316, 0.6983501042252255, 0.23278336807507516, 0.8821619506220676, 0.5826285666825347, 0.229230911481653, 0.09551287978402209, 0.009551287978402209, 0.05730772787041325, 0.009551287978402209, 0.7532638621639228, 0.43643902021471437, 0.43643902021471437, 0.43428008216858394, 0.14476002738952798, 0.07238001369476399, 0.28952005477905596, 0.24418268150282346, 0.5581318434350251, 0.03488324021468907, 0.1046497206440672, 0.03488324021468907, 0.23471197626810628, 0.46942395253621255, 0.312905807589861, 0.312905807589861, 0.3020086693818462, 0.3020086693818462, 0.5813218602423074, 0.2906609301211537, 0.849679475914256, 0.0701861719376501, 0.0701861719376501, 0.7720478913141512, 0.8983039583839783, 0.2561372467089639, 0.5122744934179277, 0.12806862335448194, 0.26857566057042265, 0.5371513211408453, 0.22142317338544684, 0.4428463467708937, 0.6271567718137416, 0.9203252656230582, 0.8441326965319493, 0.7959359692694863, 0.3917471481281171, 0.2268009804952257, 0.10309135477055714, 0.20618270954111428, 0.04123654190822285, 0.7260130698333083, 0.1870341495874754, 0.1870341495874754, 0.6078609861592951, 0.8870878843680156, 0.06336342031200111, 0.8646214247309065, 0.04323107123654533, 0.04323107123654533, 0.86923830735628, 0.6723515529221458, 0.41627794633629334, 0.6993013419966818, 0.2585764084783644, 0.1292882042391822, 0.1292882042391822, 0.2585764084783644, 0.7483054983596682, 0.4250979346153056, 0.34868630811566087, 0.31381767730409477, 0.10460589243469826, 0.03486863081156608, 0.13947452324626433, 0.03486863081156608, 0.9386192422405928, 0.7458349177205745, 0.4629540038996981, 0.09608479326220148, 0.34939924822618723, 0.05240988723392809, 0.008734981205654682, 0.026204943616964044, 0.5134469335925258, 0.1411979067379446, 0.24388729345644974, 0.025672346679626288, 0.06418086669906573, 0.012836173339813144, 0.40500556202646104, 0.04500061800294012, 0.49500679803234127, 0.03000041200196008, 0.14885038488187874, 0.2977007697637575, 0.44655115464563616, 0.1613557164892973, 0.1613557164892973, 0.1613557164892973, 0.3227114329785946, 0.1613557164892973, 0.6179347143604951, 0.20597823812016502, 0.20597823812016502, 0.22985778626161965, 0.22985778626161965, 0.4597155725232393, 0.8742875316178915, 0.8958480899709751, 0.8952786893709851, 0.5796088599385824, 0.23680313465987815, 0.4736062693197563, 0.1418734923247137, 0.047291164108237894, 0.7093674616235685, 0.023645582054118947, 0.047291164108237894, 0.927284161973808, 0.7009384932526008, 0.8706946071234443, 0.05121732983079084, 0.07950264006953649, 0.7950264006953648, 0.10600352009271531, 0.7399524956300545, 0.30033386291183395, 0.6006677258236679, 0.7602653448143873, 0.46555862118733804, 0.11638965529683451, 0.3491689658905035, 0.8776041368668445, 0.09751157076298272, 0.7819048360639834, 0.8918469267500668, 0.9276958442625922, 0.20760910873240362, 0.6228273261972109, 0.5945631312547104, 0.5505610323130075, 0.1712856544973801, 0.19575503371129155, 0.036704068820867165, 0.036704068820867165, 0.04249629116567744, 0.764933240982194, 0.16998516466270977, 0.04249629116567744, 0.3773769951622062, 0.0628961658603677, 0.5031693268829416, 0.3781319836709261, 0.7475564500849782, 0.12459274168082969, 0.5578552124874911, 0.2182911701038009, 0.10914558505190045, 0.048509148911955756, 0.048509148911955756, 0.012127287227988939, 0.7559453126933945, 0.4761896831115708, 0.40816258552420354, 0.8934617751390876, 0.06721553235968311, 0.06721553235968311, 0.302469895618574, 0.5377242588774649, 0.18216270996382503, 0.5009474524005189, 0.06831101623643439, 0.20493304870930318, 0.050298334170608944, 0.8550716809003521, 0.07544750125591342, 0.7839437900154592, 0.23174554422302054, 0.6952366326690617, 0.7752437598942336, 0.11714965071404579, 0.8200475549983205, 0.23458028193388858, 0.2010688130861902, 0.2010688130861902, 0.33511468847698367, 0.5895280383909091, 0.07018190933225107, 0.2807276373290043, 0.028072763732900432, 0.3256827922388749, 0.41253153683590826, 0.04342437229851666, 0.13027311689554996, 0.06513655844777498, 0.02171218614925833, 0.8364111895606231, 0.8321325088171199, 0.43686159876046804, 0.8155246637536645, 0.5372103637682403, 0.6799015839835703, 0.8456839623920446, 0.08456839623920445, 0.30286927529256097, 0.6057385505851219, 0.16174131509822887, 0.8087065754911443, 0.6176486697968906, 0.4194612577941394, 0.18642722568628417, 0.04660680642157104, 0.13982041926471314, 0.18642722568628417, 0.3682178126324329, 0.16876649745653174, 0.030684817719369407, 0.3989026303518023, 0.030684817719369407, 0.4322327366144435, 0.4648192633060225, 0.39841651140516215, 0.01660068797521509, 0.09960412785129054, 0.01660068797521509, 0.01660068797521509, 0.49324682411534326, 0.43343842404677363, 0.28895894936451577, 0.1402202101195332, 0.0701101050597666, 0.0701101050597666, 0.701101050597666], \"Term\": [\"abraham\", \"absurdity\", \"accident\", \"accidentally\", \"acs\", \"acs\", \"acs\", \"adam\", \"adam\", \"adam\", \"adaptor\", \"adb\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"address\", \"administration\", \"administration\", \"administration\", \"adobe\", \"advance\", \"advance\", \"advance\", \"advance\", \"affects\", \"agencies\", \"agencies\", \"aggression\", \"ahh\", \"ahh\", \"air\", \"air\", \"air\", \"air\", \"algorithm\", \"algorithm\", \"algorithm\", \"alias\", \"allah\", \"alomar\", \"amendment\", \"amour\", \"amp\", \"andrew\", \"andrew\", \"andrew\", \"andrew\", \"andrew\", \"andrew\", \"angels\", \"angels\", \"angels\", \"animation\", \"ankara\", \"apostles\", \"app\", \"app\", \"app\", \"apple\", \"apple\", \"approval\", \"apr\", \"apr\", \"apr\", \"apr\", \"apr\", \"apr\", \"arab\", \"arab\", \"arabs\", \"arabs\", \"arc\", \"archie\", \"areas\", \"armenian\", \"armenians\", \"armored\", \"army\", \"asking\", \"asking\", \"asking\", \"asking\", \"asking\", \"assaults\", \"assert\", \"assert\", \"assert\", \"astronomy\", \"astronomy\", \"atheism\", \"atheism\", \"atheist\", \"atheist\", \"atheist\", \"atheists\", \"atheists\", \"ati\", \"atm\", \"autocad\", \"automatics\", \"awesome\", \"awesome\", \"baerga\", \"banks\", \"banks\", \"baptism\", \"barrel\", \"baseball\", \"baseball\", \"basing\", \"batf\", \"battery\", \"beast\", \"bel\", \"bel\", \"belief\", \"belief\", \"beliefs\", \"beliefs\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"believed\", \"bell\", \"bell\", \"bell\", \"bell\", \"bell\", \"bhj\", \"bhj\", \"bible\", \"bible\", \"bible\", \"bible\", \"bike\", \"bio\", \"birthday\", \"birthday\", \"birthday\", \"bishop\", \"bitmap\", \"blessed\", \"blessed\", \"bmp\", \"bmp\", \"bmp\", \"bmp\", \"bmw\", \"bmw\", \"bodies\", \"bodies\", \"bomb\", \"bonds\", \"bonds\", \"bone\", \"bone\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"books\", \"books\", \"books\", \"books\", \"books\", \"books\", \"borders\", \"bos\", \"brain\", \"brain\", \"brain\", \"brain\", \"braves\", \"break\", \"brind\", \"bring\", \"bring\", \"bring\", \"bring\", \"bring\", \"brooks\", \"brought\", \"bruins\", \"buf\", \"buf\", \"business\", \"business\", \"business\", \"business\", \"bxn\", \"cadre\", \"cal\", \"cal\", \"cal\", \"calculator\", \"calculator\", \"callback\", \"callback\", \"canadiens\", \"capacitors\", \"cape\", \"captain\", \"car\", \"car\", \"carbs\", \"card\", \"card\", \"card\", \"card\", \"cardinals\", \"carnegie\", \"cars\", \"cartridges\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"cassels\", \"catbyte\", \"catcher\", \"catholic\", \"catholic\", \"celebrate\", \"cellular\", \"cellular\", \"cellular\", \"centaur\", \"centris\", \"cga\", \"champaign\", \"champions\", \"champions\", \"chances\", \"charger\", \"chastity\", \"cheapest\", \"cheat\", \"cheat\", \"cheers\", \"cheers\", \"cheers\", \"cheers\", \"cheers\", \"chemistry\", \"chemistry\", \"chi\", \"chicago\", \"chicago\", \"chicago\", \"children\", \"children\", \"children\", \"chip\", \"chip\", \"chip\", \"chip\", \"chips\", \"chips\", \"chips\", \"chips\", \"chipset\", \"chipset\", \"chocolate\", \"chocolate\", \"chris\", \"chris\", \"chris\", \"chris\", \"chris\", \"christ\", \"christ\", \"christ\", \"christian\", \"christian\", \"christian\", \"christian\", \"christian\", \"christianity\", \"christianity\", \"christianity\", \"christians\", \"christians\", \"christians\", \"christians\", \"church\", \"church\", \"church\", \"church\", \"cica\", \"cica\", \"cipher\", \"circuit\", \"circuits\", \"circuits\", \"civil\", \"classified\", \"classified\", \"clause\", \"cleveland\", \"cleveland\", \"cleveland\", \"clh\", \"clipper\", \"clipper\", \"clipper\", \"coach\", \"coaches\", \"coaches\", \"cobb\", \"cobb\", \"cobra\", \"coercion\", \"collision\", \"color\", \"color\", \"color\", \"color\", \"colorado\", \"colorado\", \"colormap\", \"colormap\", \"colormap\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"com\", \"combo\", \"combo\", \"comeback\", \"comm\", \"comm\", \"commitment\", \"commitment\", \"communist\", \"company\", \"company\", \"company\", \"condition\", \"condition\", \"condition\", \"condition\", \"connects\", \"constant\", \"constant\", \"constant\", \"constant\", \"constitution\", \"contact\", \"contact\", \"contact\", \"contact\", \"controller\", \"controller\", \"converter\", \"convertible\", \"cooling\", \"coprocessor\", \"corn\", \"corn\", \"course\", \"course\", \"course\", \"course\", \"course\", \"creation\", \"credible\", \"creed\", \"crimes\", \"cripple\", \"cruiser\", \"crypt\", \"crypto\", \"cryptography\", \"cubs\", \"cubs\", \"cubs\", \"cubs\", \"cult\", \"cursor\", \"cycles\", \"dave\", \"dave\", \"dave\", \"dave\", \"dave\", \"dave\", \"david\", \"david\", \"david\", \"david\", \"david\", \"david\", \"david\", \"david\", \"david\", \"davidian\", \"davidian\", \"davidsson\", \"davidsson\", \"dawn\", \"dawn\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dayton\", \"dayton\", \"dealership\", \"death\", \"death\", \"death\", \"deciding\", \"deed\", \"deed\", \"deeds\", \"defenseman\", \"defensively\", \"deficit\", \"deleted\", \"deleted\", \"deleted\", \"deleted\", \"deleted\", \"deleted\", \"dell\", \"democrats\", \"demonstrating\", \"demos\", \"denis\", \"denning\", \"des\", \"design\", \"design\", \"design\", \"designers\", \"designers\", \"deskjet\", \"destruction\", \"det\", \"detector\", \"detroit\", \"detroit\", \"detroit\", \"diagrams\", \"dial\", \"dial\", \"did\", \"did\", \"did\", \"did\", \"did\", \"did\", \"didn\", \"didn\", \"didn\", \"didn\", \"didn\", \"died\", \"difficulties\", \"directory\", \"directory\", \"dis\", \"disk\", \"disk\", \"disk\", \"distributor\", \"distributor\", \"ditto\", \"ditto\", \"division\", \"division\", \"division\", \"division\", \"division\", \"dod\", \"dod\", \"does\", \"does\", \"does\", \"does\", \"does\", \"does\", \"does\", \"does\", \"does\", \"doesn\", \"doesn\", \"doesn\", \"doesn\", \"doesn\", \"doesn\", \"dog\", \"dog\", \"don\", \"don\", \"don\", \"don\", \"don\", \"don\", \"dorothy\", \"dos\", \"dos\", \"dos\", \"dot\", \"doug\", \"doug\", \"doug\", \"doug\", \"doug\", \"dpi\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drivers\", \"drivers\", \"drivers\", \"driveway\", \"driving\", \"dsl\", \"dtmedin\", \"duke\", \"duke\", \"duke\", \"duo\", \"earth\", \"earth\", \"earth\", \"earth\", \"earth\", \"easter\", \"easter\", \"easter\", \"edge\", \"edit\", \"edm\", \"edu\", \"edu\", \"edu\", \"edu\", \"edu\", \"edu\", \"edu\", \"edu\", \"edu\", \"edu\", \"edu\", \"edu\", \"edu\", \"eff\", \"effects\", \"eisa\", \"email\", \"email\", \"email\", \"email\", \"email\", \"email\", \"email\", \"encrypted\", \"encryption\", \"encryption\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"enforcement\", \"enforcement\", \"engine\", \"era\", \"era\", \"era\", \"escrow\", \"escrowed\", \"esdi\", \"esdi\", \"espn\", \"eternal\", \"eternal\", \"ethernet\", \"event\", \"event\", \"event\", \"event\", \"evidence\", \"evidence\", \"evidence\", \"excellent\", \"excellent\", \"excellent\", \"excerpts\", \"existence\", \"existence\", \"expects\", \"expo\", \"expose\", \"expose\", \"extras\", \"fact\", \"fact\", \"fact\", \"fade\", \"fairing\", \"faith\", \"faith\", \"faith\", \"fallacy\", \"fan\", \"fan\", \"fans\", \"fans\", \"fascism\", \"fascism\", \"fate\", \"father\", \"father\", \"father\", \"father\", \"fbi\", \"fbi\", \"federal\", \"federal\", \"females\", \"fidonet\", \"fight\", \"file\", \"file\", \"file\", \"file\", \"file\", \"files\", \"files\", \"files\", \"films\", \"floppy\", \"floppy\", \"floppy\", \"flows\", \"fluke\", \"flyers\", \"flyers\", \"font\", \"font\", \"fonts\", \"fonts\", \"fooled\", \"ford\", \"foreground\", \"foreground\", \"foreground\", \"fork\", \"format\", \"format\", \"format\", \"formats\", \"formats\", \"forum\", \"forward\", \"fpu\", \"freeware\", \"freeware\", \"fri\", \"ftp\", \"ftp\", \"ftp\", \"ftp\", \"fujitsu\", \"fun\", \"functional\", \"fundamentalist\", \"fundamentalist\", \"gainey\", \"galileo\", \"galley\", \"game\", \"game\", \"game\", \"game\", \"game\", \"games\", \"games\", \"games\", \"games\", \"garrett\", \"gary\", \"gary\", \"gateway\", \"gay\", \"gay\", \"geb\", \"genocide\", \"ghetto\", \"gif\", \"gifs\", \"gifs\", \"giz\", \"god\", \"god\", \"god\", \"god\", \"god\", \"going\", \"going\", \"going\", \"going\", \"going\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gordon\", \"gordon\", \"gosh\", \"gov\", \"gov\", \"gov\", \"gov\", \"gov\", \"gov\", \"government\", \"government\", \"government\", \"government\", \"govt\", \"grabbed\", \"graphics\", \"graphics\", \"graphics\", \"graphics\", \"graphics\", \"graphics\", \"gravity\", \"greek\", \"greek\", \"gregory\", \"grin\", \"grounded\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"gsfc\", \"guerillas\", \"guns\", \"guns\", \"hall\", \"hall\", \"hall\", \"hall\", \"handgun\", \"hank\", \"har\", \"harmless\", \"harris\", \"harris\", \"harvard\", \"harvey\", \"harvey\", \"hawk\", \"hawk\", \"hawks\", \"heaven\", \"heaven\", \"heaven\", \"helicopters\", \"hell\", \"hell\", \"hell\", \"hell\", \"hellman\", \"help\", \"help\", \"help\", \"help\", \"henrik\", \"heterosexual\", \"hewlett\", \"hewlett\", \"hillary\", \"hit\", \"hit\", \"hit\", \"hobby\", \"hockey\", \"holy\", \"holy\", \"homosexual\", \"homosexual\", \"homosexual\", \"homosexuality\", \"homosexuals\", \"homosexuals\", \"homosexuals\", \"honda\", \"honda\", \"honda\", \"hood\", \"hook\", \"hook\", \"hook\", \"horses\", \"hospitals\", \"housing\", \"hst\", \"hudson\", \"huntsville\", \"huntsville\", \"hype\", \"hype\", \"icon\", \"icon\", \"icons\", \"icons\", \"icons\", \"idaho\", \"idaho\", \"ide\", \"ide\", \"idiotic\", \"ieee\", \"iisi\", \"iisi\", \"illinois\", \"illinois\", \"immune\", \"includes\", \"includes\", \"includes\", \"includes\", \"indiana\", \"indiana\", \"info\", \"info\", \"info\", \"info\", \"informatik\", \"infrared\", \"ingr\", \"ink\", \"ink\", \"innings\", \"innings\", \"instruct\", \"instruction\", \"instruction\", \"instruction\", \"insurance\", \"intel\", \"intel\", \"intellect\", \"intensity\", \"interested\", \"interested\", \"interested\", \"interested\", \"intergraph\", \"ireland\", \"irq\", \"irrational\", \"isa\", \"isa\", \"islam\", \"islamic\", \"islanders\", \"islanders\", \"israel\", \"israel\", \"israel\", \"israeli\", \"israeli\", \"israelis\", \"israelis\", \"israelites\", \"jagr\", \"jagr\", \"jagr\", \"jerk\", \"jerk\", \"jerk\", \"jersey\", \"jersey\", \"jersey\", \"jerusalem\", \"jesus\", \"jesus\", \"jesus\", \"jew\", \"jewish\", \"jewish\", \"jewish\", \"jews\", \"jews\", \"jobs\", \"johnson\", \"johnson\", \"johnson\", \"jointly\", \"joke\", \"joke\", \"joke\", \"joystick\", \"joystick\", \"judaism\", \"judge\", \"judge\", \"jupiter\", \"jupiter\", \"just\", \"just\", \"just\", \"just\", \"just\", \"just\", \"just\", \"just\", \"keenan\", \"kent\", \"kent\", \"kent\", \"kent\", \"key\", \"key\", \"key\", \"keys\", \"keys\", \"killed\", \"killed\", \"killing\", \"killing\", \"kingdom\", \"kingdom\", \"kingman\", \"knife\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"koresh\", \"koresh\", \"kurds\", \"larc\", \"larc\", \"laser\", \"laser\", \"laser\", \"laserjet\", \"launches\", \"law\", \"law\", \"law\", \"law\", \"lds\", \"leafs\", \"league\", \"league\", \"leagues\", \"leaks\", \"lebanese\", \"left\", \"left\", \"left\", \"leftover\", \"legislation\", \"lerc\", \"level\", \"level\", \"level\", \"level\", \"libertarians\", \"licensing\", \"life\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"lineup\", \"lineup\", \"liquid\", \"liquid\", \"lisa\", \"listserv\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"looking\", \"looking\", \"looking\", \"looking\", \"looking\", \"loser\", \"loser\", \"lost\", \"lost\", \"lost\", \"lost\", \"lost\", \"lunar\", \"lutheran\", \"lyme\", \"mac\", \"mac\", \"mail\", \"mail\", \"mail\", \"mail\", \"mail\", \"mail\", \"mailing\", \"mailing\", \"mailing\", \"mailing\", \"mailing\", \"mailing\", \"maine\", \"maine\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"manager\", \"manager\", \"manager\", \"manager\", \"manual\", \"manual\", \"manual\", \"manual\", \"manuals\", \"manuals\", \"map\", \"map\", \"map\", \"mapped\", \"marriage\", \"marriage\", \"mask\", \"mask\", \"mask\", \"math\", \"math\", \"matrix\", \"max\", \"max\", \"max\", \"mcrae\", \"meant\", \"meant\", \"meant\", \"medin\", \"mellon\", \"members\", \"members\", \"mercedes\", \"mercedes\", \"messenger\", \"mets\", \"metzger\", \"micron\", \"microsoft\", \"migraine\", \"migraine\", \"militia\", \"min\", \"min\", \"min\", \"minnesota\", \"miracles\", \"mississippi\", \"mlb\", \"moa\", \"modem\", \"modem\", \"modems\", \"mon\", \"mon\", \"mon\", \"mon\", \"mon\", \"mon\", \"money\", \"money\", \"money\", \"monitor\", \"monitor\", \"monitor\", \"monitors\", \"montana\", \"montreal\", \"moon\", \"moon\", \"moore\", \"moral\", \"moral\", \"moral\", \"morality\", \"morality\", \"morals\", \"morals\", \"mormon\", \"mormons\", \"mormons\", \"mother\", \"mother\", \"mother\", \"mother\", \"motherboards\", \"motherboards\", \"motorcycling\", \"mouse\", \"mouse\", \"moving\", \"msdos\", \"msdos\", \"msdos\", \"msg\", \"muslims\", \"mvp\", \"mvp\", \"mwm\", \"naive\", \"nasa\", \"nasa\", \"nasa\", \"nasa\", \"national\", \"national\", \"national\", \"nazis\", \"ncsa\", \"ncsl\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"neil\", \"netcom\", \"netcom\", \"netcom\", \"networking\", \"networking\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newest\", \"news\", \"news\", \"news\", \"news\", \"news\", \"nhl\", \"nist\", \"northeast\", \"norton\", \"notice\", \"nsa\", \"nth\", \"ntsc\", \"nyi\", \"nyr\", \"objective\", \"objective\", \"objective\", \"objective\", \"obo\", \"obo\", \"obsolete\", \"occupying\", \"octopus\", \"oem\", \"offended\", \"offer\", \"offer\", \"offer\", \"offer\", \"offers\", \"offers\", \"offers\", \"ohm\", \"ohm\", \"olwm\", \"openlook\", \"orbit\", \"orbit\", \"orbit\", \"orbital\", \"orbital\", \"orchid\", \"organizers\", \"orientation\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"orthodox\", \"orthodox\", \"orthodox\", \"ott\", \"packard\", \"packard\", \"padres\", \"pairs\", \"pal\", \"palestine\", \"palestinean\", \"palestineans\", \"palestinian\", \"paperback\", \"paradise\", \"paradise\", \"paradox\", \"partition\", \"parts\", \"parts\", \"pass\", \"pds\", \"peace\", \"peace\", \"peace\", \"pens\", \"pentium\", \"pentium\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"perry\", \"perry\", \"petri\", \"pex\", \"pgp\", \"pgp\", \"pharisees\", \"phi\", \"phigs\", \"phillies\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phones\", \"physician\", \"physician\", \"picks\", \"pirates\", \"pirates\", \"pit\", \"pitcher\", \"pitching\", \"pitt\", \"pittsburgh\", \"pittsburgh\", \"pixmap\", \"pixmap\", \"pkp\", \"plaintext\", \"plant\", \"plant\", \"plants\", \"plants\", \"play\", \"play\", \"play\", \"play\", \"played\", \"played\", \"player\", \"player\", \"player\", \"players\", \"players\", \"playoffs\", \"pmetzger\", \"political\", \"population\", \"popup\", \"popup\", \"port\", \"port\", \"port\", \"ported\", \"ported\", \"ports\", \"postage\", \"postage\", \"postscript\", \"postscript\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"powerpc\", \"powerpc\", \"preached\", \"president\", \"president\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"price\", \"price\", \"price\", \"price\", \"priest\", \"printer\", \"printer\", \"printers\", \"privacy\", \"privacy\", \"probably\", \"probably\", \"probably\", \"probably\", \"probably\", \"probably\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"profit\", \"profit\", \"profit\", \"program\", \"program\", \"program\", \"program\", \"program\", \"proportional\", \"proportional\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"propulsion\", \"protestant\", \"proton\", \"proton\", \"providence\", \"pts\", \"pub\", \"pub\", \"public\", \"public\", \"public\", \"public\", \"public\", \"puck\", \"quack\", \"quack\", \"quadra\", \"quadra\", \"que\", \"que\", \"qur\", \"quran\", \"race\", \"radar\", \"radar\", \"radiator\", \"ram\", \"ram\", \"ram\", \"randy\", \"randy\", \"range\", \"rangers\", \"rape\", \"rape\", \"reactor\", \"reagan\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"rear\", \"rear\", \"recchi\", \"rectangle\", \"rectangle\", \"redesign\", \"refresh\", \"refresh\", \"refusal\", \"reject\", \"relay\", \"religion\", \"religion\", \"religion\", \"religious\", \"religious\", \"religious\", \"rental\", \"republicans\", \"resistant\", \"resolutions\", \"revelation\", \"revelation\", \"reverse\", \"ride\", \"riding\", \"riding\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"rights\", \"rights\", \"rights\", \"rights\", \"risc\", \"risc\", \"risc\", \"rll\", \"robert\", \"robert\", \"robert\", \"robert\", \"robert\", \"robert\", \"roberts\", \"rochester\", \"rochester\", \"romans\", \"royals\", \"royals\", \"rsa\", \"rubber\", \"rubber\", \"rubber\", \"runs\", \"runs\", \"runs\", \"sabbath\", \"sabbath\", \"sabbath\", \"said\", \"said\", \"said\", \"said\", \"said\", \"said\", \"said\", \"saint\", \"sale\", \"sale\", \"sale\", \"salvation\", \"salvation\", \"sam\", \"sam\", \"sam\", \"sam\", \"sam\", \"sandberg\", \"sanderson\", \"sandy\", \"sarcasm\", \"sarcasm\", \"satan\", \"satan\", \"satan\", \"savard\", \"say\", \"say\", \"say\", \"say\", \"say\", \"scheme\", \"scheme\", \"school\", \"school\", \"school\", \"school\", \"school\", \"science\", \"science\", \"science\", \"science\", \"science\", \"sciences\", \"scientific\", \"scorer\", \"scoring\", \"screen\", \"screen\", \"screen\", \"scsi\", \"scsi\", \"scsi\", \"sdio\", \"season\", \"season\", \"season\", \"secret\", \"secret\", \"secure\", \"secure\", \"security\", \"security\", \"security\", \"sell\", \"sell\", \"sell\", \"semiconductor\", \"semiconductor\", \"send\", \"send\", \"send\", \"send\", \"send\", \"send\", \"send\", \"serve\", \"server\", \"server\", \"server\", \"services\", \"services\", \"services\", \"sexual\", \"sexual\", \"shaft\", \"shaft\", \"shaft\", \"shameful\", \"shareware\", \"shareware\", \"shareware\", \"sharks\", \"shearson\", \"shift\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shock\", \"sig\", \"sig\", \"sig\", \"sig\", \"signal\", \"simmons\", \"simms\", \"simms\", \"sin\", \"sin\", \"sin\", \"skepticism\", \"skipjack\", \"soccer\", \"socialists\", \"societies\", \"software\", \"software\", \"software\", \"soldiers\", \"son\", \"son\", \"son\", \"soon\", \"soon\", \"soon\", \"soon\", \"soviet\", \"sox\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"spacecraft\", \"spacecraft\", \"spelled\", \"spirit\", \"spirit\", \"springfield\", \"src\", \"ssd\", \"ssto\", \"stacker\", \"stage\", \"stage\", \"stage\", \"state\", \"state\", \"state\", \"state\", \"stats\", \"stats\", \"std\", \"steam\", \"steam\", \"stefan\", \"stephenson\", \"sticker\", \"stl\", \"stomach\", \"stomach\", \"string\", \"string\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"subdirectory\", \"subjective\", \"subscribe\", \"subscribe\", \"suck\", \"suck\", \"suck\", \"sue\", \"sue\", \"suit\", \"suit\", \"suit\", \"sunrise\", \"sunview\", \"surprise\", \"surprise\", \"surprise\", \"surrender\", \"surrender\", \"survey\", \"survivor\", \"survivor\", \"svga\", \"swapping\", \"swelling\", \"swelling\", \"sys\", \"talent\", \"tartar\", \"tasks\", \"teachings\", \"team\", \"team\", \"team\", \"teams\", \"teams\", \"ted\", \"ted\", \"ted\", \"tells\", \"tells\", \"tells\", \"tells\", \"temperature\", \"temperature\", \"temperatures\", \"tempest\", \"terminals\", \"terminals\", \"test\", \"test\", \"test\", \"test\", \"tgv\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"theory\", \"theory\", \"theory\", \"thereof\", \"thereof\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"things\", \"things\", \"things\", \"things\", \"things\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thou\", \"thou\", \"thread\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tire\", \"tires\", \"tires\", \"title\", \"title\", \"title\", \"title\", \"today\", \"today\", \"today\", \"today\", \"today\", \"todd\", \"todd\", \"token\", \"token\", \"toner\", \"toner\", \"tonight\", \"tonight\", \"tor\", \"toronto\", \"toronto\", \"toronto\", \"towers\", \"traded\", \"traded\", \"traded\", \"trades\", \"trades\", \"train\", \"train\", \"translate\", \"treatment\", \"treaty\", \"trillion\", \"true\", \"true\", \"true\", \"true\", \"true\", \"truetype\", \"truth\", \"truth\", \"truth\", \"turkish\", \"turkish\", \"turn\", \"turn\", \"turn\", \"tvtwm\", \"ucs\", \"udel\", \"ufl\", \"uiuc\", \"uiuc\", \"uiuc\", \"uiuc\", \"ulf\", \"unity\", \"university\", \"university\", \"university\", \"university\", \"university\", \"university\", \"unix\", \"urbana\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"using\", \"using\", \"using\", \"using\", \"uucp\", \"uucp\", \"uucp\", \"uunet\", \"uunet\", \"uunet\", \"uunet\", \"uunet\", \"values\", \"values\", \"values\", \"van\", \"van\", \"van\", \"vat\", \"vehicle\", \"venus\", \"versa\", \"verses\", \"verses\", \"version\", \"version\", \"version\", \"version\", \"version\", \"vesa\", \"vesselin\", \"vga\", \"vga\", \"video\", \"video\", \"video\", \"villages\", \"vinyl\", \"vinyl\", \"virgin\", \"virginia\", \"virginia\", \"virginia\", \"vlb\", \"vlb\", \"volunteer\", \"vram\", \"waco\", \"wagon\", \"wagon\", \"wallet\", \"want\", \"want\", \"want\", \"want\", \"want\", \"war\", \"war\", \"war\", \"war\", \"water\", \"water\", \"water\", \"watson\", \"wave\", \"wave\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"weiss\", \"widget\", \"widget\", \"widgets\", \"win\", \"win\", \"win\", \"win\", \"window\", \"window\", \"window\", \"window\", \"windows\", \"windows\", \"windows\", \"winfield\", \"winnipeg\", \"winnipeg\", \"wiretap\", \"women\", \"women\", \"word\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"wuarchive\", \"wustl\", \"xcreatewindow\", \"xdm\", \"xerox\", \"xputimage\", \"xterm\", \"xterm\", \"xwindows\", \"xwindows\", \"yankees\", \"yankees\", \"yanks\", \"yeah\", \"yeah\", \"yeah\", \"yeah\", \"yeah\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yearly\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"yount\", \"yup\", \"yup\", \"zip\", \"zip\", \"zip\", \"zip\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [16, 7, 13, 6, 11, 19, 5, 8, 12, 14, 10, 4, 15, 1, 2, 9, 18, 17, 20, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el9851120818824486109621610\", ldavis_el9851120818824486109621610_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el9851120818824486109621610\", ldavis_el9851120818824486109621610_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el9851120818824486109621610\", ldavis_el9851120818824486109621610_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "15     0.195576  0.043593       1        1  34.043427\n",
       "6      0.120526  0.138568       2        1  20.066897\n",
       "12     0.299475 -0.132850       3        1  12.540161\n",
       "5      0.017137  0.188807       4        1   5.209774\n",
       "10     0.104041  0.180372       5        1   4.859943\n",
       "18     0.139450 -0.149233       6        1   3.561402\n",
       "4     -0.120754 -0.015554       7        1   2.472662\n",
       "7     -0.052732 -0.030381       8        1   2.044849\n",
       "11    -0.029780 -0.073130       9        1   1.982630\n",
       "13    -0.087406  0.005970      10        1   1.504950\n",
       "9     -0.074859 -0.028866      11        1   1.408547\n",
       "3     -0.025622 -0.053174      12        1   1.304860\n",
       "14    -0.064001 -0.011279      13        1   1.287910\n",
       "0     -0.072994 -0.015142      14        1   1.273234\n",
       "1     -0.068393 -0.004103      15        1   1.262125\n",
       "8     -0.064321 -0.018974      16        1   1.201762\n",
       "17    -0.058346 -0.009243      17        1   1.042810\n",
       "16    -0.054621 -0.008681      18        1   0.992643\n",
       "19    -0.053633 -0.008068      19        1   0.970807\n",
       "2     -0.048743  0.001367      20        1   0.968607, topic_info=     Category        Freq    Term       Total  loglift  logprob\n",
       "4497  Default   52.000000     key   52.000000  30.0000  30.0000\n",
       "3523  Default   82.000000     god   82.000000  29.0000  29.0000\n",
       "2653  Default   90.000000     edu   90.000000  28.0000  28.0000\n",
       "8252  Default  112.000000  thanks  112.000000  27.0000  27.0000\n",
       "1322  Default   42.000000    chip   42.000000  26.0000  26.0000\n",
       "...       ...         ...     ...         ...      ...      ...\n",
       "8267  Topic20    1.719096  theory   16.841984   2.3550  -5.7826\n",
       "8906  Topic20    1.228214    wave    8.026150   2.7599  -6.1188\n",
       "1002  Topic20    1.138414   bring   14.067291   2.1229  -6.1947\n",
       "912   Topic20    1.206459    book   37.538219   1.1994  -6.1367\n",
       "913   Topic20    1.079081   books   20.675158   1.6842  -6.2483\n",
       "\n",
       "[1011 rows x 6 columns], token_table=      Topic      Freq          Term\n",
       "term                               \n",
       "13       10  0.696130       abraham\n",
       "24       19  0.444972     absurdity\n",
       "49        2  0.821916      accident\n",
       "51       14  0.746203  accidentally\n",
       "88        2  0.249232           acs\n",
       "...     ...       ...           ...\n",
       "9131     18  0.288959           yup\n",
       "9140      1  0.140220           zip\n",
       "9140      6  0.070110           zip\n",
       "9140      8  0.070110           zip\n",
       "9140     12  0.701101           zip\n",
       "\n",
       "[1994 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[16, 7, 13, 6, 11, 19, 5, 8, 12, 14, 10, 4, 15, 1, 2, 9, 18, 17, 20, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USE sklearn: https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb\n",
    "# (TODO) USING GRAPHLAB: Adapted from: https://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb#topic=0&lambda=1&term=\n",
    "\n",
    "# imports for visualizing LDA\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# preparing data set\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "newsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "docs_raw = newsgroups.data\n",
    "print(len(docs_raw))\n",
    "\n",
    "# Convert to document-term matrix\n",
    "# Next, the raw documents are converted into document-term matrix, possibly as raw counts or in TF-IDF form.\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(docs_raw)\n",
    "print(dtm_tf.shape)\n",
    "\n",
    "# Use the TF-IDF vectorizer and transformer --> use TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(docs_raw)\n",
    "print(dtm_tfidf.shape)\n",
    "\n",
    "\n",
    "# Fit Latent Dirichlet Allocation models\n",
    "# Finally, the LDA models are fitted.\n",
    "\n",
    "# for TF DTM\n",
    "#lda_tf = LatentDirichletAllocation(n_topics=20, random_state=0)\n",
    "#lda_tf.fit(dtm_tf)\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "lda_tfidf.fit(dtm_tfidf)\n",
    "\n",
    "# pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)\n",
    "\n",
    "# using the tf-idf model to visualize TF-IDF\n",
    "pyLDAvis.sklearn.prepare(lda_tfidf, dtm_tfidf, tfidf_vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below this is archived LDA code from towards data science; very solid stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-da844e728b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdt_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'T1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'T2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'T3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_topics'"
     ]
    }
   ],
   "source": [
    "# TODO: LDA Example code\n",
    "# Source: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "# TODO: Looks like we can apply LDA on many different matrices, including count vector or tfidf vector\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=3, max_iter=10000, random_state=0)\n",
    "dt_matrix = lda.fit_transform(cv_matrix)\n",
    "features = pd.DataFrame(dt_matrix, columns=['T1', 'T2', 'T3'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: View the words for the topics\n",
    "# Source: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "tt_matrix = lda.components_\n",
    "for topic_weights in tt_matrix:\n",
    "    topic = [(token, weight) for token, weight in zip(vocab, topic_weights)]\n",
    "    topic = sorted(topic, key=lambda x: -x[1])\n",
    "    topic = [item for item in topic if item[1] > 0.6]\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.640s.\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 0.435s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.431s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.407s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
      "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
      "Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
      "Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 1.828s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: just people don like did know make really right think say things time look way didn ve course probably good\n",
      "Topic #1: help thanks windows know hi need using does looking anybody appreciated card mail software use info email ftp available pc\n",
      "Topic #2: does god believe know mean true christians read point jesus christian church come people fact says religion say agree bible\n",
      "Topic #3: know thanks mail interested like new just bike email edu advance want contact really list heard com post hear information\n",
      "Topic #4: 10 new 30 12 20 50 11 sale 16 15 time 14 old power ago good 100 great offer cost\n",
      "Topic #5: number 1993 data subject government new numbers provide information space following com research include large note group major time talk\n",
      "Topic #6: edu problem file com remember try soon article mike files code program sun free send think cases manager little called\n",
      "Topic #7: game year team games world fact second case won said win division play best clearly claim allow example used doesn\n",
      "Topic #8: think don drive hard need bit mac make sure read apple going comes disk computer case pretty drives software ve\n",
      "Topic #9: good just use like doesn got way don ll going does chip better doing bad key want sure bit car\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 6.229s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
      "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
      "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
      "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
      "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
      "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
      "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
      "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
      "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
      "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Example code from https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Word Clouds: Getting the Gist\n",
    "\n",
    "Word Clouds are an intuitively visual way to present vast amounts of textual information, revealing the key themes conveyed in a collection of documents. Prima facie, generating word clouds feel like a simple problem, yet very quickly deeper considerations arise, including (1) using appropriate preprocessing (stop words, stemming, lemmatization), (2) whether to include documents of class $j \\neq i$ when constructing a word cloud for documents of class, and (3) how to scale tokens, given (1) and (2). In this section, we explore combinations of different approaches, and conclude that the most suitable approach depends on our philosophy in constructing these word clouds.\n",
    "\n",
    "\n",
    "## 7.1 Baseline Wordclouds\n",
    "\n",
    "`TODO`\n",
    "\n",
    "For our baselines word clouds for documents of class i, we perform basic stopwords removal, with no stemming and lemmatization. We use: \n",
    "- Token raw frequency (not considering other classes $\\neq i$)\n",
    "- TF-IDF weighting (considering other classes $j \\neq i$)\n",
    "\n",
    "Note that since each trip report may contain multiple classes, we will include a document that contains k classes in k word clouds.\n",
    "\n",
    "## 7.2 Intermediate Wordclouds\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 7.3 Log Odds Ratio, an homage to Statistical Inference\n",
    "\n",
    "`TODO`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Erowid Corpus with Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 | Retrieve the reports for different substances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using report, title, substance, substance.unique_label as id variables\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 6 x 2\u001b[39m\n",
      "  title                           `n()`\n",
      "  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                           \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m ::: The City of Light :::          21\n",
      "\u001b[90m2\u001b[39m ... Meh                            21\n",
      "\u001b[90m3\u001b[39m ...But I Did Everything Right!     21\n",
      "\u001b[90m4\u001b[39m ...Until the Crystal Cracked!      21\n",
      "\u001b[90m5\u001b[39m ...When I'm Closing in on Death    21\n",
      "\u001b[90m6\u001b[39m .3 PPM of Our Air                  21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0d07c5064cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'R'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-o tripReports -o tripReports.long'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'# ^ export tripReports and tripReports.long for use in python environment\\n# https://stackoverflow.com/questions/55841165/share-variables-between-r-and-python-in-jupyternotebook\\n\\n### create long data format, using report, title, substance, substance.unique_label as id variables\\n# glimpse(tripReports)\\ntripReports.long <- reshape2::melt(tripReports)\\n\\n### validate the long data format\\ntripReports.long %>% select(-report) %>% group_by(title) %>% summarize(n()) %>% head()\\n# glimpse(tripReports.long)\\n# glimpse(tripReports)\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</Users/alextzhao/opt/anaconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-130>\u001b[0m in \u001b[0;36mR\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/ipython/rmagic.py\u001b[0m in \u001b[0;36mR\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mlocalconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m                     \u001b[0moutput_ipy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobalenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_ipy\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/environments.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, item, wantfun)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \"\"\"\n\u001b[1;32m     56\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwantfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;31m# TODO: There is a design issue here. The attribute __rname__ is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# intended to store the symbol name of the R object but this is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/pandas2ri.py\u001b[0m in \u001b[0;36mrpy2py_listvector\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrpy2py_listvector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'data.frame'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy2ri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/pandas2ri.py\u001b[0m in \u001b[0;36mrpy2py_dataframe\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrpy2py_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     items = OrderedDict((k, rpy2py(v) if isinstance(v, Sexp) else v)\n\u001b[0;32m--> 219\u001b[0;31m                         for k, v in obj.items())\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPandasDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrownames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/pandas2ri.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrpy2py_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     items = OrderedDict((k, rpy2py(v) if isinstance(v, Sexp) else v)\n\u001b[0m\u001b[1;32m    219\u001b[0m                         for k, v in obj.items())\n\u001b[1;32m    220\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPandasDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/vectors.py\u001b[0m in \u001b[0;36mitems\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mit_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mit_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/vectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/pandas2ri.py\u001b[0m in \u001b[0;36mri2py_vector\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSexpVector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mri2py_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy2ri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rpy2/robjects/numpy2ri.py\u001b[0m in \u001b[0;36mrpy2py_sexp\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrpy2py_sexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_vectortypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mRTYPES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVECSXP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%R -o tripReports -o tripReports.long\n",
    "# ^ export tripReports and tripReports.long for use in python environment\n",
    "# https://stackoverflow.com/questions/55841165/share-variables-between-r-and-python-in-jupyternotebook\n",
    "\n",
    "### create long data format, using report, title, substance, substance.unique_label as id variables\n",
    "# glimpse(tripReports)\n",
    "tripReports.long <- reshape2::melt(tripReports)\n",
    "\n",
    "### validate the long data format\n",
    "tripReports.long %>% select(-report) %>% group_by(title) %>% summarize(n()) %>% head()\n",
    "# glimpse(tripReports.long)\n",
    "# glimpse(tripReports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alextzhao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Use the wordcloud package to generate baseline wordclouds\n",
    "# https://github.com/amueller/word_cloud\n",
    "# https://www.datacamp.com/community/tutorials/wordcloud-python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from wordcloud import STOPWORDS as wordcloud_STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # utilities to interoperate between R and pandas data frames\n",
    "# # see https://rpy2.github.io/doc/latest/html/pandas.html\n",
    "# import rpy2.robjects as ro\n",
    "# from rpy2.robjects.packages import importr\n",
    "# from rpy2.robjects import pandas2ri\n",
    "# from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "# # convert the R data frame to a pandas dataframe\n",
    "# with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "#   pd_tripReports = ro.conversion.rpy2py(tripReports)\n",
    "\n",
    "# NOTE: We can just read directly from the csv file that we saved earlier\n",
    "pd_tripReports = pd.read_csv(\"tripReportsEncoded.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare a few substances lists for use\n",
    "\n",
    "# a list of all the substances we want word clouds for\n",
    "substances_all_list = pd_tripReports.loc[:, 'substance_mushrooms':'substance_syrian_rue'].columns.tolist()\n",
    "# [ 'mushrooms', 'lsd', 'cannabis', ... etc] for use as stopwords\n",
    "substances_all_plainstring_list = [substance.replace(\"substance_\", \"\") for substance in substances_all_list]\n",
    "\n",
    "# TODO: Prepare regexes to match with all the observed spellings of different substances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the text for interested substances\n",
    "# a dictionary mapping {substance_<name> : text}\n",
    "reports_text = {}\n",
    "# Concatenate with \" \" by default, TODO: Can specify separator as such: str.cat(sep=\"...\")\n",
    "for substance in substances_all_list:\n",
    "    reports_text[substance] = pd_tripReports.query(\"{} == 1\".format(substance))[\"report\"].str.cat().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.3 |  Stemming and Lemmatizing the text\n",
    "\n",
    "For our baseline results, stemming and lemmatizing is optional; nevertheless, we include the procedures here as an extension, and for the reader to explore.  \n",
    "\n",
    "`Stemming` refers to the often crude procedure of removing the ends of words to find the \"word stem\"; e.g.: walks -> walk;`Porter Stemmer`, `Lovins Stemmer`, `Paice Stemmer` are three examples of commonly used stemming algorithm. Some `Porter Stemmer` rules are found below ([source](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html))\n",
    "\n",
    "![Example of Porter Stemmer](images/infographic_porter-stemmer.png)\n",
    "\n",
    "`Lemmatization`, to quote verbatim [Stanford's Information Retrival Texbook](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), \"usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma\". In other words, lemmatization returns the \"cannonical\" form of a word.\n",
    "\n",
    "Some examples of lemmatization: ([source](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/))\n",
    "- rocks : rock\n",
    "- corpora : corpus\n",
    "- better : good\n",
    "\n",
    "In a very real sense, lemmatization is a more complex procedure and can often produce more desirable results  \n",
    "However, it is interesting to note that with sufficiently large datasets in industrial machine learning applications (such as those done at Google), stemming and lemmatization are uncommonly used (**source**: in conversation with [Lyle Ungar](https://www.cis.upenn.edu/~ungar/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an upper',\n",
       " 'base',\n",
       " 'bath salt',\n",
       " 'dream herbal incense)',\n",
       " 'ecstasy (unknown',\n",
       " 'indian leaf',\n",
       " 'london underground dream)',\n",
       " 'magic silver',\n",
       " 'sensory deprivation',\n",
       " 'white rock opium',\n",
       " '(15x extract)',\n",
       " '(2c-t-7',\n",
       " '(amt + mdma)',\n",
       " '(amt + methamphetamine)',\n",
       " '(hydrocodone',\n",
       " '(methyl',\n",
       " '(quetiapine) seroquel',\n",
       " '1',\n",
       " '1 soma',\n",
       " '1-4 butanediol',\n",
       " '10x',\n",
       " '10x extract)',\n",
       " '10x extracts)',\n",
       " '15x extracts)',\n",
       " '19-norandrostenedione (nor-19)',\n",
       " '2-aminoindan',\n",
       " '2-aminoindan (im)',\n",
       " '2-c-t-2',\n",
       " '2-cb',\n",
       " '2-cd']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTIONAL\n",
    "# Lammatization\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 | Generating Baseline Wordclouds\n",
    "\n",
    "#### 1.1.3.1 | Wordclouds Based on Word Frequency\n",
    "\n",
    "Note: Using the `wordcloud` package, we are making numerous assumptions, including:\n",
    "- Word scaling based on `frequency` only\n",
    "-  of words: the `wordcloud` package has a `relative_scaling` attribute, which specifies the importance of relative word frequencies for font-size. If `relative_scaling = 0`, the only word-ranks are considered; `relative_scaling=1` means words size is directly proportional to their frequency; Default of `relative_scaling = 0.5` considers both word frequency and word rank, and is used in our analysis. Insight retrieved from `wordcloud` [documentation](http://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud) and from [this tutorial](https://www.datacamp.com/community/tutorials/wordcloud-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now plotting wordclouds for 5 substances...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6c346c932534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# creating subplots: https://stackoverflow.com/questions/25239933/how-to-add-title-to-subplots-in-matplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# changing figure size: https://stackoverflow.com/questions/332289/how-do-you-change-the-size-of-figures-drawn-with-matplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Word Clouds based on word Frequencies\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#### Create Wordclouds\n",
    "# a list of the \"classical psychedelics\"\n",
    "substances_psychedelics_list = [\"substance_mushrooms\", \"substance_lsd\", \"substance_mescaline\", \"substance_5_meo_dmt\", \"substance_dmt\"]\n",
    "\n",
    "\n",
    "# NOTE: change this line of code to change which substances to plot\n",
    "substances_to_plot = substances_psychedelics_list\n",
    "num_plots = len(substances_to_plot)\n",
    "print(\"Now plotting wordclouds for {} substances...\".format(num_plots))\n",
    "\n",
    "# creating subplots: https://stackoverflow.com/questions/25239933/how-to-add-title-to-subplots-in-matplotlib\n",
    "# changing figure size: https://stackoverflow.com/questions/332289/how-do-you-change-the-size-of-figures-drawn-with-matplotlib\n",
    "fig = plt.figure(figsize=(50, 10))\n",
    "fig.suptitle(\"Word Clouds based on word Frequencies\", fontsize=16)\n",
    "\n",
    "for index, substance in enumerate(substances_to_plot):\n",
    "    wordcloud = WordCloud(stopwords=stopword_list, max_font_size=50, max_words=100, background_color=\"white\").generate(reports_text_tokenized[substance])\n",
    "    ax = plt.subplot(num_plots, 1, index+1) # note here for subplots index must be [1, num_plots]\n",
    "    ax.set_title(\"{} Wordcloud\".format(substance))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.savefig('wordclouds_naive_psychedelics.png')\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "### Plotting a single wordcloud\n",
    "# wordcloud = WordCloud(stopwords=stopwords_all, max_font_size=50, max_words=100, background_color=\"white\").generate(reports_text_tokenized[\"substance_mushrooms\"])\n",
    "# plt.figure()\n",
    "# plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3.2 Wordclouds Based on `tf-idf`\n",
    "\n",
    "Using just the raw word frequencies, we see many of the most commonly used words are shared across all trip reports, giving us little if any insight into the most significant words that is associated with each type of substance. To improve upon the raw frequency baseline, we use use the `tf-idf` for each (word, document) pair as a weighting term.\n",
    "\n",
    "`tf-idf` stands for `term frequency-inverse document frequency`, and is specified for each (word, document) pair. It is by far one of the most widely used weighting scheme in considering how important a word is to a document, and is widely used in text mining and information retrieval. For an approachable academic introduction to `tf-idf`, see Chris Callison-Burch's [lecture slides](http://computational-linguistics-class.org/slides/05-vector-semantics.pdf)  \n",
    "\n",
    "- `tf` stands for `Term Frequency` ([Luhn 1957](http://web.stanford.edu/class/linguist289/luhn57.pdf)). The term trequency of word i in document j is given by:\n",
    "$tf_{ij}$ = # of times word $i$ appears in document $j$\n",
    "\n",
    "- `idf` stands for `Inverse Document Frequency` ([Spark Jones 1972](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.8343&rep=rep1&type=pdf)). The inverse document frequency of word $i$ is given by\n",
    "\n",
    "$$idf_i = log\\left(\\cfrac{N}{df_i}\\right)$$\n",
    "\n",
    "Where $df_i$ is the `document frequency of word i`, the number of documents containing word $i$  \n",
    "\n",
    "With these definitions, we stay the tf-idf weighting of word $i$ in document $j$ is given by\n",
    "\n",
    "$$ tf.idf(i,j) = tf_{ij} \\cdot idf_{i}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace numbers from corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.2.1 `tfidf wordclouds` >>> Using only 19 different documents\n",
    "\n",
    "We first group all the reports for a particular substance into one document, for a total of 19 documents or `samples`. Under this schema, each substance has precisely one document, which is the concatenation of all the reports corresponding to that substance. Note that there are concerns with doing this, as `idf` typically assumes a large number of documents in the corpora; we will explore an alternative approach later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## See TFidfVectorizer docs: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.csr import csr_matrix # need this if we want to save tfidf_matrix; see https://stackoverflow.com/questions/34449127/sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "\n",
    "\n",
    "# NOTE: reports_text_tokenized stores the tokenized corpus\n",
    "# List of substances under consideration, sorted to preserve alphabetical ordering\n",
    "substances_list_sorted = sorted(list(reports_text_tokenized.keys()))\n",
    "substances_list_sorted\n",
    "\n",
    "# TfidfVectorizer expects a list of strings, so we prepare the corpus in the appropriate format\n",
    "corpus_psychedelics_sorted = []\n",
    "for substance in substances_list_sorted:\n",
    "    corpus_psychedelics_sorted.append(reports_text_tokenized[substance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on the specification of TfidfVectorizer; additional information found in [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "- `strip_accents=ascii` helps us strip accented characters such as `√°` to `a`\n",
    "- `lowercase=True`: automatically converts all the text to lowercase\n",
    "- `analyzer=word`: considers words as the basic unigram unit; we can also do a character level model with `char`\n",
    "- `stop_words=english`: uses built in stopwords given by `sklearn`; Follow these [instructions](https://awhan.wordpress.com/2016/06/05/scikit-learn-nlp-list-english-stopwords/) to view all the stopwords for `sklearn` and `nltk`\n",
    "- `ngram_range=(1, 1)`: setting to (1, 1) gives us unigrams. For bigrams and trigrams, we use (1, 2) and (1, 3) respectively\n",
    "- `max_df=1.0`: using the float `1.0` sets the maximum document frequency to 100\\% of the documents\n",
    "- `min_df=1`: ignores all words with document frequency less than 1\n",
    "- `max_features=None`: Don't put a cap on the number of features\n",
    "- `binary=False`: Use the tf counts, instead of setting all non-zero counts to 1\n",
    "- `dtype=float64`: Default datatype for tf-idf values\n",
    "- `norm=l2`: uses the L2 norm for each row of tf-idf values, so each row sums to 1\n",
    "- `use_idf=True`: use idf weighting (instead of just tf)\n",
    "- `smooth_idf=True`: this is akin to laplace smoothing, and helps us prevent 0 divisions\n",
    "- `sublinear_tf=False`: Do not replace tf with 1 + log(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alextzhao/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1616: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. float64 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# create and use a TfidfVectorizer\n",
    "# We specify explicitly the parameters used, even for defaults, in order to be precise with our assumptions\n",
    "# and for reproducibity\n",
    "# note: sklearn by default throws away punctuations; we accept this as we are not performing sentiment analysis\n",
    "# and related tasks where punctuations may be highly informative\n",
    "\n",
    "# https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XeLZRzJKjmE\n",
    "vectorizer_tfidf = TfidfVectorizer(strip_accents='ascii',\n",
    "                                  lowercase=True,\n",
    "                                  analyzer='word',\n",
    "                                  stop_words='english',\n",
    "                                  ngram_range=(1, 1),\n",
    "                                  max_df=1.0,\n",
    "                                  min_df=1,\n",
    "                                  max_features=None,\n",
    "                                  vocabulary=None,\n",
    "                                  binary=False,\n",
    "                                  dtype='float64',\n",
    "                                  norm='l2',\n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  sublinear_tf=False)\n",
    "\n",
    "X_vectors_tfidf = vectorizer_tfidf.fit_transform(corpus_psychedelics_sorted)\n",
    "# vectorizer_tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['substance_5_meo_dmt',\n",
       " 'substance_alcohol',\n",
       " 'substance_ayahuasca',\n",
       " 'substance_cannabis',\n",
       " 'substance_dmt',\n",
       " 'substance_ibogaine',\n",
       " 'substance_kava',\n",
       " 'substance_ketamine',\n",
       " 'substance_kratom',\n",
       " 'substance_lsd',\n",
       " 'substance_mdma',\n",
       " 'substance_mescaline',\n",
       " 'substance_methamphetamine',\n",
       " 'substance_morning_glory',\n",
       " 'substance_mushrooms',\n",
       " 'substance_nitrous_oxide',\n",
       " 'substance_pcp',\n",
       " 'substance_salvia',\n",
       " 'substance_syrian_rue']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look again at the sorted ordering of the substances\n",
    "substances_list_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to extract keywords from documents\n",
    "# from http://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.XeLcAzJKjmE\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "\n",
      "===Keywords===\n",
      "seeds 0.381\n",
      "time 0.286\n",
      "experience 0.282\n",
      "felt 0.284\n"
     ]
    }
   ],
   "source": [
    "# Tutorial on how to extract key words based on tf-idf: http://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.XeLcAzJKjmE\n",
    "\n",
    "# Using the tf-idf frequencies\n",
    "feature_names_tfidf = vectorizer_tfidf.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sorted_items=sort_coo(X_vectors_tfidf.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names_tfidf,sorted_items,10)\n",
    " \n",
    "# now print the results\n",
    "print(\"\\n=====Doc=====\")\n",
    "# print(doc[:1000])\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])\n",
    "\n",
    "    \n",
    "# # get the first vector out (for the first document)\n",
    "# first_vector_tfidfvectorizer= X_vectors_tfidf[0]\n",
    " \n",
    "# # place tf-idf values in a pandas data frame\n",
    "# df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=feature_names_tfidf, columns=[\"tfidf\"])\n",
    "# df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sidebar: Some Interesting Observations from `Tfidfvectorizer` output\n",
    "It's interesting to note that looking at the Tfidfvectorizer feature_names, we see many numbers. These numbers either stand alone or are followed by units and quantifiers.  ,  \n",
    "\n",
    "These units seem to be used to specify multiple types of quantities, such as:\n",
    "- weight (that of substance ingested or user): \n",
    "    - 'c', 'g', 'gm', 'gram', 'grams', 'lbs', 'mcg', 'mg', 'microgram', 'mics', 'o', 'oz', 'pounds', 'ug'\n",
    "- volume or length\n",
    "    - 'cc', 'cm', 'cup', 'ft', 'i', 'inch', 'iu', 'kg', 'kgs', 'meters', 'metres', 'mile', 'miles', 'ml', 'sq', 'tbs', 'tbsp', 'yards'\n",
    "- time or time period\n",
    "    - \"'s' (e.g.: in 'the 1990s')\", 'XXhXXm', 'am', 'hours', 'hr', 'min', 'minute', 'pm', 'sec'\n",
    "- user age\n",
    "    - 'day', 'y', 'yo', 'yr'\n",
    "- scales and dimensions\n",
    "    - \"'5x' (e.g.: 5x salvia extract)\", '1x15', '1x50mg'\n",
    "- addresses\n",
    "    - \"st\"\n",
    "- speed\n",
    "    - \"mph\"\n",
    "- numerical quantification\n",
    "    - \"'strips' (e.g.: number of lsd strips)\", \"'ths' (as in 100ths times)\", 'nd', 'th'\n",
    "- sound information\n",
    "    - 'bpm', 'bps', 'hz'\n",
    "\n",
    "A closer examination of this numberical data may yield very interesting information about user demographics, set and setting, and dosage. All of this may be extremely valuable since dosage, set and setting, are among the most important factors in determining one's subjective experience when consuming a drug, and demographics may be correlated with the valence of a particular experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extension - extracting bigrams and trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3.2.2 TODO: `tfidf wordclouds` >>> using all documents separately with multilabel\n",
    "\n",
    "TODO: How do we handle multilabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4: (For Fun) Creating custom word cloud masks\n",
    "\n",
    "To make the wordclouds more interesting, we can add custom image masks to the wordclouds, as inspired by [this article](https://towardsdatascience.com/creating-word-clouds-with-python-f2077c8de5cc), [this tutorial](https://www.datacamp.com/community/tutorials/wordcloud-python), and `wordcloud` package [documentation](http://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot wordcloud with image mask\n",
    "# Make sure white parts are 255 instead of 0, as per: https://www.datacamp.com/community/tutorials/wordcloud-python\n",
    "# TODO: Why is the wordcloud blurry, and why can I not get the mask to work appropriately?\n",
    "def transform_format(val):\n",
    "    if val == 0: return 255\n",
    "    else: return val\n",
    "    \n",
    "mask_mushroom = np.array(Image.open(\"images/util_mushroom_vector_mask.png\"))\n",
    "# vectorization of transform_format function above; invert the mask to be in the right format\n",
    "mask_mushroom[mask_mushroom > 240] = 30\n",
    "mask_mushroom[mask_mushroom == 0] = 255\n",
    "mask_mushroom\n",
    "\n",
    "#plot a mushroom wordmap about mushrooms\n",
    "#increase \"resolution\": https://stackoverflow.com/questions/28786534/increase-resolution-with-word-cloud-and-remove-empty-border\n",
    "\n",
    "# wordcloud_mushroom_masked = WordCloud(width=1000, height=10000, stopwords=stopword_list, mask=mask_mushroom, max_font_size=50, max_words=100, background_color=\"black\").generate(reports_text_tokenized[\"substance_mushrooms\"])\n",
    "wordcloud_mushroom_masked = WordCloud(max_font_size=50, max_words=100, mask=mask_mushroom, stopwords=stopword_list, background_color=\"white\").generate(reports_text_tokenized[\"substance_mushrooms\"])\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(wordcloud_mushroom_masked, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.savefig('wordcloud_naive_mushroom_masked')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Wordclouds with Log Odds Ratio\n",
    "\n",
    "#### TODO\n",
    "\n",
    "This approach uses Informative Prior Log Odds IPLO ratio method, which is used in natural language processing to identify the differences in language usage patters between two groups. The foundations rests upon log odds ratio, a widely used technique in statistical inference and hypothesis testing. Intuitively, we ask, how much more likely is a word to belong to a particular document, as opposed to in another document. In a very deep way, log odds ratio relates to the F statistic in statistical inference, and is used widely for model selection.( // TODO: Cite ISLR / ESM)\n",
    "\n",
    "The theoretical framework is outlined in more detail [here](TODO) // TODO cite CCB's log odds paper\n",
    "\n",
    "In the case of multilabelled psychedelic trip reports, we can take a one-vs-all approach, i.e.: compare documents with the label `substance.mushrooms` and documents without the label `substance.mushrooms`. This will discover words / tokens that are particularly informative of the `substance.mushrooms` class. Note again that reports with a particular `substance.*` label are not necessarily uniquely `substance.*`, and may contain other labels since people have a tendency to consume multiple different types of drugs simultaneously.\n",
    "\n",
    "Code adapted from [john-hewitt](https://github.com/john-hewitt/iplo-analysis) and Chris Callison Burch's research group (reproduction with explicit permission from Callison-Burch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Logs Odds Ratio Code Adapted from Chris Callison Burch's Lab!!!\n",
    "# TODO: Implement for psychedelic trip reprots\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Wordclouds with other methods\n",
    "\n",
    "### TODO: \n",
    "\n",
    "- Logistic regression weights\n",
    "- Weights from other classification methods\n",
    "- LDA document embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Visualizing High Dimensions: Clustering and Principle Component Analysis (PCA)\n",
    "\n",
    "`TODO`\n",
    "\n",
    "- TODO: Agglomerative Hiearchical Clustering\n",
    "- TODO: KNN (?) - Probably more of a classification algorithm\n",
    "- TODO: Search up state of the art document clustering techniques\n",
    "- TODO: K Means clustering; \"There are multiple ways to select the optimal value of k like using the Sum of Squared Errors metric, Silhouette Coefficients and the Elbow method\"\n",
    "- TODO: [Affinity Propagation](https://en.wikipedia.org/wiki/Affinity_propagation)\n",
    "- TODO: See [advanced techniques](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa) for using dense document embeddings\n",
    "- TODO: Clustering can be based on many diferent document level features:\n",
    "    - TF\n",
    "    - TF IDF\n",
    "    - LDA Term Document Matrix\n",
    "    - Dense Embeddings (average of all the word embeddings, from something like Word2Vec); finetune existing dense word embedding with something like ULMfit\n",
    "- Visualize Clusters with PCA: See [advanced tecniques](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n",
    "\n",
    "## 8.1 High Dimensionality: Everpresent and Difficult to Grok\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 8.2 Clustering\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 8.3 Principle Component Analysis\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 8.4 Other Methods\n",
    "\n",
    "`TODO`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarity_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-0cece96fdad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhierarchy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdendrogram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m pd.DataFrame(Z, columns=['Document\\Cluster 1', 'Document\\Cluster 2', \n\u001b[1;32m      8\u001b[0m                          'Distance', 'Cluster Size'], dtype='object')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'similarity_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Example of Agglomerative Hiearchical Clustering \n",
    "# Source: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(similarity_matrix, 'ward')\n",
    "pd.DataFrame(Z, columns=['Document\\Cluster 1', 'Document\\Cluster 2', \n",
    "                         'Distance', 'Cluster Size'], dtype='object')\n",
    "\n",
    "\n",
    "\n",
    "###### Visualizing the clustering process\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Data point')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(Z)\n",
    "plt.axhline(y=1.0, c='k', ls='--', lw=0.5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-37f3dcb1584c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcluster_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfcluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mcluster_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ClusterLabel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Obtain cluster levels\n",
    "# Source: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "# TODO: Compare the automatic clusters with the substance multilables\n",
    "# TODO: Can use a \"vector\" of the labels as a training label?\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "max_dist = 1.0\n",
    "\n",
    "cluster_labels = fcluster(Z, max_dist, criterion='distance')\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering based on LDA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Example code for clustering with LDA embeddings\n",
    "# Source: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3, random_state=0)\n",
    "km.fit_transform(features)\n",
    "cluster_labels = km.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other ways of Visualizing Trip reports\n",
    "\n",
    "- Clustering with ordinary `K-means` and `minibatch K-means`; see [sklearn example](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)\n",
    "- `Principle Components Analysis (PCA)` of trip reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "2019-11-30 00:37:12,265 INFO Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
      "2019-11-30 00:37:12,270 INFO Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
      "  --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
      "  --no-idf              Disable Inverse Document Frequency feature weighting.\n",
      "  --use-hashing         Use a hashing feature vectorizer\n",
      "  --n-features=N_FEATURES\n",
      "                        Maximum number of features (dimensions) to extract\n",
      "                        from text.\n",
      "  --verbose             Print progress reports inside k-means algorithm.\n",
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "3387 documents\n",
      "4 categories\n",
      "\n",
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 1.086469s\n",
      "n_samples: 3387, n_features: 10000\n",
      "\n",
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, compute_labels=True, init='k-means++',\n",
      "                init_size=1000, max_iter=100, max_no_improvement=10,\n",
      "                n_clusters=4, n_init=1, random_state=None,\n",
      "                reassignment_ratio=0.01, tol=0.0, verbose=False)\n",
      "done in 0.154s\n",
      "\n",
      "Homogeneity: 0.539\n",
      "Completeness: 0.572\n",
      "V-measure: 0.555\n",
      "Adjusted Rand-Index: 0.576\n",
      "Silhouette Coefficient: 0.007\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: god people jesus say don believe christian bible com religion\n",
      "Cluster 1: graphics university image thanks ac files uk file com 3d\n",
      "Cluster 2: space com nasa access henry digex gov article pat toronto\n",
      "Cluster 3: sandvik keith sgi com livesey kent caltech apple newton solntze\n"
     ]
    }
   ],
   "source": [
    "# TODO: Example code from: https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py\n",
    "\n",
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", type=\"int\",\n",
    "              help=\"Preprocess documents with latent semantic analysis.\")\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", dest=\"minibatch\", default=True,\n",
    "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", default=False,\n",
    "              help=\"Use a hashing feature vectorizer\")\n",
    "op.add_option(\"--n-features\", type=int, default=10000,\n",
    "              help=\"Maximum number of features (dimensions)\"\n",
    "                   \" to extract from text.\")\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", dest=\"verbose\", default=False,\n",
    "              help=\"Print progress reports inside k-means algorithm.\")\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "# categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "print()\n",
    "\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(\"Extracting features from the training dataset \"\n",
    "      \"using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    if opts.use_idf:\n",
    "        # Perform an IDF normalization on the output of HashingVectorizer\n",
    "        hasher = HashingVectorizer(n_features=opts.n_features,\n",
    "                                   stop_words='english', alternate_sign=False,\n",
    "                                   norm=None, binary=False)\n",
    "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
    "                                       stop_words='english',\n",
    "                                       alternate_sign=False, norm='l2',\n",
    "                                       binary=False)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=opts.use_idf)\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "if opts.n_components:\n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "    t0 = time()\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(opts.n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "if opts.minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=opts.verbose)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "if not opts.use_hashing:\n",
    "    print(\"Top terms per cluster:\")\n",
    "\n",
    "    if opts.n_components:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9: Predicting Substance Labels from Trip Reports\n",
    "\n",
    "`TODO`\n",
    "\n",
    "- Baseline: multinomial Bayes and other techniques; see sklearn [classifying text documents](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py)\n",
    "- Baseline: Logistic Regression:\n",
    "    - Coefficients returned by logistic regression can be used as weights to scale words in worclouds\n",
    "    - TODO: How do we handle the multilabel case?\n",
    "    - Appropriate regularization like LASSO / Ridge (Elastic Net) for feature selection\n",
    "- Baseline: Naive Bayes classifier\n",
    "- BERT fine-tuning\n",
    "    - [Huggingface](https://github.com/huggingface/transformers) + transformers - See Chris Callison-Burch‚Äôs recommendations\n",
    "- Handling `multilabel learning`, see [sklearn multiclass and multilabel learning](https://scikit-learn.org/stable/modules/multiclass.html#multiclass-and-multilabel-algorithms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 The Classification Problem: Our Goals\n",
    "`TODO`\n",
    "\n",
    "## 9.2 Multilabel Considerations\n",
    "`TODO`\n",
    "\n",
    "\n",
    "Using [multilabel classification example](https://scikit-learn.org/stable/auto_examples/plot_multilabel.html#multilabel-classification) from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Example code from sklearn, see: https://scikit-learn.org/stable/auto_examples/plot_multilabel.html#multilabel-classification\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "\n",
    "def plot_hyperplane(clf, min_x, max_x, linestyle, label):\n",
    "    # get the separating hyperplane\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(min_x - 5, max_x + 5)  # make sure the line is long enough\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "    plt.plot(xx, yy, linestyle, label=label)\n",
    "\n",
    "\n",
    "def plot_subfigure(X, Y, subplot, title, transform):\n",
    "    if transform == \"pca\":\n",
    "        X = PCA(n_components=2).fit_transform(X)\n",
    "    elif transform == \"cca\":\n",
    "        X = CCA(n_components=2).fit(X, Y).transform(X)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "\n",
    "    classif = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "    classif.fit(X, Y)\n",
    "\n",
    "    plt.subplot(2, 2, subplot)\n",
    "    plt.title(title)\n",
    "\n",
    "    zero_class = np.where(Y[:, 0])\n",
    "    one_class = np.where(Y[:, 1])\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=40, c='gray', edgecolors=(0, 0, 0))\n",
    "    plt.scatter(X[zero_class, 0], X[zero_class, 1], s=160, edgecolors='b',\n",
    "                facecolors='none', linewidths=2, label='Class 1')\n",
    "    plt.scatter(X[one_class, 0], X[one_class, 1], s=80, edgecolors='orange',\n",
    "                facecolors='none', linewidths=2, label='Class 2')\n",
    "\n",
    "    plot_hyperplane(classif.estimators_[0], min_x, max_x, 'k--',\n",
    "                    'Boundary\\nfor class 1')\n",
    "    plot_hyperplane(classif.estimators_[1], min_x, max_x, 'k-.',\n",
    "                    'Boundary\\nfor class 2')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.xlim(min_x - .5 * max_x, max_x + .5 * max_x)\n",
    "    plt.ylim(min_y - .5 * max_y, max_y + .5 * max_y)\n",
    "    if subplot == 2:\n",
    "        plt.xlabel('First principal component')\n",
    "        plt.ylabel('Second principal component')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "X, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n",
    "                                      allow_unlabeled=True,\n",
    "                                      random_state=1)\n",
    "\n",
    "plot_subfigure(X, Y, 1, \"With unlabeled samples + CCA\", \"cca\")\n",
    "plot_subfigure(X, Y, 2, \"With unlabeled samples + PCA\", \"pca\")\n",
    "\n",
    "X, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n",
    "                                      allow_unlabeled=False,\n",
    "                                      random_state=1)\n",
    "\n",
    "plot_subfigure(X, Y, 3, \"Without unlabeled samples + CCA\", \"cca\")\n",
    "plot_subfigure(X, Y, 4, \"Without unlabeled samples + PCA\", \"pca\")\n",
    "\n",
    "plt.subplots_adjust(.04, .02, .97, .94, .09, .2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Initial Attempts\n",
    "`TODO`\n",
    "\n",
    "## 9.4 The Power of Transfer Learning: State of the Art\n",
    "`TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 NLP in Practice: Using automatic piplines for text feature extraction and classification\n",
    "See sklearn [pipline example](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py)\n",
    "\n",
    "TODO: Modify to suit multi-label learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CODE FROM https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py\n",
    "\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "# License: BSD 3 clause\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "\n",
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(tol=1e-3)),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (20,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                               n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Generation of Trip Reports\n",
    "`TODO`\n",
    "\n",
    "- Hidden Markov Model baseline\n",
    "- LSTM baselines\n",
    "    - Character Model\n",
    "    - Word model\n",
    "- Finetuning BERT to generate trip reports\n",
    "\n",
    "\n",
    "## 10.1 The Generation Problem: Our Goals\n",
    "`TODO`\n",
    "## 10.2 Hidden Markov Baseline\n",
    "`TODO`\n",
    "## 10.3 Long Short Term Memory Network Baseline\n",
    "`TODO`\n",
    "## 10.4 Fine Tuning BERT: State of the Art\n",
    "`TODO`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Feature Importance\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 11.1 The Revealing Nature of Noticing\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 11.2 LIME: State of the Art Feature Importance\n",
    "\n",
    "`TODO`\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Meta-discussions: Neuroscience, Ethics, Mycelial Connectedness, and Future Directions - The Truly Important Stuff\n",
    "\n",
    "## 12.1 What is Language\n",
    "‚Üí model discussed with Daniel Colson\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.2 What is (Big) Data\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.3 What is Data Privacy and Security\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.4 What is Reproducibility\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.5 Open Science: A Proposal for Collaborative Research\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.6 Towards Mycelial Connectedness\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 12.7 The Institute of Paradise Engineering\n",
    "\n",
    "`TODO`\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Gratefulness: Acknowledgements of Foundational Connectness\n",
    "\n",
    "`TODO`\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Appendix: archived code, explanations, and miscellaneous musings\n",
    "\n",
    "## 14.1 Word Cloud of this thesis\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 14.2 Word Clouds of Pivotal Conversations\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 14.3 Original Thesis Proposal\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 14.4 Updated Thesis Proposal\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## 14.5 The Mathematics of Higher Dimensions\n",
    "\n",
    "`TODO`\n",
    "\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- TODO: Use Bibtex  \n",
    "- TODO: Is there a way to link to citations from Jupyter Notebook? Maybe just use latex\n",
    "- [Workflow for citations in jupyter notebook](https://sylvaindeville.net/2015/07/17/writing-academic-papers-in-plain-text-with-markdown-and-jupyter-notebook/)\n",
    "- [The Automated Academic](https://sylvaindeville.net/2015/01/04/the-automated-academic/)\n",
    "- TODO: Cite scikit learn\n",
    "- [Publishing on Figshare](https://figshare.com/)\n",
    "- [Fine tuning word embeddings on smaller dataset](https://arxiv.org/abs/1801.06146)\n",
    "- TODO: [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\n",
    "- TODO: [Use sciPy pretrain transfer learning](https://spacy.io/usage/v2-1#pretraining) to improve the accuracy of model, similar to `Google`'s BERT and `fast.ai`'s ULMFiT\n",
    "- TODO: [spaCy Visualizers](https://spacy.io/usage/visualizers) SOOOO COOOL!!\n",
    "- Publishing my paper:\n",
    "    - [Binder, turn git repo into notebooks](https://mybinder.org/) with docker images\n",
    "    - [Jupyter Hub, serve notebooks to multiple users](https://jupyterhub.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['report', 'title', 'substance']\n",
      "[\"After having had some success with other forms of legal highs, I decided to experiment with salvia.  I had heard it was an intense but short acting psychedelic, which intrigued me.  Although I have loved psychedelics since I first discovered them eight years ago, I dont use them that often in part because of the time involved in tripping and analyzing the experience.  That said I have used psychedelics somewhere around three dozen times in my life.  My favorite method of tripping is LSD, but I have also used mushrooms, morning glory and baby woodrose seeds.  I have also used a variety of other drugs, including stimulants, depressants, empathogens, and deleriants.\\n\\n\\n\\nI ordered salvia online, choosing a 5x extract.  When the package arrived my husband wanted to try it immediately.  I prefer to prepare more for trips but knowing the effects werent supposed to last very long I said why not?  My husband, who uses psychedelics rarely now but used them a lot when he was younger, went first.  He took a large hit from our glass pipe.  He coughed a lot but reported no initial effects.  Only two minutes later he took two smaller but decent sized hits.  He started to smile that tripped out smile and I knew it had hit him.  He was silent but looked around like everything was captivating.  After a few minutes I asked how he felt and he said great and started to laugh.  He described a pouring feeling as though he was being poured into his body.  When he had recovered a bit more he told me it was like acid, which surprised me as the reports I read were not really acidlike.  He stumbled a bit but seemed to be mostly normal after ten minutes.  I asked him to hand me the pipe when he felt sober enough to babysit me and he handed it over five minutes later (15 minutes after smoking).\\n\\n\\n\\nHere is where I made a big mistake.  Seeing that it had required 3 hits to get my husband off, I decided to take two hits.  I had read enough reports to know better, but the extremely short duration of the trip made me feel safe.  I took the first hit slowly and smoothly (a pleasant smoke I might add), holding it in for about 20 seconds.  I blew out a huge cloud of smoke, and started to take another hit.  Halfway through the second hit (while still inhaling) I got tunnel vision.  I hardly felt the smoke leave my lungs.  The whole room was spinning and everything had trails.  I was shocked and amazed at the change in my surroundings.  It was like a cross between shroom trails and the bed spins (like from too much alcohol).  I felt two things at once.  Part of me was feeling totally disoriented and wanted everything to stop moving.  And part of me was having fun.  I realized that if I looked at the center of the room things were much better and I started to laugh really hard.  I was in ecstasy and in pain.\\n\\n\\n\\nThen the center of the room began to spin wildly too.  I felt like I was in the hold of a ship being thrashed about by massive waves.  My husband was standing half on the balcony smoking.  At this point he walked in a step and said Welcome to my world.  At that instant the whole room landed hard on its side, taking my husband and me with it.  Gravity was now pulling me in towards the kitchen, which is normally North.  South, the direction of the balcony, was up.  Both me and my husband were stuck sideways, our legs dangling in the air, inches from the kitchen counter's side, which was now the floor.  I was able to move my parts around my center of gravity, but not able to move my center of gravity.  I did not fall to the 'floor' because I was caught in the grip of two opposing forces.  One at my back and the other at my front.\\n\\n\\n\\nI was certain that I had been sucked into an alternate dimension or world, which I called sideways world.  This was what my husband meant when he said welcome to my world, or so I thought at the time. Although this new world had definite direction it was spinning about this new axis at lightening speed. I kept trying to turn, hoping that a new perspective would stop everything, but I felt as much as saw the added trails this caused.  In fact my whole body was spinning and contracting around its axis, which ran down the spine.  This was highly painful and utterly terrifying.  I have had scary moments on drugs before but nothing that comes close to this.  I didnt know that fear could be like that.  I had had one terrifying moment of clarity in a dream once, when I knew that there was no afterlife and that my consciousness would cease to exist.  This rivaled that dream in the fear department but was far more frantic.\\n\\n\\n\\nI hated my husband for damning me to this world.  I knew that it was a drug experience, and that it would end, but I had forgotten how long it would take and thought I would be like this for hours.  I was certain that he was sideways too, that he knew what this drug would do to me and let me do it anyway, so that he wouldn't be alone in this sideways world.  I looked at the door to the balcony.  The door my husband had stepped through before he turned over.  It was salvation.  The way out.  The door to the straight world.  I tried very hard to reach that door, but the world was spinning too wildly and all I could do was spin about my axis.  The door seemed to be closer at times and farther at others but I dont recall ever making progress toward it or moving away.  I yelled at my husband for putting me here, though I wasnt sure if the words came out.  I remember him talking to me but I was too frantic to listen.  His answers made no sense either.  In response to something I said he replied But Im from this world.  This struck me as utterly stupid.  He had walked into the sideways world through that door.  I saw him do it.  How could he deny it?  But as I told him this something clicked.\\n\\n\\n\\nThat door wasnt the door to the straight world!  At the same time the spinning slowed and I realized I was coming down.  Thank God!  I lay on the couch for several minutes, letting the feeling slip away.  Still terrified and in pain but now with the knowledge that it was ending.  All I had to do was wait.\\n\\n\\n\\nWhen the last of the terror had subsided I sat up and looked around.  Things were glowy and trippy like the come up of acid and I felt stoned with a mdma edge.  This lasted for about half an hour and was great.  If this was all salvia did I would smoke it all the time.  I no longer felt fear at all but a sense of awe at what the drug had done to me.  My hubby was obviously very shaken.  I knew I put him through a lot and told him several times that I was better now.  As the come down progressed I discussed my experience with him and discovered that what he saw was quite different from what I had perceived.\\n\\n\\n\\nFor his point of view, everything was fine at first.  I was laughing for at least three minutes and appeared to be enjoying myself (though in reality the fear was already there).  He stepped in after finishing a cigarette and said Welcome to my world because everything was funny to him too.  At this point I apparently jumped up and said I had to get out of this world.  I ran for the balcony door and my husband, afraid I meant to jump off the balcony, stopped me.  The next five minutes were a drag out physical struggle in which I constantly lunged for the door while he restrained me and tried to talk me down.  He said he told me a lot of things that I have no memory of.  I was also yelling and screaming at the top of my lungs.  Hubby was afraid someone would call the cops with all the ruckus I was causing.  Most of this time I was on the floor, but I fought my way up again (apparently I managed to get up several times which is surprising because my husband is much stronger than I am physically) and he put me on the couch were I lay and went limp for the first time.  This is were I came back to reality.\\n\\n\\n\\nA few days later I decided to try again, this time with less salvia.  I did not want to experience a full salvia trip again but was curious where this powerful drug might take me with more caution.  I filled the pipe bowl with a 80/20 mixture of mugwort (gives a nice stoned feeling) and salvia.  I smoked this in two hits and found myself in the happy glowy MDMA like state that I had experienced before as a come down.  I waited 5 minutes and nothing else happened so I made another bowl with a half and half mixture of the same drugs.  A hit of that put me in a deeper trip.  I experience the same feeling but with far less intensity.  This time it was the closet door that I felt I needed to go to.  I closed my eyes and buried my head in my husbands shirt until the feeling past.  I can see how many people see tunnels on salvia, since I keep focusing on doors.  After this second attempt I realized that salvia was going to keep taking me to the same place until I had a better idea of where I wanted to go and how to get there.\\n\\n\\n\\nI came away from this trip terrified of salvia but also deeply intrigued.  Someday I would like to try it again, at a smaller dose and with more preparation.  This drug has the power to take me somewhere amazing but I am not ready for it yet.\", 'Sideways World', 'Salvia divinorum (5x extract)']\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset trips.csv\n",
    "import csv\n",
    "printed = 0\n",
    "with open('trips.csv') as f:\n",
    "  csv_reader = csv.reader(f)\n",
    "  for line in csv_reader:\n",
    "    if printed < 2:\n",
    "        print(line)\n",
    "        printed += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epilogue: The Stories that are Not Part of the Story\n",
    "\n",
    "`TODO`\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Issues Encoutered, and solutions\n",
    "\n",
    "- Python not working:  \n",
    "`brew install python`  \n",
    "`brew update python`  \n",
    "- Need to update xcode command line tools after every major / minor os upgrade: `xcode-select --install`; see [here](https://stackoverflow.com/questions/52522565/git-is-not-working-after-macos-update-xcrun-error-invalid-active-developer-pa)\n",
    "\n",
    "- Issues with installing [wordcloud](https://www.datacamp.com/community/tutorials/wordcloud-python)\n",
    "- TODO: Write an article explaining managing python versions on Mac, using both `Anacoda` and `pip`\n",
    "- Cannot do `brew link <packageName>`; see [here](https://github.com/caskformula/homebrew-caskformula/issues/10)\n",
    "- Adding `autocompletion` to jupyter notebook with [this tool](https://github.com/ipython-contrib/jupyter_contrib_nbextensions)\n",
    "- Could not for the life of me get `toc2` to work\n",
    "- 5 tries... with trying to get code hiding to work; 2019-12-04, still no luck...\n",
    "- `TODO`: Putting a pin in this for now rip\n",
    "- see [this](https://gist.github.com/parente/35f5d3a9145bd3f030c8#file-nbviewer_code_toggle-ipynb)\n",
    "\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Fun (Optional) things discovered while working on Project\n",
    "- Add [animated progress nav](https://lab.hakim.se/progress-nav/)\n",
    "- [New York Times recommendation system](https://open.blogs.nytimes.com/2015/08/11/building-the-next-new-york-times-recommendation-engine)\n",
    "- [CSS background tricks](https://tympanus.net/codrops/css_reference/background/)\n",
    "- [spaCy webapplication demo](https://explosion.ai/demos/displacy)\n",
    "- [spaCy NER visulizer demo](https://explosion.ai/demos/displacy-ent)\n",
    "- [NLP with spaCy course](https://course.spacy.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdn.rawgit.com/parente/4c3e6936d0d7a46fd071/raw/65b816fb9bdd3c28b4ddf3af602bfd6015486383/code_toggle.js\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script src=\"https://cdn.rawgit.com/parente/4c3e6936d0d7a46fd071/raw/65b816fb9bdd3c28b4ddf3af602bfd6015486383/code_toggle.js\"></script>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
